\section{Review of Selected RDD Methods}\label{sec:review}
%% introduces notation
% Z
% Y
% \indicator{}
%  $y_{Ti}$ and $y_{Ci}$ (upper or lower case??)


% Capturing the local randomization essence of the RDD calls for
% a merger of RDD methods with concepts developed for RCTs. This
% section selectively reviews relevant literatures.
The method presented in this paper builds on existing methods for RDDs.
This section selectively reviews relevant literature.

Let $Z \in \{0,1\}$ indicate assignment to treatment ($Z=1$) as opposed to control
($Z=0$).
For the remainder of the paper, let $R$ be the \emph{centered} running
variable---the difference between the running variable and the RDD
threshold $c$---so that $Z \equiv \indicator{R< 0}$,
$\indicator{R \leq 0}$, $\indicator{R\geq 0}$ or $\indicator{R > 0}$,
depending on how intervention eligibility relates to the threshold,
where $\indicator{x}=1$ if $x$ is true and $0$ otherwise.
Let $Y$ represent the outcome of interest.
For simplicity assume non-interference, the model that
a subject's response may depend on his but not also on other subjects'
treatment assignments \citep{cox:1958,rubin:1978}. Thus we may take each $i$
to have two potential outcomes, $y_{Ti}$ and $y_{Ci}$, at most one of which is observed;
 observed
responses $Y$ coincide with $ZY_{T}+(1-Z)Y_{C}$.

\subsection{The ANCOVA Model for RDDs}\label{sec:robust-analys-covar}

The classical analysis of covariance (\textsc{ancova}) model for
groups $i=1,\ldots, k$, each including subjects $j=1, \ldots, n_{i}$,
says that
$Y_{ij} = \alpha_{i} + \beta X_{ij} + \epsilon_{ij}$, where $\epsilon_{ij}
\sim \mathrm{Normal}(0, \sigma^{2})$ is independent of the continuous
covariate $X_{ij}$.
In the classical development of RDDs, \textsc{ancova} with $k=2$
groups---treated and untreated---is a leading option among statistical
models
\citep{thistlethwaite1960regression}.
A potential outcomes version of the model is
 $Y_{Ci} = \alpha_{0} + \beta R_{i} + \epsilon_{Ci}$ and
$Y_{Ti} = \alpha_{1}  + \beta R_{i} + \epsilon_{Ti}$, with
 $\epsilon_{Ci} \sim \mathrm{Normal}(0, \sigma^{2})$ and
 $\epsilon_{Ti} \sim \mathrm{Normal}(0, \sigma^{2})$.
In marked contrast to RCTs, it is
not required that $(Y_{T}, Y_{C}) \independent Z$: to the contrary, both
$Y_{C}$ and $Y_{T}$ are presumed to associate with $R$, which in turn
determines $Z$.
Nonetheless, under this model the estimated $Z$ coefficient from the
model
\begin{equation}\label{eq:classicOLS}
Y_i=\alpha+\beta R_i+\tau Z_i+\epsilon_i,
\end{equation}
fit using ordinary least squares (OLS), is unbiased for
$\alpha_{1} - \alpha_{0}$.
Under the \textsc{ancova} model, this estimation target coincides with  $\lim_{r\downarrow 0} \EE(Y | R=r) -
\lim_{r\uparrow 0} \EE(Y | R=r)$ and, simultaneously,
limit-free estimation targets such as $\EE Y_{T}  - \EE Y_{C}$.

The OLS approach estimates $\tau$ as a parameter in regression model
\eqref{eq:classicOLS}. In contrast, the analysis of
\S~\ref{sec:using-eqref-test} took place in two separate steps:
first adjust outcomes for $R$; then, test hypotheses by contrasting
adjusted outcomes of treated and untreated subjects.
However, the OLS estimator may be restated in the manner of
\S~\ref{sec:using-eqref-test}---a more involved procedure, to be sure,
but one with an important advantage to be described in \S~\ref{sec:fuzzy-regr-disc}.
Consider the hypothesis $H: Y_{T} = Y_{C} + \tau$.
Define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H$, $Y_H=Y_C$)
and $\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
b\mathbf{r}$.
Finally, test $H$ with statistic
\begin{equation} \label{eq:MeanDiffTestStat}
\meanDiffT(\mathbf{Y}_H , \mathbf{Z}) =
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0}
\end{equation}
---where $\hat{\alpha}$ and $\hat{\beta}$ are estimated from an OLS fit of the variant of \eqref{eq:classicOLS}
with dependent variable $\mathbf{y}_{H}$.
Because of the structural relationship
between $R$ and $Z$, the null distribution
of \eqref{eq:MeanDiffTestStat} is not tractable. (In
\S~\ref{sec:using-eqref-test}, test statistics'
permutation distributions were straightforwardly enumerable
because slope and intercept parameters had been estimated
from a separate sample.) However, under the parametric \textsc{ancova} model, with
conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
\mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
$\meanDiffT(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
variance equal to the classical OLS variance of the coefficient on
$Z$.

% \S~\ref{sec:using-eqref-test}.
% Consider the hypothesis $H_\tau: Y_{T} = Y_{C} + \tau$.
% Next, define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H_\tau$, $Y_H=Y_C$)
% and residuals $\resid\equiv\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
% b\mathbf{r}$.
% Finally, test $H_\tau$ with statistic
% \begin{equation*}
% \meanDiffT(\mathbf{Y}_H , \mathbf{Z}) =
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0},
% \end{equation*}
% where $\hat{\alpha}$ and $\hat{\beta}$ are the standard OLS estimates.
% Because of the structural relationship
% between $R$ and $Z$, (and, unlike in
% \S~\ref{sec:using-eqref-test}, regression parameters were not estimated
% with a separate dataset)
% permutation tests based on this statistic are not tractable.
% However, under the parametric \textsc{ancova} model, with
% conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
% \mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
% $\meanDiffT(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
% variance equal to the classical OLS variance of the coefficient on
% $Z$.

In general, the set \{$c$:
$H_{c}: Y_{T} = Y_{C} + c$ is not rejected at level $\alpha$\}, which can be seen to be an interval, is a
$100(1-\alpha)\%$ confidence interval for $\tau$ of the Rao score type
\citep{agresti2011scoreintervals}; the $c$ solving
$\meanDiffT(\mathbf{y}_{H}, \mathbf{z}) = 0$, which can be seen to
be unique, is an M-estimate of $\tau$ under both the classical
\textsc{ancova} model and various of its generalizations.
In fact, the estimate for $\tau$ corresponding to these statistical tests is
algebraically equal to the $Z$-coefficient from an OLS estimate of
\eqref{eq:classicOLS}, and the two-sided 95\% confidence interval induced in this
manner is the familiar
$\hat{\tau} \pm 1.96\, \mathrm{SE}(\tau)$.
However, these equivalences do not necessarily extend to estimation
strategies outside of OLS, such as the robust estimators of
\S~\ref{sec:robustFitters} below.

\subsubsection{Addressing the Wald interval's shortcomings for fuzzy RDDs} \label{sec:fuzzy-regr-disc}
RDDs susceptible to non-compliance---where subjects' actual
treatments may differ from $Z$---are called ``fuzzy.''
In these cases, let $D$ indicate whether treatment was actually
received.
This $D$ is an intermediate outcome, so there are
corresponding potential outcomes $D_{C}$ and $D_{T}$, with $D \equiv ZD_{T}
+ (1-Z)D_{C}$.
Subject $i$ is a non-complier if $D_{Ci}=1$ or $D_{Ti}=0$, though we
will assume the monotonicity condition $D_{C}\equiv 0$; there may be
subjects assigned to the treatment who avoid it, but no one gets
treatment without being assigned to it.
We shall also posit the exclusion restriction,
that $Z$ influences $Y$ only by way of its effect on $D$
\citep{bloom1984ans,Angrist:etal:1996,imbens:rose:2005}.
Our focus of estimation is the
 ``treatment-on-treated'' effect (TOTE),
%complier
%average causal effect (CACE),
$\EE(Y_{T} - Y_{C}| D_{T}=1)$.

Statistical hypotheses about the TOTE take the form
$H_\tau:Y_T=Y_C+D\tau$.
To test $H_\tau$ under non-compliance, let $Y_H=Y-\tau D$,
designate $t (\mathbf{y}_H,\mathbf{r})$ as test statistic, and compare
its value to a standard Normal distribution.
(The only difference between hypothesis testing for a ``strict'' RDD, one
with full compliance, versus a fuzzy RDD, is in the formulation of
hypothesis $H$, and the construction of $Y_H$---the rest of the
process remains unchanged [\citealp{rosenbaum:1996:onAIR}].)  When
compliance is imperfect, this
iterative method yields confidence intervals with better coverage than Wald-type
confidence intervals---that is, intervals of form $\hat\tau \pm q_{*}
\mathrm{SE}(\hat\tau)$ with $\mathrm{SE}(\hat\tau)$ a single,
hypothesis-independent quantity
\citep[, Sec.~7]{imbens:rose:2005,baiocchiChengSmall2014IVtutorial}.


\subsubsection{Robust Standard Error Estimation}\label{sec:sandwich}
The \textsc{ancova} model for $(Y_{T}, Y_{C})$ is not readily dispensed with, but it
may be relaxed. OLS estimates of $\alpha_1-\alpha_{0}$ and
$\beta$ remain unbiased under non-Normality, provided
the $\epsilon$s have expectation 0 and bounded variances. The
ordinary \textsc{ancova} standard error does not require Normality of
the $( \epsilon_{i}: i )$, either, for use in large samples, although
it does require that they have a common variance.
To test
$\EE\{ \dt[\hat\theta]{{Y_H}}{ R} | Z=1\} = \EE\{ \dt[\hat\theta]{{Y_H}}{
R} | Z=0\}$
under potential heteroskedasticity, one estimates
$\var\left\{\meanDiffT(\mathbf{Y}_H , \mathbf{Z})\right\}$
using a sandwich or Huber-White estimator,
$\mathrm{SE}_{s}^{2} \left\{ \meanDiffT(\mathbf{Y}_H , \mathbf{Z}) \right\}$
\citep{huber1967behavior,mackinnonWhite1985sandwichHC,longErvin2000sandwichHC,
bellmccaffrey2002sandwichSEs,pustejovskyTipton2017sandwichSEs}, %as opposed to the %$\mathrm{SE}_{p}$; then
%$\mathrm{SE}_{u} %( \overline{e({Y_H}| R)}_{Z=1} -  \overline{e({Y_H}| R)}_{Z=0} )$
%of \eqref{eq:tudef}:
and refers $\meanDiffT(\mathbf{Y}_H,\mathbf{Z})/\mathrm{SE}_{s}$
% \begin{equation} \label{eq:tedef}
% t_{e} (\mathbf{y}_H,\mathbf{r}) =
% \frac{\overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=1} -
%                              \overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=0}}%
%                            {\mathrm{SE}_{s}\left\{ \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=1} -
%                              \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=0}
%                            \right\} }
% \end{equation}
to a $t$ or standard Normal reference distribution.
Sandwich standard errors confer robustness to misspecification of
$\var(\dt[\hat\theta]{Y_{H}}{R}\mid  R)$, not of $\EE(Y_{H}| R)$
\citep{freedman2006sch}, the latter being the topic of
the following section.
%Section~\ref{sec:robust-altern-ordin}.

% In applying \textsc{ancova} to an RCT, $\mathrm{SE}_{s}$ in
% \eqref{eq:tedef} may be taken as \eqref{eq:sudef}'s
% $\mathrm{SE}_{u}$% \citep{samii2012equivalencies}
% , a more straightforward quantity to compute; unfortunately, in RDDs
% this is not the case
% (Section~\ref{apnd:requ-forpr-eqref}). On the
% other hand, in \textsc{ancova} as applied to RDDs, if standard
% errors are calculated by the sandwich method then under
% $H: Y_{T} = Y_{C}$, \eqref{eq:tedef} is algebraically equivalent to the
% $t$-statistic of contrast $\hat{\alpha}_{1} -\hat{\alpha}_{0}$.


\subsection{Threats to RDD Validity and some Remedies}\label{sec:specification}

The \textsc{ancova} model for RDDs encodes additional assumptions,
beyond normality and homoskedasticity of regression errors and full
compliance with treatment assignment, which are not so easily
dispensed with.
% Perhaps most worryingly, the form of $\EE[(Y_{T}, Y_{C}) | R]$ may be
% misspecified, and the integrity of the RDD could be compromised by
% subjects who finely manipulate their values of $R$ around the cutoff
% in order to achieve or avoid a treatment assignment.
Methodological RDD literature has responded with specification tests
to detect these threats, or with flexible estimators that
seek to avoid them.

\subsubsection{Covariate Balance Tests}\label{sec:balanceTesting}
Analysis of RCTs and quasiexperiments often hinges on assumptions of
independence of
 $\mathbf{Z}$ from $(\mathbf{X}, \mathbf{Y}_{C}, \mathbf{Y}_{T})$.
%% Analogous statements will hold for more complex randomization or
%% ignorability assumptions as well.
 Although neither $\mathbf{Z} \independent \mathbf{Y}_{C}$ nor
 $\mathbf{Z} \independent \mathbf{Y}_{T}$ can be directly tested,
 since potential outcomes are only partly observed, assumptions of form
 $\mathbf{Z} \independent \mathbf{X}$ are falsifiable: researchers can
 conduct placebo tests for effects of $Z$ on $X$.
Of course, treatment cannot affect pre-treatment variables; this is
model-checking (%\citealp{bayarriBerger2000pvalues};
\citealp[][, \S~5.13]{cox2006pos}%; \citealp[][Ch.~6]{gelman:etal:2004};
%\citealp[][Ch.~14]{lehmannRomano2006TSH}
). % Here as
 % elsewhere \citep{box1980sab}, checks finding fault with a model prompt
 % refinement, not abandonment; refinements typical of RDD analysis remove portions of
 % the sample that are suspected of contamination.

 Writing in the RDD context, \citet{cattaneo2014randomization} test
 for marginal associations of $\mathbf{Z}$ with covariates $\mathbf{X}_{i}$,
 $i=1, \ldots, k$, using the permutational methods that are applied
 in Fisherian analysis of RCTs \citep[also see][]{liMatteiMealli2015BayesianRD}.
% Specifically, they
%  obtain p-values though \eqref{eq:01}, with test statistics
%  $t(\mathbf{x}_{i}, \mathbf{z}) = |\overline{(x_{i})}_{z=1} -
%  \overline{(x_{i})}_{z=0}|$,
%  $ i \leq k$, after conditioning on $\mathbf{X}$ in addition to
%  $\sum_{i} Z_{i}$.
Relatedly, \citet{lee2010regression} recommend a
 test for conditional association, given $R$, of $\mathbf{Z}$ and
 $\mathbf{X}$, by fitting models like those discussed in
 \S~\ref{sec:robust-analys-covar} for impact estimation, but with
 covariates rather than outcomes as independent variables.
Viewing the $R$-slopes and intercepts as simultaneously estimated
 nuisance parameters, these are balance tests applied to
 the covariates' residuals, %net of linear association with $R$,
rather than to the covariates themselves.

If there are multiple
covariates there will be several such tests. To summarize their
findings with a single p-value, the
regressions themselves may be fused within a
``seemingly unrelated regressions'' model \citep{lee2010regression};
however, to our knowledge, current software implementations do not
support the combination of linear and generalized linear
models, such as when covariates are of mixed type.
Alternatives include hierarchical Bayesian modeling
\citep{liMatteiMealli2015BayesianRD}, or combining
separate tests' p-values using the Bonferroni principle or
other multiple comparison corrections.

\subsubsection{The McCrary Density Test}\label{sec:mccrary}
McCrary's test for manipulation of treatment assignments
\citeyearpar{mccrary2008manipulation} can be understood as a %lack of
%association test applied over an $\{|R| < b\}$ window.
placebo test with the density of $R$ as the independent variable.
The test's
purpose is to expose the circumstance of subjects finely manipulating their
$R$ values in order to secure or avoid assignment to treatment. Absent
such a circumstance, if $R$ has a density then it should appear to be
roughly the same just below and above the cutpoint. McCrary's
\citeyearpar{mccrary2008manipulation} test statistic is the difference
in logs of two estimates of $R$'s density at 0, based on observations
with $R<0$ and $R>0$ respectively.
Manipulation is expected to generate a clump just beside the cut
point, on one side of it but not the other, and this in turn engenders
imbalance in terms of distance from the cut-point.

\subsubsection{Reducing the Bandwidth}\label{sec:bandwidth}
In practice, specification test failures inform sample exclusions.
When balance tests fail,
\citet{lee2010regression} would select a bandwidth $b>0$, restrict
analysis to observations with $R\in \mathcal{W} \subseteq [-b, b]$,
and repeat the test on $\{i : r_{i} \in \mathcal{W}\}$.
If that test fails, the process may be repeated with a new bandwidth
$b'<b$, and perhaps repeated again until arriving at suitable bandwidth.
This may seem to call for a further layer of multiplicity correction,
since any number of
bandwidths may have been tested before identifying a suitable
$b$% for which $H_{b}$ is sustained
; but it so happens that this form
of sequential testing implicitly corrects for multiplicity, according to the
sequential intersection union principle
(\citealp[SIUP;][, Proposition~1]{rosenbaum2008testing};
\citealp{hansenSales2015cochran}).
\citet{liMatteiMealli2015BayesianRD} and
\citet{cattaneo2014randomization} also suggest the use of covariate
balance to select a bandwidth.

Restricting analysis to data within a bandwidth may change the
interpretation of the result. The ATE and the
TOTE refer to a discrete population, and reducing the bandwidth likewise
reduces those populations---the new target populations consist of
subjects for whom $|R|\le b$.
(In contrast, the definition of the LATE is unaffected by bandwidth
choice.)

% Alternatively,
% \citet{cattaneo2014randomization} recommend testing hypotheses
% $H_{b_{i}}: {X} \independent {Z}| \{|R| < b_{i}\}$, $b_1 > b_2 > \cdots >0$, in sequence,
% proceeding until identifying  $b = \max \{ b_i: H_{b_{i}}$ is not rejected$\}$;
% the analytic sample is then restricted to  $\{i: r_{i} \in (-b,b)\}$.
% %This method's tests of each $H_{b_{i}}$ are neither covariance
% %adjusted nor multiplicity corrected.
% The analyst decides whether this test-and-reduce process
% should begin in the middle of $\mathcal{W}$ or at its boundaries.

Failures of the density test are addressed by restricting
estimation to observations with $|R|>a$, some $a \geq 0$
\citep[e.g.,][]{barrecaetal2011birthweightRDD,eggers2014validity}, and
repeating the test.
If this test rejects, we repeat the process with a new $a'>a$,
terminating the process when the p-value from the density test
exceeds a pre-set threshold.
By a second application of the SIUP,
the size of this test sequence is equal to the size of each individual
density test.
Taken together, placebo and McCrary tests restrict the sample to
$\mathcal{W} = (-b, b)$ or  $(-b, -a) \cup (a, b)$.

\subsubsection{Non-linear Models for Y as a function of R}\label{sec:nonlinear}
% \citep[\S~5]{lee2008regression, kolesarRothe17}.
%On the other hand,
The methods discussed in Sec~\ref{sec:robust-analys-covar} continue to
apply if $\EE (Y_{C}| R) = \alpha + R\beta$ is relaxed to
$\EE (Y_{C}| R) = \alpha + \bm{f}(R)\bm{\beta}$, for
$\bm{f}(\cdot)$ a $1 \times k$ vector valued function, and
$\bm{\beta}$ a $k \times 1$ vector of coefficients.
Unfortunately, if the model is fit by OLS, then such relaxation of assumptions can have the unwelcome
side effect of undercutting the robustness of the analysis. The
reasons have to do with mechanics of regression fitting.

% \marginpar{Should we also say something about local linear regression
%   here? -AS}
Polynomial specifications
$\EE(Y | R=r) = \sum_{j=0}^{J} r^{j} \beta_{j}$ are common but often
problematic; in combination with ordinary least squares fitting, they
implicitly assign case weights that can vary widely and
counterintuitively \citep{gelman2016high}.
This liability is already
in evidence with $J=1$, the linear specification, where leverage
increases with the square of $r -\bar{r}$.
If analysts select a bandwidth $b$ that is slightly too large, then
the analysis sample will include problematic observations near its outer
boundaries, precisely where leverage is at its highest.
If the analysis sample is contaminated near the cutpoint,
the bad data may not threaten linear specifications, but with $J>1$
they can still bear undue leverage.
In order to identify leverage points that are also influential,
OLS fitting is sometimes combined with specialized diagnostics such as
plots of Cook's \citeyearpar{cook1982residuals} distances.
Section~\ref{sec:robustFitters} will discuss an alternate remedy.


% \subsection{Robust Estimation of $\EE (Y_{C}| R)$}
% \label{sec:robust-altern-ordin}
% Let the running variable be so
% centered that the threshold lies at $r=0$, as in their application.
% Under the variant of \textsc{ancova} relaxing
% $Y_{C} = \alpha_{0} + \beta R + \epsilon$ to
% $Y_{C} = \alpha_{0} + \beta_{-} \indicator{R<0} R + \beta_{+}
% \indicator{R\geq 0}R  + \epsilon$,
% the $Z$-coefficient in an OLS regression of $Y$
% on the interaction of $Z$ and $R$ remains unbiased for
% $\lim_{r\downarrow 0} \EE(Y | R=r) - \lim_{r\uparrow 0} \EE(Y |
% R=r)$, as well as $\EE (Y_{T} - Y_{C} )$. (It also coincides with $\overline{\dt[\hat\theta]{{Y}}{
%   {R}}}_{Z=1} - \overline{\dt[\hat\theta]{{Y}}{ {R}}}_{Z=0}$, where
% $\dt[(a, b_{0}, b_{1})]{ \mathbf{y}}{ \mathrm{r}} = \mathbf{y} - a - (b_{0}
% \indicator{r < 0} + b_{1} \indicator{r\geq 0})\mathrm{r}$
% and $\hat\theta = (\hat\alpha, \hat{\beta}_{-}, \hat{\beta}_{+})$ are
% as estimated under the interaction model.)



\section{Randomness and Regression in RDDs}\label{sec:theMethod}

% %% RDDs differ from RCTs in an important way:
% %% $Y_C$ typically correlates with $R$, and is therefore not independent
% %% of $Z$; Strong Ignorability \eqref{eq:ignore} is violated.
% %% Can it be relaxed to an assumption that is
% %% simultaneously plausible in RDDs and compatible with similar methods?

% When circumstances call for generating random numbers, such as in
% RCTs or Monte Carlo methods, statisticians often settle instead for
% pseudo-random numbers, deterministic
% outputs of a complex algorithm and an initial seed; a person in
% possession of that seed can exactly predict the pseudo-random draws.
% ``True'' random numbers, on the other hand, derive from physical
% processes that are either chaotic \citep[e.g.][]{uchida2008fast} or
% quantum \citep[e.g.][]{stefanov2000optical}, and are not
% predictable. However, these processes often contain an element
% of predictability; % \citep{raz2005extractors};
% true random
% numbers are obtained only after separating predictable from
% non-predictable components \citep[see, e.g.][]{Nisan1999148}.%,vadhan2012pseudorandomness}.

% Similarly, RDD analysis decomposes $Y_{T}$ and $Y_C$ as the sum of systematic
% components, a function of $R$, and disturbances, relying on the nature of the decomposition to ensure
% that, vis a vis one another, the disturbances and $Z$ may
% be regarded as random.
% % There is no need
% % to assume the systematic component to equal or be consistent for
% % $\EE (Y_{C}| R)$, nor to make further distributional assumptions on
% % the disturbances.

The analysis of \S~\ref{sec:maria}
mounted an analogy between the Hurricane Maria RDD and a hypothetical
RCT, but only after a preparatory step of modeling and removing the
outcomes' non-random component.
In \S~\ref{sec:maria}, these two steps used two different
datasets---we regressed $Y$ on $R$ using data from years prior to
2017, when Maria hit, and then used 2017 data to estimate effects,
under the assumption of residual ignorability, \eqref{eq:ignore2}.
This luxury is unavailable in the typical RDD, in which both steps
must use the same data, as in \S~\ref{sec:robust-analys-covar}.
This section will describe a generalization of residual ignorability
\eqref{eq:ignore2} to the typical case, along with robust analysis techniques
incorporating the specification tests reviewed in \S~\ref{sec:specification}.

\subsection{An Analytic Model for RDDs} \label{sec:model-eey-c-r}

This section will formalize residual ignorability for the typical RDD,
which relies on a single dataset including variables $Y$, $R$, and $Z$.
The assumption is that, after a suitable residual transformation,
potential outcomes $Y_C$ are conditionally independent of $Z$.
Hence, causal inference in an RDD may take the perspective
that $Z$ is random due to randomness in $R$.

Suppose the statistician to have selected a \textit{detrending procedure}: a
trend fitter, i.e. a function of
$\{({y}_{i},d_{i},r_{i})\}_{i=1}^{n}$ returning
fitted parameters $\hat{\theta}$ in a sufficiently regular
fashion, along with a
family $\{\dt{\cdot}{\cdot}: \theta\}$ of residual or partial
residual transformations, each mapping data $(\mathbf{y}, \mathbf{r})$ to
residuals
$\{\dt {y_{i}}{{r}_{i}} \}_{i=1}^{n}$.
Appendix~\ref{sec:large-sample-rand}
%\citet[\S~\ref{sec:large-sample-rand}]{lrdauthors:supp}
states the
needed regularity condition, which is ordinarily met by OLS and always
met with our preferred fitters (\S~\ref{sec:robustFitters}).
Then, causal inference in an RDD proceeds from the following assumption:
\begin{ass}{Residual Ignorability}
\sloppy
Given a detrending procedure $(\hat{\theta}, \dt{y}{r})$,
\begin{equation}\label{ycheck}
\dt[\thetaInf]{Y_{C}}{ R }%\mathbf{\Ych}_{C}
\independent {Z}| \{R \in \mathcal{W}\}.
\end{equation}
Here $Z = f(R)$, with probability 1, for some deterministic
$f$ (such as $f(r) = \indicator{r < 0}$);  $\thetaInf$ is a constant such that
$\sqrt{n}(\hat\theta - \thetaInf)$ is bounded in probability (and thus
$\hat\theta \stackrel{P}{\rightarrow} \thetaInf$); $\mathcal{W}$
satisfies $\PP( R \in \mathcal{W}) >0$ and $0< \PP(Z=1|R\in\mathcal{W})<1$.
\end{ass}
Residual ignorability states that, though $Y_C$ may not be independent of
$Z$,  it admits a residual transformation bringing about such
independence. With $\dt[\hat\theta]{Y_{C}}{ R}$ a suitable
partial residual, residual ignorability is entailed by the
\textsc{ancova} model (\S~\ref{sec:robust-analys-covar}), or by the combination of any parametric model
for $\EE (Y_{C}| R)$ with a strict null $H$ relative to which the
value of $Y_{C}$ can be reconstructed from the values of $Y$, $D$ and
$Z$ (\S~\ref{sec:using-eqref-test}).
(In either of these cases $\dt[\thetaInf]{Y_{C}}{ R}$
is independent not only of $Z$ but also $R$,
a modest strengthening of~\eqref{ycheck}.)


\sloppy
Assuming residual ignorability, inference about treatment effects is
made conditionally, on
$\mathbf{A}= (\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$, $\mathbf{D}_{T},
\{(Y_{Ti}, Y_{Ci}, D_{Ti}, R_{i}) \indicator{{R}_{i} \not\in
  \mathcal{W}}\}_{i=1}^{n})$.
Conditioning on the full data vector when $R \not\in \mathcal{W}$
excludes observations for which \eqref{ycheck} is not assumed.
Conditioning on
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$
removes little of the randomness %entropy
in $\mathbf{R}$, leaving it available as a basis for inference.
Uncoupled to $Y_{T}$'s, the detrended  $Y_{C}$'s,
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$,
are in themselves uninformative about $\EE(Y_{T} - Y_{C})$, so
the variables comprising $\mathbf{A}$ are jointly
%\marginpar{$\leftarrow \Delta$ to $(\mathbf{Y}_{C}, N_{0}, N_{1})$?}
%S-
ancillary, just as $\bm{A}^*$ %$\mathbf{A}^{\dagger}$
was seen to be
in Section~\ref{sec:using-eqref-test}. As in Fisher-style randomization inference for RCTs, some conditioning variables are
unobserved; but this is not an impediment, at least for large-sample
inferences.

Causal inference based on residual ignorability takes place
in four steps: (1) choosing and validating the analysis sample or
bandwidth, (2) choosing an appropriate fitting procedure (we recommend
robust fitters), (3) treatment effect estimation and inference, and
(4) post-fitting diagnostics.
We will discuss each of these steps in sequence.

\subsection{Pre-Fitting Diagnostics and Bandwidth Choice}
\label{sec:bandwidthChoice}

If subject matter knowledge suggests that the ATE or TOTE would be
most relevant for subjects with $|R|\le b$, then $b$ might form an an
initial bandwidth choice. But it
is also sensible to subject this choice to specification
testing (\S~\ref{sec:specification}).

Balance tests in an an RDD (\S~\ref{sec:balanceTesting})
can be regarded as testing residual ignorability with a multivariate
``outcome'' $Y^{*}$ combining the actual outcome $Y$ with covariates $X$---\eqref{ycheck} with
$\mathbf{Y}_{C}^{*} = (\mathbf{X}, {Y}_C)$ in place of $Y_{C}$.

This calls for preliminary detrending procedures, mechanisms to
decompose  $X$ into components that are systematic or unpredictable,
vis a vis $\mathbf{R}$, just as ${\mathbf{Y}_C}$ will later be decomposed.
%% Since $Z$ cannot have an effect on a pre-treatment
%% covariate, the preliminary step of reconstructing
%% $x_{C}$ is not needed.
%%% [Reorg obviated need for the above, as this passage now precedes
%%% discussion of reconstructing Y_C]
Our analysis of the LSO data posits systematic components that are
linear and logistic-linear in $R$, depending on whether $X$ is
a measurement or binary variable. %, paralleling its choice of a linear decomposition
% for the primary outcome, subsequent GPA. (It is fitting that the $X$- and
% $Y$-specifications mirror each other in model complexity.)
The placebo check adds $Z$ to the specification and tests whether its
coefficient is zero. We implement these checks as Wald tests with
heteroskedasticity-robust standard errors, as in
\S~\ref{sec:robust-analys-covar}, using the Bonferroni method to
combine placebo checks across covariates.
To ensure adequate power to detect misspecification,
%the Bonferroni method,
we test at level $.15$, not $.05$.

We use sequential balance tests to adjust the bandwidth $b$, alongside
McCrary density tests to further
refine the analysis sample $\mathcal{W}$ (\S~\ref{sec:bandwidth}).
These specification tests rely on covariates, $R$, and $Z$, but not on
$Y$; therefore, selection of $\mathcal{W}$ is objectivity-preserving
in the sense of \citet{rubin2007design}.
% yielding a test of the
% hypothesis that all of the covariates' residuals are jointly ignorable within the
% given $\mathcal{W}$.

% If the first window tested has form $\mathcal{W} = (-b, b)$,
% write $H_{b}$ for the corresponding joint ignorability hypothesis.
% If the $R$-adjusted covariate placebo tests reject $H_{b}$ then the process is repeated for
% $\mathcal{W}' = (-b', b')$, some
% $b' < b$, and perhaps repeated again if $H_{b'}$ also is rejected.
%  To compensate for conservatism of
% the Bonferroni method, we test with size
% $\alpha_{B}=.15$, not $.05$.

% Writing $b$ for the half-width selected in this manner, i.e. $b=\max$
% \{$b_{i}: H_{b_{i}}$ is not rejected\}, we next apply a McCrary
% manipulation test to $(-b,b)$. within windows
% $\{ i: |R_{i}| \in (a_{j}, b)\}$, $0 = a_{0} < a_{1}< \cdots < b$,


\subsection{Robust Fitters}\label{sec:robustFitters}
Observations in the analysis sample that do not satisfy residual
ignorability %but include influential observations
 can undercut the validity of an RDD analysis.
Even moderate amounts of such contamination---specifically, contamination of
a $O(n^{-1/2})$-sized
share of the sample that happens to contain influential
observations---can defeat OLS-based estimation strategies, rendering
them inconsistent.
Indeed, even some robust regression methods---those engineered to meet
objectives other than bounding the influence function---may be misled
\citep{stefanski1991note}.
The inclusion of problematic observations in the analysis sample can
be due to misspecification of the model for $\EE (Y_{C}|R)$
(\S~\ref{sec:nonlinear}), manipulation of treatment assignments
(\S~\ref{sec:mccrary}), or other violations of residual ignorability,
coupled with the failure of specification tests to detect these problems.
However, no specification test is powerful enough
to reliably detect moderate contamination; if the probability of a
false alarm is controlled, then power to detect anomalies
affecting only $O(n^{-1/2})$ of the sample can only tend to a number
strictly less than 1.
However $b$ and $\mathcal{W}$ are selected, at least some
% (cf. \S~\ref{sec:bandwidth}, \S~\ref{sec:bandwidthChoice}, below),
contamination may remain in the sample.

Accordingly, consistent estimation of $\thetaInf$ requires robust
M-estimators, in Yohai and Zamar's
\citeyearpar{yohaiZamar1997locallyrobustMestimates} sense, a class
excluding maximum likelihood estimation while including
modern MM-,  SM-, and other
estimators with bounded influence function \citep[see also][, Thm.~3]{he1991localbreakdown}.
 In MM-estimation as in OLS,
coefficients $\bm{\beta}$ of a linear specification solve estimating equations
$\sum_{i} \psi\left\{ ({y}_{i} -
\bm{x}_{i}^T\bm{\beta})/s \right\} \bm{x}_{i}^T =\bm{0}$, where $s>0$ and
$\psi(\cdot)$ is an odd function satisfying $\psi(0)=0$,
$\psi'(0)=1$, and $t\psi(t)\geq 0$; bounded influence fitters replace OLS's $s\equiv 1$ with resistant preliminary
estimates of residual scale and OLS's $\psi(t) = t$ with a continuous $\psi$
that satisfies $\int_{0}^{\infty}\psi(t)dt < \infty$. This limits
the loss incurred by the fitter for failing to adapt itself to a small
portion of aberrant observations;
it is permitted to systematically down-weight them instead.


The analyses and simulations presented below use MM-estimators with bisquare $\psi$ and
``fast S'' initialization \citep{salibian-barreraYohai2006fastS}.
We are not aware of prior work addressing
potential contamination of an RDD sample with the assistance of
bounded influence MM-estimation.
Surprisingly, given their common origins in %the work of
Huber \citeyearpar{huber1964robust}, MM-estimation is not routinely
paired with sandwich estimates of variance, as in
\S~\ref{sec:sandwich} above.
Exceptions include Stata's \texttt{mmregress} and R's \texttt{lmrob},
which optionally provide Huber-White standard errors
\citep{verardiCroux2009robust,rousseuwetal2015robustbase}; our
analyses use the latter.



% : with fitting via OLS,
% this correspondence would be exact, as noted in Section~\ref{sec:robust-analys-covar}; with the robust MM-estimation we
% favor, the correspondence is one of large-sample equivalence
% %\citep[Section~\ref{sec:suppl-s-refs}]{lrdauthors:supp}
% (Appendix~\ref{sec:suppl-s-refs}).

\subsection{Treatment Effect Estimation and Inference}
\label{sec:test-hypoth-no}

For inference about $\tau$ under the model
$Y_{T} = Y_{C} + \tau D_{T}$, select a specification
$\mu_{\beta}(\cdot)$ for $\EE(Y_{C}| R)$ %, %in the application of
%Section~\ref{sec:appl-effect-acad}
such as the
linear model $\mu_{\beta}(R) =\alpha + R\beta$, a window of
analysis $\mathcal{W}$, and a fitter.

Then, separately for each hypothesis $H: \tau=\tau_0$ under
consideration, calculate
$\mathbf{y}_{H} = \mathbf{y} - \mathbf{d}\tau_{0}$, and
apply the chosen specification and fitter to
$(\mathbf{y}_{H}, \mathbf{r})$.
The combination of the data, the
model fit, and the residual transformation $\dt{\cdot}{\cdot}$ give rise to residuals
$\dt[\hat\theta]{\mathbf{y}_{H}}{\mathbf{r}}$, completing the
detrending procedure. Whether $H$ is rejected or sustained is
determined by the value of the sandwich-based \textsc{ancova} $t$-statistic
in \S~\ref{sec:sandwich}.

In practice it is expedient to use a near-equivalent
test by modifying the detrending
procedure.
When regressing $Y_{H}$ on $R$, include an additive
contribution from $Z$, so that $\mu_{\theta}(R) =\alpha +
R\beta$ is replaced with $\mu_{(\theta,\gamma)}(R) =\alpha +
\beta R + \gamma Z$. With sandwich estimates of
$\text{Cov}\{(\hat{\theta}_{H}, \hat{\gamma}_{H})\}$,
the $t$-ratio comparing $\hat{\gamma}_{H}$ to
$\text{SE}_{s}(\hat{\gamma}_{H})$ induces a generalized score test \citep{boos1992genscoretest}. Implicitly it is a two-sample
$t$-statistic with covariance adjustment for $R$ (with fitting via OLS,
this correspondence would be exact, as noted in Section~\ref{sec:sandwich}; with the robust MM-estimation we
favor, the correspondence is one of large-sample equivalence; see Appendix~\ref{sec:suppl-s-refs}).

As in \S~\ref{sec:robust-analys-covar}, the corresponding
M-estimate of the CACE is the value of $\tau_{0}$ making
$\hat{\gamma}_{H}/\text{SE}_{s}(\hat{\gamma}_{H})$ equal 0; those $\tau_{0}$ for
which $H: \tau = \tau_{0}$ is not rejected at level $\alpha$
constitute a $100(1-\alpha)\%$ confidence interval. Iteration is
facilitated by regressing $\mathbf{y}$ on $\mathbf{r}$ and
$\mathbf{z}$ with offset variable $\mathbf{d}\tau_{0}$; then only the
offset needs to be modified to test $H: \tau = \tau_{1}$,
$\tau_{1}\neq \tau_{0}$.

Strictly speaking, this estimation procedure relies on the assumption of a constant
additive treatment effect, so that
$Y_T=Y_C+D\tau_0$, for some constant $\tau_0$ (or, more generally, an
exact model for treatment effects under which $Y_C$ could be recovered
without error).
This requirement is typical of estimators derived from the inversion
of hypothesis tests \citep[e.g.][]{imbens:rose:2005}.
However, due to its use of sandwich standard errors
$\text{SE}_{s}(\cdot)$, our M-estimator is robust to some
departures from this assumption.
Specifically, under \eqref{ycheck}, $\hat\tau$ is consistent for $\tau_0$ solving
\begin{equation} \label{eq:concl1}
\EE \big\{\dt[\thetaInf]{Y_T-D_T\tau_0}{ R } \big| R \big\}
\equiv \EE \big\{ \dt[\thetaInf]{Y_{C}}{ R } \big| R \big\},
\end{equation}
providing such a $\tau_0$ exists.
That is, we assume the existence of a constant $\tau_0$ such that detrended $Y_H=Y-D\tau_0$ is
equal to detrended $Y_C$ on average, if not exactly.
The simulation study in \S~\ref{sec:levelPower} bears out this
robustness property.


\subsection{Post-Fitting Diagnostics} \label{sec:post-fitt-diagn}
Once the M-estimate for the treatment effect has been found, one
inspects the corresponding regression fit for points of high influence.
Robust MM-regression is helpful here. Besides making
influential points easier to see in residual plots, it limits
effects of data contamination, as non-conforming influence points are
down-weighted as a result of the fitting process. This down-weighting
is reflected in ``robustness weights,'' ranging from 1, for non-discounted
observations, down to 0, for the most anomalous observations.
Plotting % As
% an indicator of robustness to non-constant effects.
% (But is this overloading the discussion here?)
robustness weights against residuals may expose opportunities to
improve the fit of $\mu_{\theta}(R)$, or of the treatment effect model;
plotting them against $R$ may expose contaminated sub-regions
of $\mathcal{W}$ that specification testing failed to remove
\citep{maronna2006robust}.