\section{Selected Parametric and Robust RDD Methods}
Capturing the local randomization essence of the RDD calls for
a merger of RDD methods with concepts developed for RCTs. This
section selectively reviews relevant literatures.

Let $Z \in \{0,1\}$ indicate assignment to treatment ($Z=1$) as opposed to control
($Z=0$).  For an RDD one defines $Z \equiv \indicator{R< 0}$,
$\indicator{R \leq 0}$, $\indicator{R\geq 0}$ or $\indicator{R > 0}$,
depending on how intervention eligibility relates to the threshold,
where $\indicator{x}=1$ if $x$ is true and $0$ otherwise.
Let $Y$ represent the outcome of interest.
For simplicity assume non-interference, the model that
a subject's response may depend on his but not also on other subjects'
treatment assignments \citep{cox:1958,rubin:1978}.  Thus we may take each $i$
to have two potential outcomes, $y_{Ti}$ and $y_{Ci}$, at most one of which is observed, depending on whether $z_i=1$ or $0$, respectively;
 observed
responses $Y$ coincide with $ZY_{T}+(1-Z)Y_{C}$.

\subsection{The \textsc{Ancova} Model for RDDs}\label{sec:robust-analys-covar}

The classical analysis of covariance (\textsc{ancova}) model for
groups $i=1,\ldots, k$, each including subjects $j=1, \ldots, n_{i}$,
says that
$Y_{ij} = \alpha_{i} + \beta X_{ij} + \epsilon_{ij}$, where $\epsilon_{ij}
\sim \mathrm{Normal}(0, \sigma^{2})$ is independent of the continuous
covariate $X_{ij}$.
In the classical development of RDDs, \textsc{ancova} with $k=2$
groups---treated and untreated---is a leading option among statistical
models
\citep{thistlethwaite1960regression}.
A potential outcomes version of the model is
 $Y_{Ci} = \alpha_{0} + \beta R_{i} + \epsilon_{Ci}$ and
$Y_{Ti} = \alpha_{1}  + \beta R_{i} + \epsilon_{Ti}$, with
 $\epsilon_{Ci} \sim \mathrm{Normal}(0, \sigma^{2})$ and
 $\epsilon_{Ti} \sim \mathrm{Normal}(0, \sigma^{2})$.
In marked contrast to RCTs, it does
not require that $(Y_{T}, Y_{C}) \independent Z$: to the contrary, both
$Y_{C}$ and $Y_{T}$ are presumed to associate with $R$, which in turn
determines $Z$.
Nonetheless, under this model the estimated $Z$ coefficient from the
model
\begin{equation}\label{eq:classicOLS}
Y_i=\alpha+\beta R_i+\tau Z_i+\epsilon_i
\end{equation}
fit using ordinary least squares (OLS), is unbiased for
$\alpha_{1} - \alpha_{0}$.
This estimand is the value of  $\lim_{r\downarrow 0} \EE(Y | R=r) -
\lim_{r\uparrow 0} \EE(Y | R=r)$ and, simultaneously,
limitless estimation targets such as $\EE Y_{T}  - \EE Y_{C}$.

The OLS approach estimates $\tau$ as a parameter in regression model
\eqref{eq:classicOLS}. In contrast, the approach in
\S~\ref{sec:using-eqref-test} took place in two separate steps:
first adjusting outcomes for $R$ and then testing hypotheses using a test
statistic that contrasts treated and untreated subjects.
However, the OLS estimator may be restated in the manner of \S~\ref{sec:using-eqref-test}.
Consider the hypothesis $H: Y_{T} = Y_{C} + \tau$.
Next, define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H$, $Y_H=Y_C$)
and residuals $\resid\equiv\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
b\mathbf{r}$.
Finally, test $H$ with statistic
\begin{equation*}
t(\mathbf{Y}_H , \mathbf{Z}) =
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0},
\end{equation*}
where $\hat{\alpha}$ and $\hat{\beta}$ are the standard OLS estimates.
Because of the structural relationship
between $R$ and $Z$, (and, unlike in
\S~\ref{sec:using-eqref-test}, regression parameters were not estimated
with a separate dataset)
permutation tests based on this statistic are not tractable.
However, under the parametric \textsc{ancova} model, with
conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
\mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
$t(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
variance equal to the classical OLS variance of the coefficient on
$Z$.

% \S~\ref{sec:using-eqref-test}.
% Consider the hypothesis $H_\tau: Y_{T} = Y_{C} + \tau$.
% Next, define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H_\tau$, $Y_H=Y_C$)
% and residuals $\resid\equiv\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
% b\mathbf{r}$.
% Finally, test $H_\tau$ with statistic
% \begin{equation*}
% t(\mathbf{Y}_H , \mathbf{Z}) =
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0},
% \end{equation*}
% where $\hat{\alpha}$ and $\hat{\beta}$ are the standard OLS estimates.
% Because of the structural relationship
% between $R$ and $Z$, (and, unlike in
% \S~\ref{sec:using-eqref-test}, regression parameters were not estimated
% with a separate dataset)
% permutation tests based on this statistic are not tractable.
% However, under the parametric \textsc{ancova} model, with
% conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
% \mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
% $t(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
% variance equal to the classical OLS variance of the coefficient on
% $Z$.

In general, the set \{$c$:
$H_{c}$ is not rejected at level $\alpha$\}, which can be seen to be an interval, is a
$100(1-\alpha)\%$ confidence set for $\tau$; the $c$ solving
$t_{u}(\mathbf{y}-c\mathbf{d}, \mathbf{z}) = 0$, which can be seen to be unique, is a
corresponding point estimate.
In fact, estimate for $\tau$ corresponding to these statistical tests is
algebraically equal to the $Z$-coefficient from an OLS estimate of
\eqref{eq:classicOLS}, and the two-sided 95\% confidence interval induced in this
manner is the familiar
$\hat{\tau} \pm 1.96\, \mathrm{SE}(\tau)$.
These equivalences, however, do not necessarily extend to estimation
strategies outside of OLS.

\subsubsection{Robust Standard Error Estimation}\label{sec:sandwhich}
The \textsc{ancova} model for $(Y_{T}, Y_{C})$ is not readily dispensed with, but it
may be relaxed.  OLS estimates of $\alpha_1-\alpha_{0}$ and
$\beta$ remain unbiased under non-Normality, provided
the $\epsilon$s have expectation 0 and bounded variances.  The
ordinary \textsc{ancova} standard error does not require Normality of
the $( \epsilon_{i}: i )$, either, for use in large samples, although
it does require that they have a common variance.
To test
$\EE\{ \dt[\hat\theta]{{Y_H}}{ R} | Z=1\} = \EE\{ \dt[\hat\theta]{{Y_H}}{
R} | Z=0\}$
under potential heteroskedasticity, one estimates
$\var\left\{t(\mathbf{Y}_H , \mathbf{Z})\right\}$
using a sandwich or Huber-White estimator,
$\mathrm{SE}_{s}^{2} \left\{ t(\mathbf{Y}_H , \mathbf{Z}) \right\}$
\citep{huber1967behavior,mackinnonWhite1985sandwichHC,longErvin2000sandwichHC,
bellmccaffrey2002sandwichSEs,pustejovskyTipton2017sandwichSEs}, %as opposed to the %$\mathrm{SE}_{p}$; then
%$\mathrm{SE}_{u} %( \overline{e({Y_H}| R)}_{Z=1} -  \overline{e({Y_H}| R)}_{Z=0} )$
%of \eqref{eq:tudef}:
and refers $t(\mathbf{Y}_H,\mathbf{Z})/\mathrm{SE}_{s}$ 
% \begin{equation} \label{eq:tedef}
% t_{e} (\mathbf{y}_H,\mathbf{r}) =
% \frac{\overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=1} -
%                              \overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=0}}%
%                            {\mathrm{SE}_{s}\left\{ \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=1} -
%                              \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=0}
%                            \right\} }
% \end{equation}
to a $t$ or standard Normal reference distribution.
Sandwich standard errors confer robustness to misspecification of
$\var(\dt[\hat\theta]{Y_{H}}{R}\mid  R)$, not of $\EE(Y_{H}| R)$ \citep{freedman2006sch}, the latter being the topic of
Section~\ref{sec:robust-altern-ordin}.

% In applying \textsc{ancova} to an RCT, $\mathrm{SE}_{s}$ in
% \eqref{eq:tedef} may be taken as \eqref{eq:sudef}'s
% $\mathrm{SE}_{u}$% \citep{samii2012equivalencies}
% , a more straightforward quantity to compute; unfortunately, in RDDs
% this is not the case
% (Section~\ref{apnd:requ-forpr-eqref}).  On the
% other hand, in \textsc{ancova} as applied to RDDs, if standard
% errors are calculated by the sandwich method then under
% $H: Y_{T} = Y_{C}$, \eqref{eq:tedef} is algebraically equivalent to the
% $t$-statistic of contrast $\hat{\alpha}_{1} -\hat{\alpha}_{0}$.

\subsubsection{Fuzzy RD}
So-called ``Fuzzy'' RD describes non-compliance---when subjects' actual
treatment differed from $Z$.
In those cases, let $D$ indicate whether treatment was actually
received.
This $D$ is an intermediate outcome, so there are
corresponding potential outcomes $D_{C}$ and $D_{T}$, with $D \equiv ZD_{T}
+ (1-Z)D_{C}$.
Subject $i$ is a non-complier if $D_{Ci}=1$ or $D_{Ti}=0$, though we
will assume the monotonicity condition $D_{C}\equiv 0$ --- there may be
subjects assigned to the treatment who avoid it, but no one gets
treatment without being assigned to it.
We shall also posit the exclusion restriction,
that $Z$ influences $Y$ only by way of its effect on $D$
\citep{bloom1984ans,Angrist:etal:1996,imbens:rose:2005}.
Our focus of estimation is the
 ``treatment-on-treated'' effect (TOTE),
%complier
%average causal effect (CACE),
$\EE(Y_{T} - Y_{C}| D_{T}=1)$.

Statistical hypotheses about the TOTE take the form
$H_\tau:Y_T=Y_C+D\tau$.
To test $H_\tau$ under non-compliance, let $Y_H=Y-\tau D$,
construct test statistic $t_{e} (\mathbf{y}_H,\mathbf{r})$ as in
\S~\ref{sec:sandwich}, and compare its value to a standard normal
distribution.
That is, the only difference between ``strict'' RD analysis, in which
compliance is perfect, and fuzzy RD, is in the formulation of
hypothesis $H$, and the construction of $Y_H$---the rest of the
process remains unchanged \citep{rosenbaum:1996:onAIR}.

As compared to Wald-type
confidence intervals, that is intervals of form $\hat\tau \pm q_{*}
\mathrm{SE}(\hat\tau)$ with $\mathrm{SE}(\hat\tau)$ a single,
hypothesis independent quantity, under partial compliance this iterative method
% of separate evaluation of $\mathrm{SE}_{u} \left\{
%   \overline{({y}_{H_{\tau}})}_{Z=1} -
%   \overline{({y}_{H_{\tau}})}_{Z=0} \right\}$ for each $H_{\tau}$,
improves correspondence of nominal and actual confidence levels
% , particularly if the ``instrument,'' $Z$, is ``weak,''
% i.e. $\PP(D_{T} =1) \approx \PP(D_{C} =1)$
\citep[Sec.~7]{imbens:rose:2005,baiocchiChengSmall2014IVtutorial}.

\subsection{Misspecification, Contamination, and Their Remedies}\label{sec:specification}

The \textsc{Ancova} model for RDDs encodes additional assumptions,
beyond normality and homoskedasticity of regression errors and full
compliance with treatment assignment, which are not so easily
dispensed with.
% Perhaps most worryingly, the form of $\EE[(Y_{T}, Y_{C}) | R]$ may be
% misspecified, and the integrity of the RDD could be compromised by
% subjects who finely manipulate their values of $R$ around the cutoff
% in order to achieve or avoid a treatment assignment. 
Methodological RDD literature has responded with specification tests
to detect these threats, or with flexible or robust estimators that
seek to avoid them. 

\subsubsection{Covariate Balance Tests}
Analysis of RCTs and quasiexperiments often hinges on assumptions of
independence of
 $\mathbf{Z}$ from $(\mathbf{X}, \mathbf{Y}_{C}, \mathbf{Y}_{T})$.
%% Analogous statements will hold for more complex randomization or
%% ignorability assumptions as well.
 Although neither $\mathbf{Z} \independent \mathbf{Y}_{C}$ nor
 $\mathbf{Z} \independent \mathbf{Y}_{T}$ can be directly tested,
 since potential outcomes are only partly observed, assumptions of form
 $\mathbf{Z} \independent \mathbf{X}$ are falsifiable: researchers can
 conduct placebo tests for effects of $Z$ on $X$.
Of course, treatment cannot affect pre-treatment variables; this is
model-checking (%\citealp{bayarriBerger2000pvalues};
\citealp[][\S~5.13]{cox2006pos}%; \citealp[][Ch.~6]{gelman:etal:2004};
%\citealp[][Ch.~14]{lehmannRomano2006TSH}
).  % Here as
 % elsewhere \citep{box1980sab}, checks finding fault with a model prompt
 % refinement, not abandonment; refinements typical of RDD analysis remove portions of
 % the sample that are suspected of contamination.

 Writing in the RDD context, \citet{cattaneo2014randomization} test
 for marginal associations of $\mathbf{Z}$ with covariates $\mathbf{X}_{i}$,
 $i=1, \ldots, k$, using the permutational methods that are applied
 in Fisherian analysis of RCTs \citep[also see][]{liMatteiMealli2015BayesianRD}.
% Specifically, they
%  obtain p-values though \eqref{eq:01}, with test statistics
%  $t(\mathbf{x}_{i}, \mathbf{z}) = |\overline{(x_{i})}_{z=1} -
%  \overline{(x_{i})}_{z=0}|$,
%  $ i \leq k$, after conditioning on $\mathbf{X}$ in addition to
%  $\sum_{i} Z_{i}$.
Relatedly, \citet{lee2010regression} recommend a
 tests for conditional association, given $R$, of $\mathbf{Z}$ and
 $\mathbf{X}$, by fitting models like those discussed in
 \S~\ref{sec:robust-analys-covar} for impact estimation, but with
 covariates rather than outcomes as independent variables%
Viewing the $R$-slopes and intercepts as simultaneously estimated
 nuisance parameters, these are balance tests applied to
 the covariates' residuals, %net of linear association with $R$,
rather than to the covariates themselves.

If there are multiple
covariates there will be several such tests. To summarize their
findings with a single p-value, the
regressions themselves may be fused within a
``seemingly unrelated regressions'' model \citep{lee2010regression} or
hierarchical Bayesian model 
\citep{liMatteiMealli2015BayesianRD}, or the
separate tests' p-values could simply be combined using the Bonferroni
principle. 

\subsubsection{The McCrary Density Test}
McCrary's test for manipulation of treatment assignments
\citeyearpar{mccrary2008manipulation} can be understood as a %lack of
%association test applied over an $\{|R| < b\}$ window.
placebo test with the density of $R$ as the independent variable.
The test's
purpose is to expose the circumstance of subjects finely manipulating their
$R$ values in order to secure or avoid assignment to treatment.  Absent
such a circumstance, if $R$ has a density then it should appear to be
roughly the same just below and above the cutpoint.  McCrary's
\citeyearpar{mccrary2008manipulation} test statistic is the difference
in logs of two estimates of $R$'s density at 0, based on observations
with $R<0$ and $R>0$ respectively.
Manipulation is expected to generate a clump just beside the cut
point, on one side of it but not the other, and this in turn engenders
imbalance in terms of distance from the cut-point.

\subsubsection{Reducing the Bandwidth}
In practice, specification test failures inform sample exclusions.
Failures of the density test are addressed by restricting
estimation to observations with $|R|>a$, some $a \geq 0$
\citep[e.g.,][]{barrecaetal2011birthweightRDD,eggers2014validity}.
When balance tests fail,
\citet{lee2010regression} would select a window
$\mathcal{W} \subseteq [-b, b]$, $b>0$, % using
and repeat the test on $\{i : r_{i} \in \mathcal{W}\}$. Similarly,
\citet{cattaneo2014randomization} recommend testing hypotheses
$H_{b_{i}}: {X} \independent {Z}| \{|R| < b_{i}\}$, $b_1 > b_2 > \cdots >0$, in sequence,
proceeding until identifying  $b = \max \{ b_i: H_{b_{i}}$ is not rejected$\}$;
the analytic sample is then restricted to  $\{i: r_{i} \in (-b,b)\}$.
%This method's tests of each $H_{b_{i}}$ are neither covariance
%adjusted nor multiplicity corrected.
The analyst decides whether this test-and-reduce process
should begin in the middle of $\mathcal{W}$ or at its boundaries.

With finite samples, specification tests may be under-powered, and
fail to detect some small violations of RDD assumptions or (more
troublingly) large violations of the assumptions among relatively
small subsets of the analysis sample. 
Indeed, sample pruning guided by specification tests is expected to
leave an $O(n^{-1/2})$-sized share of the sample that perfect
information would have counseled removing. 
Depending on the outcome and running variable values of members of
these problematic subsets, their inclusion in the RDD analysis sample
may have undue influence on the treatment effect estimate. 

\subsubsection{Non-linear Models for Y as a function of R}
% \citep[\S~5]{lee2008regression, kolesarRothe17}.
%On the other hand, 
The methods discussed in Sec~\ref{sec:robust-analys-covar} continue to
apply if $\EE (Y_{C}| R) = \alpha + R\beta$ is relaxed to
$\EE (Y_{C}| R) = \alpha + \vec{f}(R)\beta$, for
$\vec{f}(\cdot)$ a $1 \times k$ vector valued function.
Unfortunately, if the model is fit by OLS, then such relaxation of assumptions can have the unwelcome
side-effect of undercutting the robustness of the analysis.  The
reasons have to do with mechanics of regression fitting.

% \marginpar{Should we also say something about local linear regression
%   here? -AS}
Polynomial specifications
$\EE(Y | R=r) = \sum_{j=0}^{J} r^{j} \beta_{j}$ are common but often
problematic; in combination with ordinary least squares fitting, they
implicitly assign case weights that can vary widely and
counterintuitively \citep{gelman2016high}.
This liability is already
in evidence with $J=1$, the linear specification, where leverage
increases with the square of $r -\bar{r}$.  With an analysis sample
of the form $\{i : R_{i} \in \mathcal{W}\}$ for a ``window''
$\mathcal{W} \ni c$, if $\mathcal{W}$ is slightly too wide then the
sample is contaminated near its outer boundaries, precisely
where leverage is at its highest.
This problem is only partially counteracted by local linear or polynomial regression
\citep[e.g.][]{imbens2012optimal,cct}, 
which downweights observations as $|R-c|$ increases linearly. 


\subsection{Robust Estimation of $\EE (Y_{C}| R)$}
\label{sec:robust-altern-ordin}
% Let the running variable be so
% centered that the threshold lies at $r=0$, as in their application.
% Under the variant of \textsc{ancova} relaxing
% $Y_{C} = \alpha_{0} + \beta R + \epsilon$ to
% $Y_{C} = \alpha_{0} + \beta_{-} \indicator{R<0} R + \beta_{+}
% \indicator{R\geq 0}R  + \epsilon$,
% the $Z$-coefficient in an OLS regression of $Y$
% on the interaction of $Z$ and $R$ remains unbiased for
% $\lim_{r\downarrow 0} \EE(Y | R=r) - \lim_{r\uparrow 0} \EE(Y |
% R=r)$, as well as $\EE (Y_{T} - Y_{C} )$.  (It also coincides with $\overline{\dt[\hat\theta]{{Y}}{
%   {R}}}_{Z=1} - \overline{\dt[\hat\theta]{{Y}}{ {R}}}_{Z=0}$, where
% $\dt[(a, b_{0}, b_{1})]{ \mathbf{y}}{ \mathrm{r}} = \mathbf{y} - a - (b_{0}
% \indicator{r < 0} + b_{1} \indicator{r\geq 0})\mathrm{r}$
% and $\hat\theta = (\hat\alpha, \hat{\beta}_{-}, \hat{\beta}_{+})$ are
% as estimated under the interaction model.)

Individual outlying observations and sample subsets that do not meet
RDD assumptions can undercut the validity of an RDD analysis, provided
that they are influential.
These threats suggest the use of robust modeling in RDDs---in
particular, to fit the
 specification using a modern MM,  SM, or similar estimator so designed
 as to possess a bounded influence function
 \citep{yohaiZamar1997locallyrobustMestimates}.
Such procedures address influence
 in the course of the fitting process.
 In MM-estimation as in OLS,
coefficients $\beta$ of a linear specification solve estimating equations
$\sum_{i} \psi\left\{ ({y}_{i} -
\vec{x}_{i}\beta)/s \right\} \vec{x}_{i} =0$, where $s>0$ and
$\psi(\cdot)$ is an odd function satisfying $\psi(0)=0$,
$\psi'(0)=1$ and $t\psi(t)\geq 0$; bounded influence fitters replace OLS's $s\equiv 1$ with resistant preliminary
estimates of residual scale, and OLS's $\psi(t) = t$ with a continuous $\psi$
that satisfies $\int_{0}^{\infty}\psi(t)dt < \infty$. This limits
the loss incurred by the fitter for failing to adapt itself to a small
portion of aberrant observations;
it is permitted to instead systematically down-weight them. When
present, such aberrancies present large residuals, that are easier to
see in ordinary diagnostic plots, and small
``robustness weights,'' an additional diagnostic not available for OLS fitting
\citep{maronna2006robust}.  We are not aware of prior work addressing
potential contamination of an RDD sample with the assistance of
bounded influence MM-estimation.

Surprisingly, given their common origins in %the work of
Huber \citeyearpar{huber1964robust}, MM estimation is not routinely
paired with sandwich estimates of variance, as in \S~\ref{sec:sandwich} and
the surrounding discussion of \S~\ref{sec:robust-analys-covar}.
Exceptions include Stata's \texttt{mmregress} and R's \texttt{lmrob},
which optionally provide Huber-White standard errors
\citep{verardiCroux2009robust,rousseuwetal2015robustbase}.


\section{Randomness and Regression in RDDs}\label{sec:theMethod}

% %% RDDs differ from RCTs in an important way:
% %% $Y_C$ typically correlates with $R$, and is therefore not independent
% %% of $Z$; Strong Ignorability \eqref{eq:ignore} is violated.
% %% Can it be relaxed to an assumption that is
% %% simultaneously plausible in RDDs and compatible with similar methods?

% When circumstances call for generating random numbers, such as in
% RCTs or Monte Carlo methods, statisticians often settle instead for
% pseudo-random numbers, deterministic
% outputs of a complex algorithm and an initial seed; a person in
% possession of that seed can exactly predict the pseudo-random draws.
% ``True'' random numbers, on the other hand, derive from physical
% processes that are either chaotic \citep[e.g.][]{uchida2008fast} or
% quantum \citep[e.g.][]{stefanov2000optical}, and are not
% predictable.  However, these processes often contain an element
% of predictability; % \citep{raz2005extractors};
% true random
% numbers are obtained only after separating predictable from
% non-predictable components \citep[see, e.g.][]{Nisan1999148}.%,vadhan2012pseudorandomness}.

% Similarly, RDD analysis decomposes $Y_{T}$ and $Y_C$ as the sum of systematic
% components, a function of $R$, and disturbances, relying on the nature of the decomposition to ensure
% that, vis a vis one another, the disturbances and $Z$ may
% be regarded as random.
% % There is no need
% % to assume the systematic component to equal or be consistent for
% % $\EE (Y_{C}| R)$, nor to make further distributional assumptions on
% % the disturbances.

The analysis of Hurricane Maria's death toll in \S~\ref{sec:maria}
treated an RDD as analogous to a randomized experiment by following a
two-step procedure: first, modeling the non-random component of
outcomes $Y$, and then treating the residuals as ignorable, \emph{vis
  a vis} treatment assignment $Z$.
In \S~\ref{sec:maria}, these two steps used two different
datasets---we regressed $Y$ on $R$ using data from years prior to
2017, when Maria hit, and used 2017 data to estimate effects.
This luxury is unavailable in the typical RDD, in which both steps
must use the same data, as in \S~\ref{sec:robust-analys-covar}. 
This section will describe a generalization of the method in
\S~\ref{sec:maria} to the typical case, while incorporating
specification tests described in \S~\ref{sec:specification} and robust
modeling strategies described in \S~\ref{sec:robust-altern-ordin}. 

\subsection{An Analytic Model for RDDs} \label{sec:model-eey-c-r}

Suppose the statistician to have selected a \textit{detrending procedure}: a
trend fitter, i.e. a function of
$\{({y}_{i},d_{i},r_{i})\}_{i=1}^{n}$ returning
fitted parameters $\hat{\theta}$ in a sufficiently regular
fashion, along with a
family $\{\dt{\cdot}{\cdot}: \theta\}$ of residual or partial
residual transformations, each mapping data $(\mathbf{y}, \mathbf{r})$ to
$\{\dt {y_{i}}{\mathbf{r}} \}_{i=1}^{n}$.
Appendix~\ref{sec:large-sample-rand}
%\citet[\S~\ref{sec:large-sample-rand}]{lrdauthors:supp}
states the
needed regularity condition, which is ordinarily met by OLS and always
met with our preferred fitters (Section~\ref{sec:test-hypoth-no}).
Causal inferences taking
the perspective that $Z$ is random due to
randomness in $R$ are then possible under the following assumption.

\begin{ass}{Residual Ignorability}
\sloppy
Given $\mathcal{W}$ with $0< \PP( R \in
\mathcal{W}, R\leq 0) < 1$ and a detrending procedure $(\hat{\theta}, \dt{y}{r})$,
\begin{equation}\label{ycheck}
\dt[\thetaInf]{Y_{C}}{ R }%\mathbf{\Ych}_{C}
\independent {Z}| \{R \in \mathcal{W}\},
\end{equation}
where $\thetaInf$ is the probability limit of $\hat\theta$.
\end{ass}
Residual Ignorability states that, though $Y_C$ may not be independent of
$Z$,  it admits a residual transformation bringing about such
independence.   With $\dt[\hat\theta]{Y_{C}}{ R}$ a suitable
partial residual, Residual Ignorability is entailed by the
\textsc{ancova} model (\S~\ref{sec:robust-analys-covar}), or by the combination of any parametric model
for $\EE (Y_{C}| R)$ with a strict null $H$ relative to which the
value of $Y_{C}$ can be reconstructed from the values of $Y$, $D$ and
$Z$ (\S~\ref{sec:randProc}).
% or where $H$ asserts $Y_{T} = Y_{C} + \tau D$ for the true CACE  $\tau
% = \EE(Y_{T}-Y_{C}| D_{T}=1)$,  and in addition
% $(Y_{T} - Y_{C}) \independent R | D_{T}$ holds.
(In either of these cases $\dt[\thetaInf]{Y_{C}}{ R}$
is independent not only of $Z$ but also $R$,
a modest strengthening of~\eqref{ycheck}.)
% relaxes
% this condition, accommodating cases in which
% $Y_{C} - \EE (Y_{C}|R)$ equals $\dt[\hat\theta]{Y_{C}}{ R}$ only
% in the on-average sense of conditional expectation given $Z$.
% Just as an accurate
% specification of %$\EE (Y | Z, X)$ or
% $\EE (Y_{C}| X)$ is helpful but
% not necessary for valid covariate adjustment in an RCT,  it is helpful
% but not necessary for Residual
% Ignorability that $\dt[\thetaInf]{Y_{C}}{R}$ be a function of
% $Y_{C}$'s residual from a consistent estimate of $\EE (Y_{C}| R)$.
% If it is, then Residual Ignorability holds with $\dt[\thetaInf}]{y}{r} = y-
% \EE(Y_{C}| R=r)$ and $\hat\theta$ a standard moment- or
% likelihood-based estimate of the parameters of $Y_{C}$'s regression on
% $R$.  If not, however, one may
% still have $\dt[\thetaInf]{Y_{C}}{ R}$ independent of $Z$, or even
% $\dt[\thetaInf]{Y_{C}}{ R} $ independent of $R$
% with some
% other choice of $e_{\theta}(\cdot)$ and $\hat\theta$.   It is neither required nor preferred that model fit be appraised in terms of squared
% error loss. Taking $\theta = (s, \mathbf{\beta})$ and
% $\dt[\theta]{y}{r} = \psi\big\{ (y - f(r) \mathbf{\beta} )/{s}\big\} $,
% with $f(\cdot)$ scalar- or vector-valued and
% $\psi(\cdot)$ a scoring intended to convey robustness to contamination of the
% $\EE ( Y_{C}| R)$ model, is quite consistent with \eqref{ycheck}.

% To assume \eqref{ycheck} is in no way to assume
% %\begin{equation} \label{eq:notycheck}
% $\dt[\hat\theta]{Y_{C}}{ R} \independent {Z}| \{R \in \mathcal{W}\}$.
% %\end{equation}
% The simplification of \eqref{ycheck} replacing its
% $\thetaInf$ with $\hat\theta$ cannot be expected to hold: unless
% $\hat\theta -\thetaInf$ has zero variance, it is almost inevitable
% that it correlate in some way with $Z$, and by extension that $\dt[\hat\theta]{Y_{C}}{ R}$ and
% $Z$ be non-independent.
% Rather, Residual Ignorability presumes
% existence, if not knowledge, of a single $\theta = \thetaInf$ relative
% to which potential partial residuals
% %$\dt[\thetaInf]{Y_{T} - \tau D_{T}}{ R}$ and
% $\dt[\thetaInf]{{Y}_{C}}{ R}$ and treatment assignment $Z$ are
% mutually independent.
% In marked contrast with RCTs, in typical RDDs there can be at
% most one $\theta$ for which this is true. To see this, consider
% $\dt[\theta]{y}{r} = y - \theta r$.  If $Z$ were jointly independent
% of $\dt[\theta_{0}]{Y_{C}}{ R}$ and $\dt[\theta_{1}]{Y_{C}}{ R}$, it would also have
% to be independent of $(\theta_{0} - \theta_{1}) R$. If $\theta_{0}
% \neq \theta_{1}$ this would entail $Z \independent R$ --- a
% contradiction, since $Z = \indicator{R<0}$.  Were $R$ a baseline variable
% in an RCT, $\dt[\theta]{Y_{C}}{ R} \independent Z$ would hold for all $\theta$,
% and inferences relying on this independence would require neither covariance adjustment
% nor consistent estimation of covariance parameters --- both of which are essential for RDDs.

\sloppy
Assuming Residual Ignorability, inference about treatment effects is
made conditionally, on
$\mathbf{A}= (\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$, $\mathbf{D}_{T},
\{(Y_{Ti}, Y_{Ci}, D_{Ti}, R_{i}) \indicator{{R}_{i} \not\in
  \mathcal{W}}\}_{i=1}^{n})$.
Conditioning on the full data vector when $R \not\in \mathcal{W}$
excludes observations for which \eqref{ycheck} is not assumed.
Conditioning on
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$
removes little of the entropy
in $\mathbf{R}$, leaving it available as a basis for inference; in
contrast, conditioning on Section~\ref{sec:randProc}'s $(N_{0}, N_{1}, \mathbf{Y}_{C})$ would
severely constrain $\mathbf{R}$.
Uncoupled to $Y_{T}$s, the detrended  $Y_{C}$s,
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$,
are in themselves uninformative about $\EE(Y_{T} - Y_{C})$, so
the variables comprising $\mathbf{A}$ are jointly
%\marginpar{$\leftarrow \Delta$ to $(\mathbf{Y}_{C}, N_{0}, N_{1})$?}
S-ancillary, just as $\mathbf{A}^{\dagger}$ was seen to be
in Section~\ref{sec:randProc}.  As in Neyman-style randomization inference for RCTs, some conditioning variables are
unobserved; but this is not an impediment, at least for large-sample
inferences.


% Because \eqref{ycheck} can be true for at most one
% $\thetaInf$, in applications it must be paired with an assumption that
% one has a estimation routine that consistently estimates $\thetaInf$.  As
% discussed in Sec.~\ref{sec:robust-analys-covar}, in RDDs the standard error accompanying
% $\overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=1} -
% \overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=0}$ must
% reflect the indirect contribution to its sampling variability of errors of estimation of
% $\thetaInf$, as the $\mathrm{SE}_{s}$ of \eqref{eq:tedef}
% does but the $\mathrm{SE}_{u}$ of \eqref{eq:tudef} does not.

\begin{comment}
To see this, consider
$\dt[{\theta}]{y }{ r} = y - r \theta$.  Up to an $o_{P}(n^{-1/2})$
error,
\begin{equation}\label{eq:2}
    \{ \overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=1} -
\overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=0} \} -
\{  \overline{\dt[\thetaInf ]{\bty }{ \mathbf{r}}}_{z=1} -
\overline{\dt[\thetaInf]{\bty }{ \mathbf{r}}}_{z=0} \}   \approx
\Big[ \EE \big\{ R \big| Z=1 \big\} - \EE
  \big\{R \big| Z=0 \big\} \Big]  (\hat\theta -
\thetaInf)^{t}.
\end{equation}
In an RCT the term in square brackets at right would be 0, ensuring that even if
variation in $\hat\theta - \thetaInf$ is positive, its large-sample
contribution to $\overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=1}
- \overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{z=0}$  is
$o_{P}(n^{-1/2})$, and is negligible; but in an RDD it is
intrinsically nonzero, and \eqref{eq:2} is $O_{P}(n^{-1/2})$.
 For the same reason, permutation
tests do not provide exact inference for RDDs: in an RDD, even under
$H$ conditioning on %$\mathbf{Y}_{C}$'s and  $\mathbf{Z}$'s order statistics
$(\mathbf{Y}_{C}, N_{0}, N_{1})$
does not eliminate variability in $\hat\theta$; the differences
at left of \eqref{eq:2} differ from one another. (In Randles's
[\citeyear{randles:1982}] %analysis of errors due to estimation of secondary parameters,
terms, RCTs give rise to ``Case A'' distributions whereas RDDs' are ``Case B.'' Only in the unusual circumstance that $\thetaInf$ is externally
determined, and is known rather than estimated, can \eqref{ycheck}
serve as a basis for exact tests.)  Asymptotically
distribution-free inferences, on the other hand, are straightforwardly
arranged by estimating
$\overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{Z=1} -
\overline{\dt[\hat{\theta}]{\bty }{ \mathbf{r}}}_{Z=0}$'s standard error
with attention to sampling variability in $\hat\theta$,  as
$\mathrm{SE}_{s}$ does.
\end{comment}

% If available, a lagged, pre-treatment version of the outcome that's
% distinct from the running variable may be the basis for particularly
% informative plots and regression diagnostics
% \citep{maynard2013strengthening}.

% Such M-estimation conveys robustness to other errors of specification
% that are quite relevant to RDDs.  In modern robust M-estimation
% \citep{maronna2006robust}, estimating equations defining the
% regression coefficients are adjusted so as to bound the influence of
% any one or small number of observations.  Under uncertainty about
% width of the analysis window $\mathcal{W}$, it will be safer to fit
% $\hat\theta$ using such a method, the points of greatest leverage
% generally being the closest to the edge of $\mathcal{W}$.  Thus
% robust regression addresses a fundamental incompatibility between RDDs
% and ordinary least squares, namely that the observations whose
% suitability for inclusion in the analytic sample is most questionable
% --- those whose values of the running variable are farthest from the
% cutpoint --- also exert the greatest leverage in estimation.



\subsection{Checking and Refining the Window ($\mathcal{W}$)}
\label{sec:bandwidthChoice}

If subject matter knowledge suggests that chance variability intrinsic
to $R$ is typically of magnitude $b$, then RDD modeling focused on
distilling local randomization might set $\mathcal{W}$ to $[-b,b]$. But it
is also sensible to subject such an initial choice to specification
testing (\S~\ref{sec:specification}).%\marginpar{I replaced $c$ here
%  with $b$ since $c$ is the cutoff-AS\\ ---Good catch thnx! -B}

Common RDD specification checks
can be regarded as testing Residual Ignorability with a multivariate
``outcome'' $Y^{*}$ combining the actual outcome $Y$ with covariates $X$---\eqref{ycheck} with
$\mathbf{Y}_{C}^{*} = (\mathbf{X}, {Y}_C)$ in place of $Y_{C}$.
% perhaps with minor adjustment to the forms of $\dt[\theta]{ \cdot }{ r}$
% and corresponding fitter $\hat{\theta}(\cdot, R)$.
%
%In addition to $\mathcal{W}$, researchers must pick $f$, a model for $Y_C$.
%% If a lagged outcome is not available, the data will not have informed the specification of
%% $\EE (Y_{C}| R=r) $ on the treatment side of the threshold.  It
%% should be treated as an approximation, and $\mathcal{W}$
%% deliberately chosen to be narrow enough to limit influences of errors
%% of approximation, as distinct from errors of estimation, on statistics
%% $t({\mathbf{y}_H} - f(\mathbf{r}; \theta(\mathrm{r},
%% {\mathbf{y}_H}))$.
%% Covariate placebo tests can be helpful in this regard.
This calls for preliminary detrending procedures, mechanisms to
decompose  $X$ into components that are systematic or unpredictable,
vis a vis $\mathbf{R}$, just as ${\mathbf{Y}_C}$ will later be decomposed.
%% Since $Z$ cannot have an effect on a pre-treatment
%% covariate, the preliminary step of reconstructing
%% $x_{C}$ is not needed.
%%% [Reorg obviated need for the above, as this passage now precedes
%%% discussion of reconstructing Y_C]
Our analysis of the LSO data posits systematic components that are
linear and logistic-linear in $R$, depending on whether $X$ is
a measurement or binary variable. %, paralleling its choice of a linear decomposition
% for the primary outcome, subsequent GPA.  (It is fitting that the $X$- and
% $Y$-specifications mirror each other in model complexity.)
The placebo check adds $Z$ to the specification and tests whether its
coefficient is zero.  We implement these checks as Wald tests with
heteroskedasticity-robust standard errors, as in
\S~\ref{sec:robust-analys-covar}, using the Bonferroni method to
combine placebo checks across covariates.
% yielding a test of the
% hypothesis that all of the covariates' residuals are jointly ignorable within the
% given $\mathcal{W}$.

If the first window tested has form $\mathcal{W} = (-b, b)$,
write $H_{b}$ for the corresponding joint ignorability hypothesis.
If the $R$-adjusted covariate placebo tests reject $H_{b}$ then the process is repeated for
$\mathcal{W}' = (-b', b')$, some
$b' < b$, and perhaps repeated again if $H_{b'}$ also is rejected.
This may seem to call for a further layer of multiplicity correction,
since any number of
bandwidths may have been tested before identifying a
$b$ for which $H_{b}$ is sustained; but it so happens that this form
of sequential testing implicitly corrects for multiplicity, according to the
sequential intersection union principle
(\citealp[SIUP;][Proposition~1]{rosenbaum2008testing};
\citealp{hansenSales2015cochran}). To compensate for conservatism of
the Bonferroni method, we test with size
$\alpha_{B}=.15$, not $.05$.
% Let
% $\{b_{1}, b_{2}, \ldots\} \subseteq (0, \infty)$ be a descending sequence of
% candidate bandwidths, with an accompanying family $\mathcal{B}$ of
% tests, one for each $H_{b}$. Define a modified family $\mathcal{B}^{*}$
% as follows: for each $b_{i}$, $H_{b_{i}}$ is rejected at level
% $\alpha$ if only if at level $\alpha$, $\mathcal{B}$ rejects each of
% $\{ H_{b_{j}}: j \leq i\}$. The
% sequential intersection union principle \citep[SIUP;][Proposition~1]{rosenbaum2008testing}
% states that if each test in $\mathcal{B}$ tests its corresponding
% $H_{b}$ with size $\alpha$ then, with probability $1-\alpha$ or more,
% $\mathcal{B}^{*}$ rejects only hypotheses $\{H_{b_{i}}:  i\}$
% that are false --- i.e., $\mathcal{B}^*$ strongly controls the
% family-wise error rate at level $\alpha$ \citep{hansenSales2015cochran}.

Writing $b$ for the half-width selected in this manner, i.e. $b=\max$
\{$b_{i}: H_{b_{i}}$ is not rejected\}, we next apply a McCrary
manipulation test to $(-b,b)$.  If this returns a $p$-value
$p_{0} < \alpha_{M}=.05$, we repeat it within windows
$\{ i: |R_{i}| \in (a_{j}, b)\}$, $0 = a_{0} < a_{1}< \cdots < b$,
terminating the process at the first $j$ for which
$p_{j} \geq \alpha_{M}$.  By a second application of the SIUP,
the size of this test sequence is $\alpha_{M}$.  Taken
together, placebo and McCrary tests restrict the sample to
$\mathcal{W} = (-b, b)$ or  $(-b, -a) \cup (a, b)$.
% Although subsequent
% diagnosis of outcome regression may in principle indicate suggest
% further exclusions, the analytic sample is expected to be $\{i: R_{i} \in \mathcal{W}\}$.

% Maximum likelihood estimation is one option for
% $\hat{\theta}_{x}(\cdot, \cdot)$, but
% %% \footnote%
% %% { In the special case that $\mathbf{x} $ is decomposed as the sum of a
% %%   linear function of $\mathrm{r}$ plus a residual
% %%   $\mathbf{x}^{\perp}$, with the linear function fitted by ordinary
% %%   least squares, $d^{2}(\mathbf{x}^{\perp}, \mathbf{z})$ can be
% %%   expediently calculated as the difference of
% %%   $d^{2}((\mathbf{x}, \mathrm{r}), \mathbf{z})$ and
% %%   $d^{2}(\mathrm{r}, \mathrm{z}) $.  }
% we prefer robust linear and logistic fitters
% \citep{rousseuwetal2015robustbase}, deeming their lesser contamination
% sensitivity a relevant advantage. Specifically, if the $\mathcal{W}$ under
% consideration is somewhat too wide, then a robustly fitted
% $\hat{\theta}$ still estimates the same $\thetaInf$ that would be
% estimated for $\mathcal{W}_{0} \subseteq \mathcal{W}$ narrow enough for covariate
% ignorability to hold. Similarly, to limit the possibility that the
% fitting routine would obscure differences between residuals above and
% beneath the threshold, we fit $R$-slopes using a specification allowing for an independent contribution from $Z$, but then set the $Z$-contribution to zero when decomposing the covariate.


\subsection{Inference About the Treatment Effect}
\label{sec:test-hypoth-no}

For inference about $\tau$ under the model
$Y_{T} = Y_{C} + \tau D_{T}$, select a specification
$\mu_{\beta}(\cdot)$ for $\EE(Y_{C}| R)$, %in the application of
%Section~\ref{sec:appl-effect-acad}
such as the
linear model $\mu_{\beta}(R) =\beta_{0} + R\beta_{1}$.
%Although a maximum likelihood
%estimate of $\beta$ may be available,
If moderate sample contamination may be present --- specifically, contamination of
a $O(n^{-1/2})$-sized
share of the sample --- consistent estimation of $\thetaInf$ requires a
bounded influence fitter (\citealp[Thm.~3]{he1991localbreakdown};
\citealp{yohaiZamar1997locallyrobustMestimates}), as opposed to
maximum likelihood.  OLS does not meet this consistency requirement;
nor do many robust regression methods engineered to meet
objectives other than bounding the influence function
\citep{stefanski1991note}.
Further, no specification test is powerful enough
to reliably detect contamination of this size; power to detect anomalies
affecting only $O(n^{-1/2})$ of the sample can only tend to a number
strictly less than 1.  %NB: This is expanded a little in a
                       %now-commented-out passage of Discussion, the
                       %on referencing vdvaart:1998.
Thus under uncertainty about the
proper limits of $\mathcal{W}$, MM-estimation (\S~\ref{sec:robust-altern-ordin}) is to be preferred.
The analyses and simulations presented below use MM-estimators with bisquare $\psi$ and
the ``fast S'' initialization of \citet{salibian-barreraYohai2006fastS}.

Separately for each hypothesis $H: \tau=\tau_0$ under
consideration, one calculates
$\mathbf{y}_{H} = \mathbf{y} - \mathbf{d}\tau_{0}$, then
applies the chosen specification and fitter to
$(\mathbf{y}_{H}, \mathbf{r})$.
The combination of the data, the
model fit, and the residual transformation $\dt{\cdot}{\cdot}$ give rise to residuals
$\dt[\hat\theta]{\mathbf{y}_{H}}{\mathbf{r}}$, completing the
detrending procedure. Whether $H$ is rejected or sustained is
determined by the value of the sandwich-based \textsc{ancova} $t$-statistic
in \S~\ref{sec:sandwich}.

In practice it is expedient to use a near-equivalent
test by modifying the detrending
procedure%, without need to separately calculate
%$\overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=1} -
%\overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=0}$ or a corresponding standard
%error
.
When regressing $Y_{H}$ on $R$, include an additive
contribution from $Z$, so that $\mu_{\beta}(R) =\beta_{0} +
R\beta_{1}$ is replaced with $\mu_{(\beta,\gamma)}(R) =\beta_{0} +
\beta_{1}R + \gamma Z$. With sandwich estimates of
$\text{Cov}\{(\hat{\beta}_{H}, \hat{\gamma}_{H})\}$,
% ?state that in case of OLS with sandwich standard errors,
% taking $e$ to be a partial residual makes \eqref{eq:tedef}
% equivalent to the t statistic on the regression coefficient
%
% <!--the below is a passage that I cut a while back, as opposed to an inline note:-->
%
% just as \eqref{eq:sudef}'s $\mathrm{SE}_{u}^2 \left(
%   \bar{Y}_{Z=1} -  \bar{Y}_{Z=0} \right)$ was seen in
% Section~\ref{sec:randProc} to estimate $\var(\bar{Y}_{Z=1} -
% \bar{Y}_{Z=0}| \mathbf{A})$ just as validly as $\var(\bar{Y}_{Z=1} -
% \bar{Y}_{Z=0}| \mathbf{Z})$, the sandwich estimate is consistent for
% the variance of $\hat\gamma$ with conditioning as discussed in
% Section~\ref{sec:model-eey-c-r}
% \citep[Section~\ref{apnd:requ-forpr-eqref}]{lrdauthors:supp}.
the t-ratio comparing $\hat{\gamma}_{H}$ to
$\text{SE}_{s}(\hat{\gamma}_{H})$ induces a generalized score test \citep{boos1992genscoretest}. Implicitly it is a two-sample
t-statistic with covariance adjustment for $R$, as in \S~\ref{sec:sandwich}: with fitting via OLS,
this correspondence would be exact, as noted in Section~\ref{sec:robust-analys-covar}; with the robust MM estimation we
favor, the correspondence is one of large-sample equivalence
%\citep[Section~\ref{sec:suppl-s-refs}]{lrdauthors:supp}
(Appendix~\ref{sec:suppl-s-refs}).
% Just as in RCTs \eqref{eq:sudef}'s $\mathrm{SE}_{u}\left( \bar{y}_{Z=1} -  \bar{y}_{Z=0} \right)$ is valid either with conditioning on $\mathbf{Z}$
% that leaves $\mathbf{Y}_{T}$ and $\mathbf{Y}_{C}$ unconstrained, or
% with conditioning only on $\mathbf{A}$,
% $\text{SE}_{s}(\hat{\gamma}_{H})$ calculated in this way is consistent for $\var^{1/2}(\hat{\gamma}_{H}\mid A)$.
% With large samples, comparing
% $\hat{\gamma}_{H}/\text{SE}_{s}(\hat{\gamma}_{H})$
% to a central $t$ or standard Normal distribution
%  gives a randomization-based test of $H$ \citep[Section~\ref{apnd:requ-forpr-eqref}]{lrdauthors:supp}.
Iterative testing is facilitated by regressing $\mathbf{y}$ on $\mathbf{r}$ and $\mathbf{z}$
with offset variable $\mathbf{d}\tau_{0}$; then only the offset needs to
be modified to test $H: Y_{T} = Y_{C} +  \tau D_{T}$ for a new $\tau$.

\subsection{Post-Fitting Diagnostics} \label{sec:post-fitt-diagn}
Once the Hodges-Lehmann estimate $\hat{\tau}_{\mathrm{HL}}$ has been found, one
inspects the corresponding regression fit for points of high influence.
Bounded influence regression is helpful here.  Besides making
influential points easier to see in residual plots, this limits
effects of data contamination, as non-conforming influence points are
down-weighted as a result of the fitting process. This down-weighting
is reflected in robustness weights, ranging from 1, for non-discounted
observations, down to 0, for the most anomalous observations.
Plotting % As
% an indicator of robustness to non-constant effects.
% (But is this overloading the discussion here?)
robustness weights against residuals may expose opportunities to
improve the fit of $\mu_{\beta}(R)$, or of the treatment effect model;
plotting them against $R$ may expose contaminated sub-regions
of $\mathcal{W}$ that specification testing failed to remove.



