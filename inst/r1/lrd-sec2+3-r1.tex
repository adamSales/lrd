\section{Review of Selected Methods}
%% introduces notation
% Z
% Y
% \indicator{}
%  $y_{Ti}$ and $y_{Ci}$ (upper or lower case??)


Capturing the local randomization essence of the RDD calls for
a merger of RDD methods with concepts developed for RCTs. This
section selectively reviews relevant literatures.

Let $Z \in \{0,1\}$ indicate assignment to treatment ($Z=1$) as opposed to control
($Z=0$).  For an RDD one defines $Z \equiv \indicator{R< 0}$,
$\indicator{R \leq 0}$, $\indicator{R\geq 0}$ or $\indicator{R > 0}$,
depending on how intervention eligibility relates to the threshold,
where $\indicator{x}=1$ if $x$ is true and $0$ otherwise.
Let $Y$ represent the outcome of interest.
For simplicity assume non-interference, the model that
a subject's response may depend on his but not also on other subjects'
treatment assignments \citep{cox:1958,rubin:1978}.  Thus we may take each $i$
to have two potential outcomes, $y_{Ti}$ and $y_{Ci}$ \marginpar{upper
  or lower case?}, at most one of which is observed, depending on whether $z_i=1$ or $0$, respectively;
 observed
responses $Y$ coincide with $ZY_{T}+(1-Z)Y_{C}$.

\subsection{The \textsc{Ancova} Model for RDDs}\label{sec:robust-analys-covar}

The classical analysis of covariance (\textsc{ancova}) model for
groups $i=1,\ldots, k$, each including subjects $j=1, \ldots, n_{i}$,
says that
$Y_{ij} = \alpha_{i} + \beta X_{ij} + \epsilon_{ij}$, where $\epsilon_{ij}
\sim \mathrm{Normal}(0, \sigma^{2})$ is independent of the continuous
covariate $X_{ij}$.
In the classical development of RDDs, \textsc{ancova} with $k=2$
groups---treated and untreated---is a leading option among statistical
models
\citep{thistlethwaite1960regression}.
A potential outcomes version of the model is
 $Y_{Ci} = \alpha_{0} + \beta R_{i} + \epsilon_{Ci}$ and
$Y_{Ti} = \alpha_{1}  + \beta R_{i} + \epsilon_{Ti}$, with
 $\epsilon_{Ci} \sim \mathrm{Normal}(0, \sigma^{2})$ and
 $\epsilon_{Ti} \sim \mathrm{Normal}(0, \sigma^{2})$.
In marked contrast to RCTs, it does
not require that $(Y_{T}, Y_{C}) \independent Z$: to the contrary, both
$Y_{C}$ and $Y_{T}$ are presumed to associate with $R$, which in turn
determines $Z$.
Nonetheless, under this model the estimated $Z$ coefficient from the
model
\begin{equation}\label{eq:classicOLS}
Y_i=\alpha+\beta R_i+\tau Z_i+\epsilon_i
\end{equation}
fit using ordinary least squares (OLS), is unbiased for
$\alpha_{1} - \alpha_{0}$.
This estimand is the value of  $\lim_{r\downarrow 0} \EE(Y | R=r) -
\lim_{r\uparrow 0} \EE(Y | R=r)$ and, simultaneously,
limitless estimation targets such as $\EE Y_{T}  - \EE Y_{C}$.

The OLS approach estimates $\tau$ as a parameter in regression model
\eqref{eq:classicOLS}. In contrast, the approach in
\S~\ref{sec:using-eqref-test} took place in two separate steps:
first adjusting outcomes for $R$ and then testing hypotheses using a test
statistic that contrasts treated and untreated subjects.
However, the OLS estimator may be restated in the manner of
\S~\ref{sec:using-eqref-test}---a more involved procedure, to be sure,
but one with an important advantage to be described in \S~\ref{sec:fuzzy-regr-disc}.
Consider the hypothesis $H: Y_{T} = Y_{C} + \tau$.
Next, define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H$, $Y_H=Y_C$)
and residuals $\resid\equiv\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
b\mathbf{r}$.
Finally, test $H$ with statistic
\begin{equation} \label{eq:MeanDiffTestStat}
\meanDiffT(\mathbf{Y}_H , \mathbf{Z}) =
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
\overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0}
\end{equation}
--- that is, the $\tau$ estimate from an OLS fit of the variant of \eqref{eq:classicOLS}
with dependent variable $\mathbf{y}_{H}$.
Because of the structural relationship
between $R$ and $Z$, its null distribution
is not tractable. (In \S~\ref{sec:using-eqref-test}, statistics
$T(\mathbf{Y}_{H}, \cdot)$ had straightforward permutation distributions
because slope and intercept parameters had been estimated
from a separate sample.) However, under the parametric \textsc{ancova} model, with
conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
\mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
$\meanDiffT(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
variance equal to the classical OLS variance of the coefficient on
$Z$.

% \S~\ref{sec:using-eqref-test}.
% Consider the hypothesis $H_\tau: Y_{T} = Y_{C} + \tau$.
% Next, define ${{Y}_H} = {Y} - \tau {Z}$ (so that under $H_\tau$, $Y_H=Y_C$)
% and residuals $\resid\equiv\dt[(a, b)]{\mathbf{y}_{H}}{ \mathbf{r}} = {\mathbf{y}_H} - a -
% b\mathbf{r}$.
% Finally, test $H_\tau$ with statistic
% \begin{equation*}
% \meanDiffT(\mathbf{Y}_H , \mathbf{Z}) =
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=1} -
% \overline{\dt[(\hat{\alpha},\hat{\beta})]{{{Y}_H}}{ {R}}}_{Z=0},
% \end{equation*}
% where $\hat{\alpha}$ and $\hat{\beta}$ are the standard OLS estimates.
% Because of the structural relationship
% between $R$ and $Z$, (and, unlike in
% \S~\ref{sec:using-eqref-test}, regression parameters were not estimated
% with a separate dataset)
% permutation tests based on this statistic are not tractable.
% However, under the parametric \textsc{ancova} model, with
% conditioning on $\mathbf{R}$ rather than on $(N_{0}, N_{1},
% \mathbf{Y}_{C})$ as in \S~\ref{sec:using-eqref-test},
% $\meanDiffT(\mathbf{Y}_H , \mathbf{Z})$ is straightforwardly Normal, with
% variance equal to the classical OLS variance of the coefficient on
% $Z$.

In general, the set \{$c$:
$H_{c}$ is not rejected at level $\alpha$\}, which can be seen to be an interval, is a
$100(1-\alpha)\%$ confidence interval for $\tau$ of the Rao score type
\citep{agresti2011scoreintervals}; the $c$ solving
$\meanDiffT(\mathbf{y}_{H}, \mathbf{z}) = 0$, which can be seen to
be unique, is an M-estimate of $\tau$ under both the classical
\textsc{ancova} model and various of its generalizations.
(In fact, the estimate for $\tau$ corresponding to these statistical tests is
algebraically equal to the $Z$-coefficient from an OLS estimate of
\eqref{eq:classicOLS}, and the two-sided 95\% confidence interval induced in this
manner is the familiar
$\hat{\tau} \pm 1.96\, \mathrm{SE}(\tau)$.
However, these equivalences do not necessarily extend to estimation
strategies outside of OLS, such as the robust estimators of
\S~\ref{sec:robustFitters} below.)

\subsubsection{Addressing the Wald interval's shortcomings for fuzzy RDDs} \label{sec:fuzzy-regr-disc}
RDDs susceptible to non-compliance---where subjects' actual
treatments may differ from $Z$--- are called ``fuzzy.''
In these cases, let $D$ indicate whether treatment was actually
received.
This $D$ is an intermediate outcome, so there are
corresponding potential outcomes $D_{C}$ and $D_{T}$, with $D \equiv ZD_{T}
+ (1-Z)D_{C}$.
Subject $i$ is a non-complier if $D_{Ci}=1$ or $D_{Ti}=0$, though we
will assume the monotonicity condition $D_{C}\equiv 0$ --- there may be
subjects assigned to the treatment who avoid it, but no one gets
treatment without being assigned to it.
We shall also posit the exclusion restriction,
that $Z$ influences $Y$ only by way of its effect on $D$
\citep{bloom1984ans,Angrist:etal:1996,imbens:rose:2005}.
Our focus of estimation is the
 ``treatment-on-treated'' effect (TOTE),
%complier
%average causal effect (CACE),
$\EE(Y_{T} - Y_{C}| D_{T}=1)$.

Statistical hypotheses about the TOTE take the form
$H_\tau:Y_T=Y_C+D\tau$.
To test $H_\tau$ under non-compliance, let $Y_H=Y-\tau D$,
designate $t (\mathbf{y}_H,\mathbf{r})$ as test statistic, and compare
its value to a standard normal distribution.
(The only difference between hypothesis testing for a ``strict'' RDD, one
with full compliance, versus a fuzzy RDD, is in the formulation of
hypothesis $H$, and the construction of $Y_H$---the rest of the
process remains unchanged [\citealp{rosenbaum:1996:onAIR}].)  As compared to Wald-type
confidence intervals, that is intervals of form $\hat\tau \pm q_{*}
\mathrm{SE}(\hat\tau)$ with $\mathrm{SE}(\hat\tau)$ a single,
hypothesis independent quantity, under partial compliance this iterative method
% of separate evaluation of $\mathrm{SE}_{u} \left\{
%   \overline{({y}_{H_{\tau}})}_{Z=1} -
%   \overline{({y}_{H_{\tau}})}_{Z=0} \right\}$ for each $H_{\tau}$,
improves correspondence of nominal and actual confidence levels
% , particularly if the ``instrument,'' $Z$, is ``weak,''
% i.e. $\PP(D_{T} =1) \approx \PP(D_{C} =1)$
\citep[Sec.~7]{imbens:rose:2005,baiocchiChengSmall2014IVtutorial}.


\subsubsection{Robust Standard Error Estimation}\label{sec:sandwich}
The \textsc{ancova} model for $(Y_{T}, Y_{C})$ is not readily dispensed with, but it
may be relaxed.  OLS estimates of $\alpha_1-\alpha_{0}$ and
$\beta$ remain unbiased under non-Normality, provided
the $\epsilon$s have expectation 0 and bounded variances.  The
ordinary \textsc{ancova} standard error does not require Normality of
the $( \epsilon_{i}: i )$, either, for use in large samples, although
it does require that they have a common variance.
To test
$\EE\{ \dt[\hat\theta]{{Y_H}}{ R} | Z=1\} = \EE\{ \dt[\hat\theta]{{Y_H}}{
R} | Z=0\}$
under potential heteroskedasticity, one estimates
$\var\left\{\meanDiffT(\mathbf{Y}_H , \mathbf{Z})\right\}$
using a sandwich or Huber-White estimator,
$\mathrm{SE}_{s}^{2} \left\{ \meanDiffT(\mathbf{Y}_H , \mathbf{Z}) \right\}$
\citep{huber1967behavior,mackinnonWhite1985sandwichHC,longErvin2000sandwichHC,
bellmccaffrey2002sandwichSEs,pustejovskyTipton2017sandwichSEs}, %as opposed to the %$\mathrm{SE}_{p}$; then
%$\mathrm{SE}_{u} %( \overline{e({Y_H}| R)}_{Z=1} -  \overline{e({Y_H}| R)}_{Z=0} )$
%of \eqref{eq:tudef}:
and refers $\meanDiffT(\mathbf{Y}_H,\mathbf{Z})/\mathrm{SE}_{s}$
% \begin{equation} \label{eq:tedef}
% t_{e} (\mathbf{y}_H,\mathbf{r}) =
% \frac{\overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=1} -
%                              \overline{\dt[\hat\theta]{{y_H}}{ r}}_{z=0}}%
%                            {\mathrm{SE}_{s}\left\{ \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=1} -
%                              \overline{\dt[\hat\theta]{{Y_H}}{ R}}_{Z=0}
%                            \right\} }
% \end{equation}
to a $t$ or standard Normal reference distribution.
Sandwich standard errors confer robustness to misspecification of
$\var(\dt[\hat\theta]{Y_{H}}{R}\mid  R)$, not of $\EE(Y_{H}| R)$
\citep{freedman2006sch}, the latter being the topic of
the following section.
%Section~\ref{sec:robust-altern-ordin}.

% In applying \textsc{ancova} to an RCT, $\mathrm{SE}_{s}$ in
% \eqref{eq:tedef} may be taken as \eqref{eq:sudef}'s
% $\mathrm{SE}_{u}$% \citep{samii2012equivalencies}
% , a more straightforward quantity to compute; unfortunately, in RDDs
% this is not the case
% (Section~\ref{apnd:requ-forpr-eqref}).  On the
% other hand, in \textsc{ancova} as applied to RDDs, if standard
% errors are calculated by the sandwich method then under
% $H: Y_{T} = Y_{C}$, \eqref{eq:tedef} is algebraically equivalent to the
% $t$-statistic of contrast $\hat{\alpha}_{1} -\hat{\alpha}_{0}$.


\subsection{Misspecification, Contamination, and Their Remedies}\label{sec:specification}

The \textsc{Ancova} model for RDDs encodes additional assumptions,
beyond normality and homoskedasticity of regression errors and full
compliance with treatment assignment, which are not so easily
dispensed with.
% Perhaps most worryingly, the form of $\EE[(Y_{T}, Y_{C}) | R]$ may be
% misspecified, and the integrity of the RDD could be compromised by
% subjects who finely manipulate their values of $R$ around the cutoff
% in order to achieve or avoid a treatment assignment.
Methodological RDD literature has responded with specification tests
to detect these threats, or with flexible or robust estimators that
seek to avoid them.

\subsubsection{Covariate Balance Tests}
Analysis of RCTs and quasiexperiments often hinges on assumptions of
independence of
 $\mathbf{Z}$ from $(\mathbf{X}, \mathbf{Y}_{C}, \mathbf{Y}_{T})$.
%% Analogous statements will hold for more complex randomization or
%% ignorability assumptions as well.
 Although neither $\mathbf{Z} \independent \mathbf{Y}_{C}$ nor
 $\mathbf{Z} \independent \mathbf{Y}_{T}$ can be directly tested,
 since potential outcomes are only partly observed, assumptions of form
 $\mathbf{Z} \independent \mathbf{X}$ are falsifiable: researchers can
 conduct placebo tests for effects of $Z$ on $X$.
Of course, treatment cannot affect pre-treatment variables; this is
model-checking (%\citealp{bayarriBerger2000pvalues};
\citealp[][\S~5.13]{cox2006pos}%; \citealp[][Ch.~6]{gelman:etal:2004};
%\citealp[][Ch.~14]{lehmannRomano2006TSH}
).  % Here as
 % elsewhere \citep{box1980sab}, checks finding fault with a model prompt
 % refinement, not abandonment; refinements typical of RDD analysis remove portions of
 % the sample that are suspected of contamination.

 Writing in the RDD context, \citet{cattaneo2014randomization} test
 for marginal associations of $\mathbf{Z}$ with covariates $\mathbf{X}_{i}$,
 $i=1, \ldots, k$, using the permutational methods that are applied
 in Fisherian analysis of RCTs \citep[also see][]{liMatteiMealli2015BayesianRD}.
% Specifically, they
%  obtain p-values though \eqref{eq:01}, with test statistics
%  $t(\mathbf{x}_{i}, \mathbf{z}) = |\overline{(x_{i})}_{z=1} -
%  \overline{(x_{i})}_{z=0}|$,
%  $ i \leq k$, after conditioning on $\mathbf{X}$ in addition to
%  $\sum_{i} Z_{i}$.
Relatedly, \citet{lee2010regression} recommend a
 tests for conditional association, given $R$, of $\mathbf{Z}$ and
 $\mathbf{X}$, by fitting models like those discussed in
 \S~\ref{sec:robust-analys-covar} for impact estimation, but with
 covariates rather than outcomes as independent variables.
Viewing the $R$-slopes and intercepts as simultaneously estimated
 nuisance parameters, these are balance tests applied to
 the covariates' residuals, %net of linear association with $R$,
rather than to the covariates themselves.

If there are multiple
covariates there will be several such tests. To summarize their
findings with a single p-value, the
regressions themselves may be fused within a
``seemingly unrelated regressions'' model \citep{lee2010regression} or
hierarchical Bayesian model
\citep{liMatteiMealli2015BayesianRD}, or the
separate tests' p-values could simply be combined using the Bonferroni
principle.

\subsubsection{The McCrary Density Test}
McCrary's test for manipulation of treatment assignments
\citeyearpar{mccrary2008manipulation} can be understood as a %lack of
%association test applied over an $\{|R| < b\}$ window.
placebo test with the density of $R$ as the independent variable.
The test's
purpose is to expose the circumstance of subjects finely manipulating their
$R$ values in order to secure or avoid assignment to treatment.  Absent
such a circumstance, if $R$ has a density then it should appear to be
roughly the same just below and above the cutpoint.  McCrary's
\citeyearpar{mccrary2008manipulation} test statistic is the difference
in logs of two estimates of $R$'s density at 0, based on observations
with $R<0$ and $R>0$ respectively.
Manipulation is expected to generate a clump just beside the cut
point, on one side of it but not the other, and this in turn engenders
imbalance in terms of distance from the cut-point.

\subsubsection{Reducing the Bandwidth}\label{sec:bandwidth}
In practice, specification test failures inform sample exclusions.
When balance tests fail,
\citet{lee2010regression} would select a bandwidth $b>0$, restrict
analysis to observations with $R\in \mathcal{W} \subseteq [-b, b]$,
and repeat the test on $\{i : r_{i} \in \mathcal{W}\}$.
If that test fails, the process may be repeated with a new bandwidth
$b'<b$, and perhaps repeated again until arriving at suitable bandwidth.
This may seem to call for a further layer of multiplicity correction,
since any number of
bandwidths may have been tested before identifying a
$b$ for which $H_{b}$ is sustained; but it so happens that this form
of sequential testing implicitly corrects for multiplicity, according to the
sequential intersection union principle
(\citealp[SIUP;][Proposition~1]{rosenbaum2008testing};
\citealp{hansenSales2015cochran}).

Alternatively,
\citet{cattaneo2014randomization} recommend testing hypotheses
$H_{b_{i}}: {X} \independent {Z}| \{|R| < b_{i}\}$, $b_1 > b_2 > \cdots >0$, in sequence,
proceeding until identifying  $b = \max \{ b_i: H_{b_{i}}$ is not rejected$\}$;
the analytic sample is then restricted to  $\{i: r_{i} \in (-b,b)\}$.
%This method's tests of each $H_{b_{i}}$ are neither covariance
%adjusted nor multiplicity corrected.
The analyst decides whether this test-and-reduce process
should begin in the middle of $\mathcal{W}$ or at its boundaries.

Similarly, failures of the density test are addressed by restricting
estimation to observations with $|R|>a$, some $a \geq 0$
\citep[e.g.,][]{barrecaetal2011birthweightRDD,eggers2014validity}, and
repeat the test.
If this test rejects, we repeat it, terminating the process at the first $j$ for which
$p_{j} \geq \alpha_{M}$.
By a second application of the SIUP,
the size of this test sequence is equal to the size of each individual
density test.
Taken together, placebo and McCrary tests restrict the sample to
$\mathcal{W} = (-b, b)$ or  $(-b, -a) \cup (a, b)$.

\subsubsection{Non-linear Models for Y as a function of R}
% \citep[\S~5]{lee2008regression, kolesarRothe17}.
%On the other hand,
The methods discussed in Sec~\ref{sec:robust-analys-covar} continue to
apply if $\EE (Y_{C}| R) = \alpha + R\beta$ is relaxed to
$\EE (Y_{C}| R) = \alpha + \vec{f}(R)\beta$, for
$\vec{f}(\cdot)$ a $1 \times k$ vector valued function.
Unfortunately, if the model is fit by OLS, then such relaxation of assumptions can have the unwelcome
side-effect of undercutting the robustness of the analysis.  The
reasons have to do with mechanics of regression fitting.

% \marginpar{Should we also say something about local linear regression
%   here? -AS}
Polynomial specifications
$\EE(Y | R=r) = \sum_{j=0}^{J} r^{j} \beta_{j}$ are common but often
problematic; in combination with ordinary least squares fitting, they
implicitly assign case weights that can vary widely and
counterintuitively \citep{gelman2016high}.
This liability is already
in evidence with $J=1$, the linear specification, where leverage
increases with the square of $r -\bar{r}$.  With an analysis sample
of the form $\{i : R_{i} \in \mathcal{W}\}$ for a ``window''
$\mathcal{W}$ containing  $c$, if $\mathcal{W}$ is slightly too wide then the
sample is contaminated near its outer boundaries, precisely
where leverage is at its highest.
In order to identify leverage points that are also influential,
OLS fitting is sometimes combined with specialized diagnostics such as
plots of Cook's distances \citeyearpar{cook1982residuals}.
The following section will discuss an alternate remedy.

% \subsection{Robust Estimation of $\EE (Y_{C}| R)$}
% \label{sec:robust-altern-ordin}
% Let the running variable be so
% centered that the threshold lies at $r=0$, as in their application.
% Under the variant of \textsc{ancova} relaxing
% $Y_{C} = \alpha_{0} + \beta R + \epsilon$ to
% $Y_{C} = \alpha_{0} + \beta_{-} \indicator{R<0} R + \beta_{+}
% \indicator{R\geq 0}R  + \epsilon$,
% the $Z$-coefficient in an OLS regression of $Y$
% on the interaction of $Z$ and $R$ remains unbiased for
% $\lim_{r\downarrow 0} \EE(Y | R=r) - \lim_{r\uparrow 0} \EE(Y |
% R=r)$, as well as $\EE (Y_{T} - Y_{C} )$.  (It also coincides with $\overline{\dt[\hat\theta]{{Y}}{
%   {R}}}_{Z=1} - \overline{\dt[\hat\theta]{{Y}}{ {R}}}_{Z=0}$, where
% $\dt[(a, b_{0}, b_{1})]{ \mathbf{y}}{ \mathrm{r}} = \mathbf{y} - a - (b_{0}
% \indicator{r < 0} + b_{1} \indicator{r\geq 0})\mathrm{r}$
% and $\hat\theta = (\hat\alpha, \hat{\beta}_{-}, \hat{\beta}_{+})$ are
% as estimated under the interaction model.)



\section{Randomness and Regression in RDDs}\label{sec:theMethod}

% %% RDDs differ from RCTs in an important way:
% %% $Y_C$ typically correlates with $R$, and is therefore not independent
% %% of $Z$; Strong Ignorability \eqref{eq:ignore} is violated.
% %% Can it be relaxed to an assumption that is
% %% simultaneously plausible in RDDs and compatible with similar methods?

% When circumstances call for generating random numbers, such as in
% RCTs or Monte Carlo methods, statisticians often settle instead for
% pseudo-random numbers, deterministic
% outputs of a complex algorithm and an initial seed; a person in
% possession of that seed can exactly predict the pseudo-random draws.
% ``True'' random numbers, on the other hand, derive from physical
% processes that are either chaotic \citep[e.g.][]{uchida2008fast} or
% quantum \citep[e.g.][]{stefanov2000optical}, and are not
% predictable.  However, these processes often contain an element
% of predictability; % \citep{raz2005extractors};
% true random
% numbers are obtained only after separating predictable from
% non-predictable components \citep[see, e.g.][]{Nisan1999148}.%,vadhan2012pseudorandomness}.

% Similarly, RDD analysis decomposes $Y_{T}$ and $Y_C$ as the sum of systematic
% components, a function of $R$, and disturbances, relying on the nature of the decomposition to ensure
% that, vis a vis one another, the disturbances and $Z$ may
% be regarded as random.
% % There is no need
% % to assume the systematic component to equal or be consistent for
% % $\EE (Y_{C}| R)$, nor to make further distributional assumptions on
% % the disturbances.

The analysis of \S~\ref{sec:maria}
mounted an analogy between the Hurricane Maria RDD and a hypothetical
RCT, but only after a preparatory step of modeling and removing the
outcomes' non-random component.
In \S~\ref{sec:maria}, these two steps used two different
datasets---we regressed $Y$ on $R$ using data from years prior to
2017, when Maria hit, and then used 2017 data to estimate effects,
under the assumption of residual ignorability, \eqref{eq:ignore2}.
This luxury is unavailable in the typical RDD, in which both steps
must use the same data, as in \S~\ref{sec:robust-analys-covar}.
This section will describe a generalization of residual ignorability
\eqref{eq:ignore2} to the typical case, along with an accompanying
method that incorporates specification tests described in \S~\ref{sec:specification} and robust
modeling strategies %described in
\S~\ref{sec:robustFitters}.

\subsection{An Analytic Model for RDDs} \label{sec:model-eey-c-r}

This section will formalize residual ignorabilty for the typical RDD,
which relies on a single dataset including variables $Y$, $R$, and $Z$.
The assumption is that, after a suitable transformation,
potential outcomes $Y_C$ are conditionally independent of $Z$.
Hence, causal inference in an RDD may take the perspective
that $Z$ is random due to randomness in $R$.

Suppose the statistician to have selected a \textit{detrending procedure}: a
trend fitter, i.e. a function of
$\{({y}_{i},d_{i},r_{i})\}_{i=1}^{n}$ returning
fitted parameters $\hat{\theta}$ in a sufficiently regular
fashion, along with a
family $\{\dt{\cdot}{\cdot}: \theta\}$ of residual or partial
residual transformations, each mapping data $(\mathbf{y}, \mathbf{r})$ to
$\{\dt {y_{i}}{\mathbf{r}} \}_{i=1}^{n}$.
Appendix~\ref{sec:large-sample-rand}
%\citet[\S~\ref{sec:large-sample-rand}]{lrdauthors:supp}
states the
needed regularity condition, which is ordinarily met by OLS and always
met with our preferred fitters (Section~\ref{sec:test-hypoth-no}).
Then, causal inference in an RDD proceeds from the following assumption:
\begin{ass}{Residual Ignorability}
\sloppy
Given $\mathcal{W}$ with $0< \PP( R \in
\mathcal{W}, R\leq 0) < 1$ and a detrending procedure $(\hat{\theta}, \dt{y}{r})$,
\begin{equation}\label{ycheck}
\dt[\thetaInf]{Y_{C}}{ R }%\mathbf{\Ych}_{C}
\independent {Z}| \{R \in \mathcal{W}\},
\end{equation}
where $\thetaInf$ is the probability limit of $\hat\theta$.
\end{ass}
Residual Ignorability states that, though $Y_C$ may not be independent of
$Z$,  it admits a residual transformation bringing about such
independence. With $\dt[\hat\theta]{Y_{C}}{ R}$ a suitable
partial residual, Residual Ignorability is entailed by the
\textsc{ancova} model (\S~\ref{sec:robust-analys-covar}), or by the combination of any parametric model
for $\EE (Y_{C}| R)$ with a strict null $H$ relative to which the
value of $Y_{C}$ can be reconstructed from the values of $Y$, $D$ and
$Z$ (\S~\ref{sec:using-eqref-test}).
(In either of these cases $\dt[\thetaInf]{Y_{C}}{ R}$
is independent not only of $Z$ but also $R$,
a modest strengthening of~\eqref{ycheck}.)


\sloppy
Assuming Residual Ignorability, inference about treatment effects is
made conditionally, on
$\mathbf{A}= (\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$, $\mathbf{D}_{T},
\{(Y_{Ti}, Y_{Ci}, D_{Ti}, R_{i}) \indicator{{R}_{i} \not\in
  \mathcal{W}}\}_{i=1}^{n})$.
Conditioning on the full data vector when $R \not\in \mathcal{W}$
excludes observations for which \eqref{ycheck} is not assumed.
Conditioning on
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$
removes little of the randomness %entropy
in $\mathbf{R}$, leaving it available as a basis for inference.
Uncoupled to $Y_{T}$s, the detrended  $Y_{C}$s,
$\dt[\thetaInf]{\mathbf{Y}_{C}}{ \mathbf{R}}$,
are in themselves uninformative about $\EE(Y_{T} - Y_{C})$, so
the variables comprising $\mathbf{A}$ are jointly
%\marginpar{$\leftarrow \Delta$ to $(\mathbf{Y}_{C}, N_{0}, N_{1})$?}
S-ancillary, just as $\bm{A}^*$ %$\mathbf{A}^{\dagger}$
was seen to be
in Section~\ref{sec:using-eqref-test}.  As in Fisher-style randomization inference for RCTs, some conditioning variables are
unobserved; but this is not an impediment, at least for large-sample
inferences.

Causal inference based on Residual Ignorability takes place
in four steps: (1) choosing and validating the analysis sample or
bandwidth, (2) choosing an appropriate fitting procedure (we recommend
robust fitters), (3) treatment effect estimation and inference, and
(4) post-fitting diagnostics.
We will discuss each of these steps in sequence.

\subsection{Pre-Fitting Diagnostics and Bandwidth Choice}
\label{sec:bandwidthChoice}

If subject matter knowledge suggests that chance variability intrinsic
to $R$ is typically of magnitude $b$, then RDD modeling focused on
distilling local randomization might set $\mathcal{W}$ to $[-b,b]$. But it
is also sensible to subject such an initial choice to specification
testing (\S~\ref{sec:specification}).

Common RDD specification checks
can be regarded as testing Residual Ignorability with a multivariate
``outcome'' $Y^{*}$ combining the actual outcome $Y$ with covariates $X$---\eqref{ycheck} with
$\mathbf{Y}_{C}^{*} = (\mathbf{X}, {Y}_C)$ in place of $Y_{C}$.

This calls for preliminary detrending procedures, mechanisms to
decompose  $X$ into components that are systematic or unpredictable,
vis a vis $\mathbf{R}$, just as ${\mathbf{Y}_C}$ will later be decomposed.
%% Since $Z$ cannot have an effect on a pre-treatment
%% covariate, the preliminary step of reconstructing
%% $x_{C}$ is not needed.
%%% [Reorg obviated need for the above, as this passage now precedes
%%% discussion of reconstructing Y_C]
Our analysis of the LSO data posits systematic components that are
linear and logistic-linear in $R$, depending on whether $X$ is
a measurement or binary variable. %, paralleling its choice of a linear decomposition
% for the primary outcome, subsequent GPA.  (It is fitting that the $X$- and
% $Y$-specifications mirror each other in model complexity.)
The placebo check adds $Z$ to the specification and tests whether its
coefficient is zero.  We implement these checks as Wald tests with
heteroskedasticity-robust standard errors, as in
\S~\ref{sec:robust-analys-covar}, using the Bonferroni method to
combine placebo checks across covariates.
To compensate for conservatism of
the Bonferroni method, we test with size
$\alpha_{B}=.15$, not $.05$.

We use sequential balance tests to adjust the bandwidth $b$ and, alongside McCrary density tests, to
refine the analysis sample $\mathcal{W}$ (\S~\ref{sec:bandwidth}).
% yielding a test of the
% hypothesis that all of the covariates' residuals are jointly ignorable within the
% given $\mathcal{W}$.

% If the first window tested has form $\mathcal{W} = (-b, b)$,
% write $H_{b}$ for the corresponding joint ignorability hypothesis.
% If the $R$-adjusted covariate placebo tests reject $H_{b}$ then the process is repeated for
% $\mathcal{W}' = (-b', b')$, some
% $b' < b$, and perhaps repeated again if $H_{b'}$ also is rejected.
%  To compensate for conservatism of
% the Bonferroni method, we test with size
% $\alpha_{B}=.15$, not $.05$.

% Writing $b$ for the half-width selected in this manner, i.e. $b=\max$
% \{$b_{i}: H_{b_{i}}$ is not rejected\}, we next apply a McCrary
% manipulation test to $(-b,b)$.   within windows
% $\{ i: |R_{i}| \in (a_{j}, b)\}$, $0 = a_{0} < a_{1}< \cdots < b$,


\subsection{Robust Fitters}\label{sec:robustFitters}
Subsets of the analysis sample that do not satisfy Residual
Ignorability but include influential observations
 can undercut the validity of an RDD analysis. \marginpar{better?}
If such moderate sample contamination may be present --- specifically, contamination of
a $O(n^{-1/2})$-sized
share of the sample --- an OLS fitter will not be consistent for $\hat{\theta}$;
nor will many robust regression methods engineered to meet
objectives other than bounding the influence function
\citep{stefanski1991note}.
Further, no specification test is powerful enough
to reliably detect contamination of this size; power to detect anomalies
affecting only $O(n^{-1/2})$ of the sample can only tend to a number
strictly less than 1.
Hence, the
proper limits of $\mathcal{W}$ will be uncertain
% (c.f. \S~\ref{sec:bandwidth}, \S~\ref{sec:bandwidthChoice}, below),
and some sample contamination may be present.

Instead, consistent estimation of $\thetaInf$ requires a modern MM,  SM, or similar estimator so designed
 as to possess a bounded influence function (\citealp[Thm.~3]{he1991localbreakdown};
\citealp{yohaiZamar1997locallyrobustMestimates}), as opposed to
maximum likelihood.
Such procedures address influence in the course of the fitting process.
 In MM-estimation as in OLS,
coefficients $\beta$ of a linear specification solve estimating equations
$\sum_{i} \psi\left\{ ({y}_{i} -
\vec{x}_{i}\beta)/s \right\} \vec{x}_{i} =0$, where $s>0$ and
$\psi(\cdot)$ is an odd function satisfying $\psi(0)=0$,
$\psi'(0)=1$ and $t\psi(t)\geq 0$; bounded influence fitters replace OLS's $s\equiv 1$ with resistant preliminary
estimates of residual scale, and OLS's $\psi(t) = t$ with a continuous $\psi$
that satisfies $\int_{0}^{\infty}\psi(t)dt < \infty$. This limits
the loss incurred by the fitter for failing to adapt itself to a small
portion of aberrant observations;
it is permitted to instead systematically down-weight them.
The analyses and simulations presented below use MM-estimators with bisquare $\psi$ and
the ``fast S'' initialization of \citet{salibian-barreraYohai2006fastS}.
We are not aware of prior work addressing
potential contamination of an RDD sample with the assistance of
bounded influence MM-estimation.

Surprisingly, given their common origins in %the work of
Huber \citeyearpar{huber1964robust}, MM estimation is not routinely
paired with sandwich estimates of variance, as in \S~\ref{sec:sandwich} and
the surrounding discussion of \S~\ref{sec:robust-analys-covar}.
Exceptions include Stata's \texttt{mmregress} and R's \texttt{lmrob},
which optionally provide Huber-White standard errors
\citep{verardiCroux2009robust,rousseuwetal2015robustbase}.



% : with fitting via OLS,
% this correspondence would be exact, as noted in Section~\ref{sec:robust-analys-covar}; with the robust MM estimation we
% favor, the correspondence is one of large-sample equivalence
% %\citep[Section~\ref{sec:suppl-s-refs}]{lrdauthors:supp}
% (Appendix~\ref{sec:suppl-s-refs}).

\subsection{Treatment Effect Estimation and Inference}
\label{sec:test-hypoth-no}

For inference about $\tau$ under the model
$Y_{T} = Y_{C} + \tau D_{T}$, select a specification
$\mu_{\beta}(\cdot)$ for $\EE(Y_{C}| R)$, %in the application of
%Section~\ref{sec:appl-effect-acad}
such as the
linear model $\mu_{\beta}(R) =\beta_{0} + R\beta_{1}$, and a fitter,
such as OLS.

Separately for each hypothesis $H: \tau=\tau_0$ under
consideration, one calculates
$\mathbf{y}_{H} = \mathbf{y} - \mathbf{d}\tau_{0}$, then
applies the chosen specification and fitter to
$(\mathbf{y}_{H}, \mathbf{r})$.
The combination of the data, the
model fit, and the residual transformation $\dt{\cdot}{\cdot}$ give rise to residuals
$\dt[\hat\theta]{\mathbf{y}_{H}}{\mathbf{r}}$, completing the
detrending procedure. Whether $H$ is rejected or sustained is
determined by the value of the sandwich-based \textsc{ancova} $t$-statistic
in \S~\ref{sec:sandwich}.

In practice it is expedient to use a near-equivalent
test by modifying the detrending
procedure.
When regressing $Y_{H}$ on $R$, include an additive
contribution from $Z$, so that $\mu_{\beta}(R) =\beta_{0} +
R\beta_{1}$ is replaced with $\mu_{(\beta,\gamma)}(R) =\beta_{0} +
\beta_{1}R + \gamma Z$. With sandwich estimates of
$\text{Cov}\{(\hat{\beta}_{H}, \hat{\gamma}_{H})\}$,
the t-ratio comparing $\hat{\gamma}_{H}$ to
$\text{SE}_{s}(\hat{\gamma}_{H})$ induces a generalized score test \citep{boos1992genscoretest}. Implicitly it is a two-sample
t-statistic with covariance adjustment for $R$ (with fitting via OLS,
this correspondence would be exact, as noted in Section~\ref{sec:sandwich}; with the robust MM estimation we
favor, the correspondence is one of large-sample equivalence; see Appendix~\ref{sec:suppl-s-refs}).

As in \S~\ref{sec:robust-analys-covar}, the corresponding
M-estimate of the CACE is the value of $\tau_{0}$ making
$\hat{\gamma}_{H}/\text{SE}_{s}(\hat{\gamma}_{H})$ equal 0; those $\tau_{0}$ for
which $H: \tau = \tau_{0}$ is not rejected at level $\alpha$
constitute a $100(1-\alpha)\%$ confidence interval.  Iteration is
facilitated by regressing $\mathbf{y}$ on $\mathbf{r}$ and
$\mathbf{z}$ with offset variable $\mathbf{d}\tau_{0}$; then only the
offset needs to be modified to test $H: \tau = \tau_{1}$,
$\tau_{1}\neq \tau_{0}$.


\subsection{Post-Fitting Diagnostics} \label{sec:post-fitt-diagn}
Once the M-estimate for the treatment effect has been found, one
inspects the corresponding regression fit for points of high influence.
Bounded influence regression is helpful here.  Besides making
influential points easier to see in residual plots, this limits
effects of data contamination, as non-conforming influence points are
down-weighted as a result of the fitting process. This down-weighting
is reflected in ``robustness weights,'', ranging from 1, for non-discounted
observations, down to 0, for the most anomalous observations.
Plotting % As
% an indicator of robustness to non-constant effects.
% (But is this overloading the discussion here?)
robustness weights against residuals may expose opportunities to
improve the fit of $\mu_{\beta}(R)$, or of the treatment effect model;
plotting them against $R$ may expose contaminated sub-regions
of $\mathcal{W}$ that specification testing failed to remove
\citep{maronna2006robust}.