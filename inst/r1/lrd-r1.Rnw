\documentclass[12pt]{article}
\usepackage{setspace}
\input{lrd-preamble.tex}

% For jebs the appendix is included w/ main paper, I think...
% \externaldocument{lrd-mathyappendix-r0}

<<echo=FALSE,include=FALSE>>=
if(grepl('acs',Sys.info()['login'])){
    dataResults_location <- '~/lrd/inst/' #(I suppose)
    lrd_package_location <- '~/lrd/'
} else {
    dataResults_location <- '~/Dropbox/adam.sales/jebs/dataResults/'
    lrd_package_location <- #modify as needed to fit your setup
        "~/Documents/workingcopies/adamSales/lrd/"
}
@

\title{Limitless Regression Discontinuity}
%  Discontinuity Designs}%

% \author{Adam Sales \& Ben B. Hansen\thanks{The authors thank Susan
%     Dynarski, Roc\'{i}o Titiunik, Matias Cattaneo, Guido Imbens, Brian Junker,
%     Justin McCrary, Walter Mebane, Kerby Shedden, Jeff Smith, the participants in the
%     University of Michigan Causal Inference in Education Research
%     Seminar and anonymous reviewers for helpful suggestions. They
%     also thank Jeffrey Howard and Alexis Santos-Lozada, for sharing
%     non-public research replication materials.  This
%     research was supported by the Institute of Education Sciences,
%     U.S. Department of Education (R305B1000012), the U.S. National
%     Science Foundation (SES 0753164) and an NICHD center grant (R24 HD041028).
%     Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors. }}


\begin{document} % ordinary compilation
%\SweaveOpts{concordance=TRUE}
\maketitle
\begin{abstract}
\input{abstractBH.txt}
\end{abstract}


\doublespacing

<<echo=FALSE,include=FALSE>>=
knitr::opts_chunk$set(echo=FALSE,results='hide',message=FALSE,error=TRUE,warning=FALSE,cache=FALSE,dev='tikz')
@

<<analyses>>=

library(xtable)
library(Hmisc)

latexSN <- function(x){
    x <- format(x)
    x <- sedit(x, c("e+00", "e-0*", "e-*", "e+0*", "e+*"), c("",
        "$\\\\times 10^{-*}$", "$\\\\times 10^{-*}$", "$\\\\times 10^{*}$",
        "$\\\\times 10^{*}$}"))
    x
}

drs <- list.files(dataResults_location)
RDanalysis <- max(drs[grep('RDanalysis',drs)])

if (!exists("analysis_objects")) analysis_objects <- load(paste0(dataResults_location,RDanalysis))
@

\newpage
\section{Introduction} \label{sec:introduction}

In a regression discontinuity design
\citep[RDD;][]{thistlethwaite1960regression}, treatment is allocated to subjects for
whom a ``running variable'' $R$ exceeds (or falls below) a pre-determined cut-point.
\citet{lee2008randomized} argued that the regression discontinuity design features ``local
randomization'' of treatment assignment, and is therefore ``a highly
credible and transparent way of estimating program effects''
\citep[][p. 282]{lee2010regression}.
%According to the local randomization heuristic, regression
%discontinuity designs (RDDs)
%can recover important advantages of randomized controlled trials (RCTs).

Take the RDD found in  \citet*[][hereafter LSO]{lindo2010ability}.
LSO attempt to estimate the effect of
``academic probation,'' an intervention for struggling college students,
 on students' subsequent grade point averages (GPAs).
At one large Canadian university, students with first-year GPAs below
a cutoff were put on probation.
Comparing subsequent GPAs for students with
first-year GPA---the running variable---just below and above the cutoff should
reveal the effectiveness of the policy at
promoting satisfactory grades.

LSO's data analysis, like most RDD studies, used ordinary regression analyses to target an extraordinary parameter.
In Imbens and Lemieux's telling \citeyearpar{imbens2008regression}, for example, the target of estimation is not
the average treatment effect (ATE) in any one region around the cutoff
but rather the ``local'' average treatment effect, or ``LATE'':
the limit of ATEs over concentric ever-shrinking regions, essentially
an ATE over an infinitesimal interval.
Following this ``limit understanding,'' it is common to analyze RDDs
using regression. If $Y$ is the outcome of interest, estimate the functional
relationships of $r$ to $\EE( Y | R=r)$ on either side of the cutoff.  The difference between the
two regression functions, as evaluated at the cut-point,
is interpreted as the treatment effect
\citep[e.g.,][]{berk1983capitalizing,angrist1999using}.%,oreopoulos2006estimating}.

However,
the GPAs in the academic probation study are
discrete, measured in 1/100s of a grade point;
hence, limits of functions of GPA do not exist\footnote{Non-existence
  of the LATE when $R$ is discrete
  \emph{by definition} is arguably a more severe problem than bias due
  to rounding in $R$, as discussed in,
  e.g. \citet{kolesarRothe17} and \citet{dong2015regression}.}.
Further,
re-analysis of LSO's RDD uncovers evidence of ``social corruption''
\citep{wongWing}---some students appear to have
finely manipulated their GPAs to avoid probation.
This necessitates %a ``donut design''
%\citep[c.f.][p. 2119]{barreca2011saving},
excluding subjects immediately on or around the cut-point---precisely
those students to whom the LATE might most plausibly pertain.
Either circumstance calls
into question the appropriateness of limit-based methods.


\citet{cattaneo2014randomization} base RDD inference on
the model that, in effect, the RDD is a randomized controlled trial
(RCT), at
least in sufficiently small neighborhoods of the cut-point.
Under this assumption, once
attention is confined to a such a region, the difference of
$Y$-means between subjects falling above and below the cut-point estimates the ATE within that region.
Despite being natural as a specification of Lee's local randomization concept,
the RCT model involves an independence condition that is rarely plausible in RDDs.
In the LSO example, the data refute this model --- unless one
rejects all but the small share of the sample contained in a narrow band
of the cutpoint, sacrificing power and external validity.

To circumvent limitations of the simple RCT model, and of the limit understanding, this paper weds parametric and local
randomization ideas into a novel identifying assumption termed ``residual
ignorability.'' The residual ignorability assumption and corresponding ATE
estimates pertain to all subjects in the data analysis sample; discrete
GPAs do not pose a threat. Manipulation of the running variable
remains a threat, but one that Section~\ref{sec:theMethod}'s combination
of sample pruning and robust M-estimation is uniquely equipped
to address.

The remainder of Section~\ref{sec:introduction} uses a public health
example to introduce residual ignorability, and review distribution-free analysis of randomized
controlled trials (RCTs).  Limitless RDD analysis
combines these ideas with classical, wholly
parametric methods for RDDs (\S~\ref{sec:robust-analys-covar});
%robust estimation (\S~\ref{sec:robust-altern-ordin});
and RDD specification
tests (\S~\ref{sec:specification}).  Section~\ref{sec:theMethod}
adapts residual ignorability to data configurations typical of education studies, and sets out an analysis plan anticipating common challenges
of RDD analysis.  Section~\ref{sec:appl-effect-acad} executes the plan
with the LSO study, Section~\ref{sec:simulation}
explores the method's performance in simulations and Section~\ref{sec:discussion} takes stock.

Computer code and data for all analyses are provided in an online
supplement.\marginpar{OR: \dots at \url{www.github.com/}\dots}

% Although Huber-White standard errors are common in RDD analysis,
% ``Huberized'' fitting methods are not; they should be.
% Robust M-estimation is uniquely suited to the LSO study, because
% manipulation checks suggest that it calls for a form of robustness to
% sample contamination that only such methods can provide
% (\S~\ref{sec:discussion}). Even when underlying data deficits
% are not suspected, the hybrid of MM estimation with generalized score
% testing that this paper presents (\S~\ref{sec:test-hypoth-no}) can be
% strikingly efficient and robust to misspecification
% of $\EE (Y_{C} | R)$ (\S~\ref{sec:simulation}).

%% The resulting method features assumptions that align better with the
%% intuition that RDDs %heuristic motivation that RDDs
%% feature %leverage
%% natural randomization in the vicinity of a cut
%% point; % difficult sentence
%% offers new options in the face of validity threats; and enables
%% distribution-free, if inexact, causal comparisons between specifically
%% defined groups.
%%

\subsection{The Death Toll from Hurricane \maria}\label{sec:maria}

<<loadMariaDat,echo=FALSE, include=FALSE>>=
library(dplyr)
library(ggplot2)
library(robustbase)
library(pbs)
library(perm)

if(grepl('acs',Sys.info()['login'])){
    #dataResults_location <- '../jebs/dataResults/' #(I suppose)
    lrd_package_location <- '~/lrd/'
} else {
    #dataResults_location <- '~/Dropbox/adam.sales/jebs/dataResults/'
    lrd_package_location <- #modify as needed to fit your setup
        "~/Dropbox/Documents/wcs/lrd/"
}

datM <- read.csv(paste0(lrd_package_location,
                       'extdata/death_file_JAMA.csv')
                )

datM$hurYear <- datM$year==2017
datM$year <- as.factor(datM$year)
rownames(datM) <- paste0(month.abb[datM$month_num],substr(datM$year,3,4))

month_lengths  <-
    c(31, 28.25, 31, 30, 31, 30,
      31, 31, 30, 31, 30, 31)

datM$dailyDeath <- datM$deaths_original/month_lengths[datM$month_num]
datM$adjDeath <- datM$dailyDeath*mean(month_lengths)

## are we adjusting deaths for month length??
## choose one of the following:
#dat$deaths <- dat$adjDeath
datM$deaths <- datM$deaths_original

@

<<modelMariaDeaths,include=FALSE>>=
### model monthly deaths prior to 2017
mod  <-   lmrob(deaths~year+
                     pbs(month_num, knots=seq(2,11,by=3)),
                 data=datM,subset=!hurYear)

### predict monthly
fitCurve <- function(x,year='avg'){
  nd <- data.frame(month_num=x,year=ifelse(year=='avg','2010',year))
  out <- predict(mod,nd)
  if(year=='avg') out <- out+sum(coef(mod)[2:7])/8
  out
}


datM$yhat <- NA
datM$yhat[!datM$hurYear] <- predict(mod)
datM$yhat[datM$hurYear] <- fitCurve(datM$month_num[datM$hurYear],'avg')

datM$resid <- datM$deaths-datM$yhat

datM$Z <- datM$month_num>8

@

<<bandwidth,include=FALSE,eval=FALSE>>=
## are we limiting the window of analysis?
## set eval=FALSE if not.
## modify the following for a different window:
window <- 6:11
datM <- subset(datM,month_num%in%window)

@

<<mariaFigs,eval=TRUE,include=FALSE,cache=TRUE>>=
pd <- rbind(datM,datM)
pd$Deaths <- c(datM$deaths,datM$resid)
pd$type <- factor(c(rep('Death Counts',nrow(datM)),rep('Residual Death Counts',nrow(datM))),
                  levels=c('Death Counts','Residual Death Counts'))
pd$hurYear <- factor(ifelse(pd$hurYear,'2017','2010-2016'))

ggplot(pd,aes(month_num,Deaths,group=year,color=hurYear,size=hurYear,alpha=hurYear))+
  geom_line(size=1)+geom_point()+
  scale_x_continuous(NULL,breaks=1:12,labels=month.abb,minor_breaks=NULL)+ylab(NULL)+
  scale_color_manual(name='Year',values=c('grey','black'))+
  geom_vline(xintercept=9,linetype='dotted')+
  scale_size_discrete(range=c(1,2),guide=FALSE)+scale_alpha_discrete(range=c(.9,1),guide=FALSE)+
  facet_wrap(~type,nrow=1,scales='free_y')+
  geom_hline(data=data.frame(yint=0,type='Residual Death Counts'),aes(yintercept=yint),
    size=2,color='black',linetype='dashed')+
  stat_function(data=data.frame(x=datM$month_num,type='Death Counts',y=datM$deaths,year='avg',hurYear=TRUE),
    aes(x=x,y=y), fun=fitCurve,size=2,color='black',linetype='twodash')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),legend.pos='top')

ggsave('figure/maria.png',width=6.4,height=3,dpi=300)
@







Hurricane Maria struck the island of Puerto Rico on September 20,
2017.
In spite of widespread devastation, for nearly a year after the
hurricane, the Puerto Rican government%
\marginpar{\& the White House,  I think AS: I can't find anything
  about \emph{official} white house numbers, just Trump mouthing off.} officially maintained a
surprisingly low death toll of 64 based on examinations of individual
death reports.
Estimates from investigative journalists and academic
researchers were higher.
Santos-Lozada and Howard's \citeyearpar{santos2018use} authoritative
analysis considered recorded mortality in
months before and after the hurricane, estimating
Maria to have caused 1139 excess
deaths. This section demonstrates residual ignorability,
if not the scope and particulars of the method detailed in Section~\ref{sec:theMethod},
in a limited reanalysis of these monthly death counts.


In this example, let $i=1,\dots,12$ denote the months of 2017 and let Puerto
Rico's monthly death counts $Y_i$ constitute the outcome.
The running variable $R_i=i$ is month order and months are
``treated,'' $Z_i=1$,
if and only if $R\ge 9$, Hurricane Maria having occurred on September 20.
Following \citet{neyman:1923} and \citet{rubin1974estimating}, we may
then take each $i$ to have two potential outcomes: $Y_{Ti}$, a potential
response under the treatment condition (the number of deaths that
would occur were $i$ to fall after Maria); and $Y_{Ci}$, a potential response to
control (the death count were $i$ to fall before Maria).
For each $i$, at most one of $Y_{Ci}$ and $Y_{Ti}$ is
observed, depending on $Z_i$; observed responses $Y$ coincide with
$ZY_{T}+(1-Z)Y_{C}$.
Differences $\tau_i=Y_{Ti}-Y_{Ci}$, $i=9,
\ldots, 12$, represent mortality caused by Maria.  We will discuss RDD estimation
of total excess mortality for 2017, $\sum_{i\ge 9} \tau_{i}$, in due
course.\marginpar{AS: I think $\ge 9$ in the sum avoids confusion}
The remainder of this section demonstrates how to test the
hypothesis $\tau_{i}\equiv 0$, all $i$, using a Fisher randomization
test --- but without assuming % this phrasing disambiguates
the following independence property: % the symbol $\independent$
\begin{ass}{Strong Ignorability, \citealp{rosenbaum1983central}}
\begin{equation}\label{eq:ignore}
Y_{C} \independent Z.
\end{equation}
\end{ass}

Although RCTs validate \eqref{eq:ignore} as a
matter of course, it is too strong to plausibly apply to the mortality series surrounding
Maria.
For \eqref{eq:ignore} to hold, $Z$ must be independent
of monthly death counts that would have been observed in
the absence of ``exposure'' to \maria.
The distribution underlying the September through December counts would
be no different than that of the year's first eight months.
Monthly mortality in Puerto Rico for the period 2010-2017, shown in the left panel of
Figure~\ref{fig:maria}, shows there is no precedent for such an equivalence.
Rather, a marked seasonal trend is apparent, with death counts being higher
in the winter months than during the rest of
the year; \eqref{eq:ignore} cannot be sustained.
(For RDD methodology founded on \eqref{eq:ignore}, see
\citet{cattaneo2015randomization},
\citet{aronowRDDinterference} or \citet{matteiMealliObsStud}.)


Dependence between $R$ and $Y_C$, violating
\eqref{eq:ignore}, is common in RDDs; in the tradition of regression
discontinuity, we address this dependence by modeling conditional
mean function of $Y_C$ given $R$.
%% This model is fit using the same data, namely $Y$, $R$, and $Z$, that
%% is used to estimate treatment effects, a practice which leads to a
%% number of statistical and causal complications.
%% In this example, we will side-step those complications by exploiting
%% the historical death counts from years 2010--2016 in the SLH dataset.
%% The remainder of the paper will discuss the typical case in which such
%% historical data are not available.
Inspection of the 2010--16 mortality series (Figure~\ref{fig:maria}) reveals
a periodic, non-linear relationship
between calendar month and death count, with some years appearing to be
more hazardous than others.
To accommodate these factors, we regressed 2010-2016 monthly death counts
on dummy variables for year and a periodic b-spline for month order,
with knots at February, May, August, and November.
There are several outlying observations, one of which \citet{santos2018use} remove
from the sample prior to analysis.
%for instance, death counts in the final three months of 2014---in particular,
%October---were unusually high.\footnote{We are unaware of weather
%  or other events explaining the mortality spike of late 2014. Official
%  statistics \citep[][p. 44]{PRsalud} attribute
%  most of 2014's excess deaths to common chronic diseases.}
Rather than identifying and removing outliers informally, we fit the regression model
using a robust redescending M-estimator, which systematically
down-weights and sometimes rejects outliers that would otherwise be
influential \citep{maronna2006robust}.
This model fit is displayed as a dashed black line in Figure
\ref{fig:maria}.

Now let $\hat{Y}_C(R_i)$ be that model's prediction for month $i$ in
2017, and let $\dt[]{Y_i}{R_i} = Y_i-\hat{Y}_C(R_i)$ be the prediction residual,
with potential values $\dt[]{Y_{Ci}}{R_i}=Y_{Ci}-\hat{Y}_C(R_i)$ and
$\dt[]{Y_{Ti}}{R_i}=Y_{Ti}-\hat{Y}_C(R_i)$.
Instead of \eqref{eq:ignore}, we assume only that the model we have fit to
pre-2017 monthly death counts captured and removed seasonal
mortality trends, such that the potential residuals $\{{\resid}_{Ci}: i\} \equiv \{ \dt[]{Y_{Ci}}{R_i} : i\}$
can be regarded as random, at least as far as $Z$ is concerned:
%\begin{ass}{$\resid$-Ignorability}
\begin{equation}\label{eq:ignore2}
\resid_{C} \independent Z .
\end{equation}
%\end{ass}
The right panel of Figure \ref{fig:maria} shows residualized death
counts $\resid = \dt[]{Y}{R}$ as a function of month order $R$.
Seasonal mean trends are no longer in evidence; \eqref{eq:ignore2} is
thus more plausible than the standard ignorability assumption \eqref{eq:ignore}.
Aside from a technical elaboration that will be necessary to apply our method
in the general case (\S~\ref{sec:model-eey-c-r}),
assumption \eqref{eq:ignore2} is Residual Ignorability, this paper's alternative
to Strong Ignorability as a basis for analysis of RDDs.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{figure/maria.png}
\caption{Monthly death counts in Puerto Rico from years 2010--2017, before and after
  residualization. The plot on the left shows monthly death counts,
  adjusted for month length. A vertical dotted line denotes September,
  the month \maria\ hit. The fit of the robust model described in the
  text is shown as a dashed black line. The plot on the right shows
  the monthly residuals of the model fit, with a dashed line denoting
  zero.}
\label{fig:maria}
\end{figure}

\subsection{Using~\eqref{eq:ignore2} to Test the Hypothesis of Strictly No Effect}
\label{sec:using-eqref-test}

In parallel with Fisherian analysis of RCTs \citeyearpar{fisher:1935},
which can be regarded as conditioning on the potential outcome random vector $\bm{Y}_C \equiv \{Y_{Ci}: i\}$,
our Maria analysis conditions on the potential residual vector
$\bm{\resid}_C\equiv\{\resid_{Ci}\}_{i=1}^{12}$.
(Here and throughout, boldface indicates the concatenation of
$n$ variables or constants.)
%An oddity of conditioning on $\bm{\resid}_C$, or $\bm{Y}_C$,
%is that it does not correspond to any simple reduction of observed
%data, since neither $\bm{Y}_C$ nor $\resid_{Ci}$ is observed unless $Z_{i}=0$.
The approach is feasible when testing
``strict'' null hypotheses, hypotheses that designate a value for each
$\resid_{Ti} - \resid_{Ci}$ with
$Z_{i}=1$, not just $\EE(\resid_{T} - \resid_{C} | Z =1)$.
This includes the hypothesis
of strictly no effect, $H_{0}: Y_{T} \equiv Y_{C}$, under which
$\bm{\resid}_{T} \equiv \bm{\resid}_{C}=\bm{\resid}$.
%Recall that $\resid_T-\resid_C=Y_T-Y_C$, so null hypotheses about
%$Y$, which are ultimately of interest,
%are exactly equivalent to null hypotheses about $\resid$, which may be
%more readily tested.


For this analysis $Y$, $R$, and $Z$ data for years 2010--2016 are treated as fixed --- formally,
inference will be made after conditioning not only on $\bm{\resid}_C$ but also on the values of $\{(Y_i, R_i, Z_i) : i \leq 0\}$.
To make use of Fisher's \citeyearpar{fisher:1935} permutation technique,
we likewise condition on the realized sizes $N_{1}$
and $N_{0}$ of the treatment and control group samples%
\footnote{\textit{Design of Experiments} [\citeyear[ch.~1]{fisher:1935}]
takes sample sizes to be fixed, but Fisher's example of the purple flowers %NB: In Little's version,
%% the flowers are red not purple.  (In case a reviewer says they couldn't locate the
%% example there...)
[\citealp[\textit{e.g.},][]{little:1989,upton1992FET}] demonstrates his view that random $N_{0}$ and $N_{1}$
should be treated as fixed after conditioning upon them.
For quite different assumptions supporting permutation tests in RDDs,
see \citet{canayKamat17RDDpermutations}.
}%
, where $N_{j} \equiv \sum_{i=1}^{12} \indicator{Z_{i}=j}$.
Such conditioning is appropriate because the conditioning statistic
$\bm{A}^*=\{\bm{\resid}_C,N_1,N_0; (Y_{-71}, R_{-71}), \ldots, (Y_{-1}, R_{-1})\}$
is ancillary to, i.e. carries no information about, the target of estimation
$\EE (\resid_{T}-\resid_{C})$.  ($\bm{A}^*$ carries full information about $\bm{\resid}_{C}$ but none on $\bm{\resid}_T$.
$\bm{A}^*$ would not be ancillary to targets of the form $\EE(Z| \mathcal{E})$
or $\EE(\resid_{C}| \mathcal{E})$, some event $\mathcal{E}$.)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% change this if we use a different window of analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<estimates,include=FALSE>>=
N1 <- sum(datM$hurYear&datM$Z)
N0 <- sum(datM$hurYear&!datM$Z)
N <- N1+N0
cardZ <- choose(N,N1)

## adapted from http://rsnippets.blogspot.com/2012/04/generating-all-subsets-of-set.html
all.subsets <- function(set,size) {
  n <- length(set)
  bin <- expand.grid(plyr::rlply(n, c(F, T)))
  if(!missing(size)) bin <- bin[rowSums(bin)==size,]
  bin
}

subs <- all.subsets(1:N,N1)
stopifnot(nrow(subs)==cardZ)

rr <- datM$resid[datM$hurYear]
z <- datM$Z[datM$hurYear]

testStat <- function(Z,RR)
  sum(RR[Z])

ts0 <- apply(subs,1,testStat,RR=rr)
stopifnot(length(unique(ts0))==cardZ) ## no Test stat value appears more than once; simplifying

pfun <- function(tau){
  RR <- rr-z*tau
  ts <- testStat(z,RR)
  TS <- apply(subs,1,testStat,RR=RR)
  nless <- sum(TS<ts)
  nmore <- cardZ-nless-1
  pHigh <- (nmore+.5)/cardZ
  pLow <- (nless+.5)/cardZ
  c(nless=nless,nmore=nmore,p=2*min(pHigh,pLow),diff=abs(mean(TS)-ts))
}

pval <- pfun(0)

tau <- 0
rej <- TRUE
diff <- NULL
while(rej){
  tau <- tau+1
  p <- pfun(tau)
  diff <- rbind(diff,c(tau,p['diff']))
  rej <- p['p']<0.05
}
lb <- tau
p.lower <- p
while(!rej){
  tau <- tau+1
  p <- pfun(tau)
  diff <- rbind(diff,c(tau,p['diff']))
  rej <- p['p']<0.05
}
ub <- tau
p.upper <- p

hl <- diff[which.min(diff[,2]),1]

@
Under $H_0$, we may exactly enumerate the sampling distribution of any
test statistic $T(\bm{\resid}_C,\bm{Z})=T(\bm{\resid},\bm{Z})$ conditional on
$\bm{A}^*$; the permutational p-value is found by comparing a test statistic
to its conditional distribution thus enumerated.
In this analysis $\hat{Y}_C(R_i)$ cannot itself be influenced by \maria,
since it is based on a model fit to pre-\maria\ death counts \citep[c.f.][]{rebarPaper}.
Hence, the effect of \maria\ on $\resid$ (i.e. $\resid_T-\resid_C$) is exactly equal to its effect on $Y$.
The null hypothesis $H_0$ states that hurricane
Maria caused precisely no change to each month's death count, nor to its residual.
Under $H_0$, we condition on $\bm{A}^*$ and calculate the
sampling distribution of the treatment group residual mean $T(\bm{\resid},\bm{Z})=\bar{\resid}_{Z=1}$, by calculating its
value for all $\binom{N_0 + N_1}{N_1} =\binom{\Sexpr{N}}{\Sexpr{N1}}=\Sexpr{cardZ}$ possible permutations
of $\bm{Z}$.  The null distribution of $T(\bm{\resid}, \bm{Z})$ is simply that of the
mean of a size-4, without-replacement sample from $\{E_1, \ldots, E_{12}\}$.
It turns out that only \Sexpr{round(pval['nmore'])} permutations of $\bm{Z}$ result in higher
test statistic values than the realized value
$\bar{\resid}_{z=1}=$\Sexpr{round(testStat(z,rr))} (which is unique in
  the distribution).
This implies a two-sided ``mid'' p-value \citep{agresti2005comment} of
$2(\Sexpr{pval['nmore']}+0.5)/\Sexpr{cardZ}\approx\Sexpr{round(pval['p'],2)}$
for $H_0$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This combination of regression and permutation testing applies just as
readily to test the hypothesis $\resid_T = \resid_C + c$, for any
constant $c$.  In the Maria example, no such hypothesis is sustainable
at level 0.05 unless $c\geq$ \Sexpr{round(lb,-1)}, corresponding to
\Sexpr{round(4*lb, -1)} excess deaths due to the hurricane.  Upper
confidence limits and Hodges-Lehmann-type estimates of the effect can
also be obtained in this way. Rather that pursuing this approach
further, we now turn to developing a residual ignorability-based
procedure using M-estimation \citep[][also called ``generalized
estimating equations'' or ``generalized method of
moments'']{huber1964robust,stefanski2002calculus}, which is better
adapted to data scenarios without the luxury of separate samples for estimation of
trends in the absence of treatment and of treatment effects themselves.


\input{lrd-sec2+3-r1}

\section{The Effect of Academic Probation}\label{sec:appl-effect-acad}
\begin{wrapfigure}{l}{0.6\textwidth}%[htb]
%\begin{figure}[!ht]
\centering
%\mbox{
%\subfigure[]{
\includegraphics[width=0.58\textwidth]{figure/rddFig-1.png}%}
%\quad
%\subfigure[]{\includegraphics[width=.45\textwidth]{../figure/hs_gpaFig-1.png}}
%}
\caption{%(a)
 \texttt{nextGPA} by centered first year GPA, binned by unique first-year GPA values. Point size is proportional to
the number of students in the bin. %First-year GPAs (x-axis) have been
%  centered around a campus-specific cut-point for academic probation. Subsequent
% GPA (y-axis) has been averaged according to first-year GPA. %(b)
%Log-transformed high-school grade percentile (a covariate), also averaged by first-year college
% GPAs.
}
\label{LSO}
%\end{figure}
\end{wrapfigure}
% Their research question was whether AP caused a change in
% \texttt{nextGPA}: did students on AP tend to have higher (or lower) subsequent
% GPAs?

One outcome in LSO's academic probation (AP) study was
\texttt{nextGPA},
grade-point average
for the first term after the first year in which the student was
enrolled, ordinarily the next fall.
Figure~(\ref{LSO}) plots it against
first-year GPA.
In all but 50 of 44,362 cases, being on AP coincided with whether
first-year cumulative GPA  ---  the running
variable, $R$ --- fell below a cutoff.
The university in question had three campuses, two of which had
cutoffs of 1.5; the other had a cutoff of 1.6.
To combine data from the three schools, LSO centered each student's
first-year GPA at the appropriate $c$, making $r_i$ the difference of student $i$'s
realized first year GPA and the cutoff at his or her campus.   Figure~\ref{LSO}
follows LSO in this, displaying on its $x$-axis these $r_{i}$s; it also averages \texttt{nextGPA} values over students
with equal first-year GPA, as opposed to plotting students individually.
%Then, $Z=\indicator{R\le 0}$.
There is both a discontinuity in \texttt{nextGPA} values as $R$
crosses 0, and a distinctly non-null regression relationship on either side of that
threshold.   How large an AP effect may we infer from
these features?  How much of the data bear directly in this inference?

\subsection{Choosing $\mathcal{W}$ and $\mu_{\beta}(\cdot)$} \label{sec:choosing-mathcalw-f}
The region %A relevant region in which to estimate a treatment effect is within
$\mathcal{W}_{0.5} =  \pm $ 0.5 %$0.3$
grade-points %.
%Conventionally, $0.3$ represents the difference in grade points
%between a C, say, and a C-, or any other grade half-step.
%This
includes students whose AP status could change if their
grades in half their classes changed by a full mark (say from D
to C).
Simplicity recommends a linear specification for the outcome
regression on the forcing variable, and the scatter of $Y$ versus $R$
did not suggest otherwise; so we designated
$\mu_{\beta}(R_i)=\beta_0+\beta_1 R_i$
%(subject to potential revision following diagnostic checks).
and proceeded to specification checks, as
discussed in Section~\ref{sec:specification}.

Following LSO, we conducted placebo tests with high-school
grade percentile rankings, number of credits attempted in first year
of college, first language other than English,
birth outside of North America, age at college entry,
and which of
the university's 3
campuses the student attended.
For the measurement variables, this amounted to fitting
\textsc{ancova} models, whereas binary covariates
were decomposed as logistic-linear in $R$ and $Z$;
in both cases subsequent Wald tests of $Z$'s coefficient used
Huber-White standard errors.
% High school grade percentiles were
% logit-transformed, improving the fit of the linear model.
For $\mathcal{W}_{0.5}$  each (Bonferroni-corrected) $p$-value
exceeds $0.2$; downward adjustment of the bandwidth
is not indicated.

% Unfortunately, there is no lagged measurement of the outcome
% \citep{maynard2013strengthening}; the data do not speak to the
% possibility of
% $\EE( Y_{C}| R= r)$ being more nearly linear to the left than to
% the right of the threshold, where it is not directly estimable.  On
% the other hand, Figure~(\ref{LSO}) %\marginpar{Remember to connect
% %  to IK failure!}
% indicates that the regression of (logit-transformed)
% high-school grade percentiles on $R$ is more curved than the outcome's
% regression on $R$, and the linear residualization used in our placebo
% check makes no allowance for curvature. In consequence, this
% conditional association
% test rejects if curvature in the high-school
% grades regression is detectable, lending the overall assessment a
% measure of conservatism. In any case, for $\mathcal{W}_{0.5}$  each
% (Bonferroni-corrected)  $p$-value exceeds $0.2$; downward adjustment of the bandwidth
% is not indicated.
%% The combined differences statistic of
%% \citet{hansen:bowers:2008}, %suggested an omnibus statistic
%% %for testing
%% applied to the 7 residual vectors as determined on
%% $\mathcal{W}_{0.5}$, yielded a p-value of $p=$Sexpr{round(balance0.5,2)}, greater
%% than $\alpha_\mathcal{F}=0.1$.  (This is an approximate, Normal-theory
%% $p$-value, as computed with software of \citet{bowers2016ritools}.)

<<echo=FALSE,include=FALSE>>=
stopifnot(SHmain[['bal.pval']]>.2)
stopifnot(mccrary1<.001)
stopifnot(max(c(frandsen1[,'p'],frandsen2[,'p']))<0.001)
@

The McCrary density test \citep{mccrary2008manipulation} identifies a
discontinuity in the running variable at the cut-point ($p<0.001$).
The Frandsen test for manipulation
in a discrete running variable \citep{frandsenTest}
likewise detected an anomaly at a wide range of tuning parameter values
($p<0.001$ for $0\le k\le0.1$).
AP is a
dubious distinction, and savvy students may try to avoid it. %, perhaps
%by dropping a course, pleading with an instructor, or otherwise
%manipulating their GPAs.
Inspection of the distribution of $R$
reveals an unusual number of students whose first-year GPAs were
exactly equal to the AP cutoff, $R=0$.
 %\Sexpr{latexSN(signif(mccrary1,2))}.
Although the forcefulness of this rejection may be due
in part to the discreteness of the GPA distribution, the finding is
corroborated by the fact that the number of students attempting four
or fewer credits was also unusually high in the $R=0$ subgroup,
suggesting that some students dropped courses to dodge AP.
% GPAs of four or fewer credits are necessarily clumpier than
% GPAs of 4.5 credits or more since they average fewer components.
% Indeed, $R$'s other high points within $b=0.5$ of the cutpoint,
% $R=$ Sexpr{paste(paste0(hipts,conn.chars), collapse=" ")}, also
% feature more students attempting four or fewer credits.
In any event, repeating the McCrary procedure
in a window $\mathcal{W}=\{i: |R_i|\in (0,0.5)\}$, with students with
$R=0$ removed, results in a p-value of \Sexpr{signif(mccraryDougnut,2)}.

\subsection{AP Outcome Analysis}
\begin{wrapfigure}{l}{0.6\textwidth}%[htb]
%\begin{figure}[ht!]
{\centering
\includegraphics[width=.58\textwidth]{figure/weightPlot-1.png}
}
\caption{Robustness weights from robust MM estimation of the model
  that $\EE(Y_{C} | R=r)$ is linear in $r$ while $Y_{T} = Y_{C} + \tau_{0} D_{T}$, with $\tau_{0}$ set at $\hat{\tau}=$\Sexpr{round(SHmain[['CI']][3], 2)}.}
\label{fig:robweights}
%\end{figure}
\end{wrapfigure}
Table~\ref{tab:results} gives a set of %p-values, point estimates, and
%interval
estimates for the effect of AP.
The first row of Table~\ref{tab:results} gives our main result, with
window of analysis
$\mathcal{W}=\{i: |R_i|\in (0,0.5)\}$ and a linear model of $\EE(Y_C|R)$,
$\mu_\beta(R)=\beta_0+\beta_1 R$, estimating the TOTE based on subjects'
received treatments $D$. For the best fitting version
of the model, robustness weights range from .28 to 1.
These weights
show little association with $R$,
%(Supplemental Appendix~\ref{apnd:rob-plot}).
although the
lowest weights occur just above the cut-point and near $\mathcal{W}$'s
edges (Figure~\ref{fig:robweights}).

For students in $\mathcal{W}$, the main analysis estimates an average
treatment effect of \Sexpr{round(SHmain[['CI']][3], 2)}, with a 95\% confidence
interval of \Sexpr{ciChar(SHmain[['CI']][1:2])}.

The next three rows relax each of the main model's specifications.
The row labeled ``Adaptive $\mathcal{W}$'' reports the results using
 the wider, adaptively chosen window $\mathcal{W}_{a}=\{i:R_i\in
(0,$\Sexpr{SHdataDriven[['W']][2]}$)\}$.
The ``Cubic'' row allows for a cubic
relationship between first year GPA and subsequent GPA, with
$\mu_\beta(R)=\beta_0+\beta_1R+\beta_2R^2+\beta_3 R^3$.
This specification performed well in simulations %available in an
%online supplement.%
described in Section~\ref{sec:simulation}. %Supplemental Appendix \ref{apnd:poly-sim}. %
Finally, the ``ITT'' row gives an ``intent to treat'' analysis,
ignoring the difference between students' actual probation and what we
would have expected based on their GPAs.

According to all four analyses, AP gave a modest benefit
over this range.
%% The results largely agree, although the fuzzy
%% RDD strategy gives a larger confidence interval, straddling zero.
%% The effect of AP students' subsequent GPAs, for students with
%% first-year GPAs about half a grade point from
%% the AP cutoff is between \ Sexpr{round(CI0.5[1],2)} and
%% \ Sexpr{round(CI0.5[2],1)}, with 95\% confidence.

% latex table generated in R 3.3.1 by xtable 1.8-2 package
% Tue May  2 18:44:54 2017
%\begin{wraptable}{L}{0.75\textwidth}%[htb]
\begin{table}[ht]
\centering
\input{tab-results}
\caption{AP impact estimates using the method of Section~\ref{sec:theMethod} and
  variants that select $\mathcal{W}$ adaptively, model the outcome
  as a cubic function of the running variable, or estimate
  intent-to-treat effects.}
\label{tab:results}
\end{table}
%\end{wraptable}


\subsection{Comparison with Selected Alternatives} \label{sec:simil-diff-with}

%\subsection{LSO Results from Other Methods} \label{sec:lso-results-from}

For comparison purposes,
we re-analyzed the LSO data using two alternative methods: local
linear regression \citep[e.g.][]{imbens2012optimal}, which targets the difference of limits
of regression functions, and the permutational
method of \citet{cattaneo2014randomization}, which does not
require a limit-based interpretation.
% These methods differ from ours and from each other, both in the data
% samples they analyze, and in their effect estimation.
The three sets of results are in Table \ref{tab:alt}.

% latex table generated in R 3.2.3 by xtable 1.8-0 package
% Wed Jun 29 11:46:37 2016
%\begin{wraptable}{L}{0.75\textwidth}%[htb]
\begin{table}[ht]
\centering
\input{tab-alt}
\caption{The effect of Academic Probation from our main analysis compared with permutation and OLS analyses.}
\label{tab:alt}
\end{table}
%\end{wraptable}

The local linear approach used the widest window, including
observations with $R<$\Sexpr{round(IK[['W']][2],2)}, and the local
permutation approach used the smallest window, only including
observations with $R<$\Sexpr{CFT[['W']][2]}.
The effect estimates from our method and local linear regression
largely agree, while the local permutation approach finds a smaller
effect, with a confidence interval excluding the other two point
estimates.

% We implemented the local linear approach using Dimmery's
% .
The data sample for the local linear approach differed from ours in two ways.
First, since the goal of local linear analysis is to estimate regression functions at the
cutoff, it makes little sense to discard observations with $R=c$,
despite counter-indications from the McCrary and Frandsen tests (Section
\ref{sec:choosing-mathcalw-f}).
Second, the \citet{imbens2012optimal} bandwidth is based on non-parametric estimates of the curvature of the
mean function $\EE(Y|R=r)$ rather than on covariates.
We computed this bandwidth using Dimmery's \citeyearpar{rdd}
implementation in \texttt{R}, using the ``rectangular'' kernel option
to facilitate comparisons across methods.
The resulting $\mathcal{W}$ is the widest shown in
Table~\ref{tab:alt} --- too wide, from viewpoints either of
Section~\ref{sec:model-eey-c-r}, or of local permutation
analysis.  For example, Section~\ref{sec:bandwidthChoice}'s placebo tests reject comparability
of detrended covariate residuals when applied with this $\mathcal{W}$
($p$=\Sexpr{round(IK[['bal.pval']],3)})%; \citet[][p. 2304]{cct}
%and \citet[][p. 14]{kolesarRothe17} have discussion of this
%possibility.
. %\marginpar{AS: Removed refs to CCT \& KR, for length \& flow. ---B \ldots}
% AS It didn't seem to me that the passage demanded a cross-ref. I
% found it disruptive to our story to reference theirs in without
% explaining what it was that they say about the "too large"
% possibility. We cite KR elsewhere, and I think there's more to be
% lost than gained from citing CCT.
% This being said I didn't track down your specific page refs -- if
% there's something I'm missing that might tip the scales in the other
% direction, do speak up. -BH

Local linear effect estimation resembles our method,
in that both require analysts to specify and fit models for $Y_C$ and $\tau$.
However, whereas ours calls for robust M-estimation, the local linear method
uses a variant of weighted least squares---when the kernel is
rectangular, as in our example, this reduces to OLS within the chosen window.
Confidence intervals are of the Wald type---that is, $\hat{\tau} \pm
z_{\alpha/2} \mathrm{SE}(\hat{\tau})$,
where $z_{\alpha/2}$ is an appropriate normal or t-distribution
quantile---rather than inversions of a family of hypothesis tests.
Recent elaborations and extensions include those of \citet{cct},
\citet{imbens2017optimized} and \citet{kolesarRothe17}.
% More recent iterations of the local linear approach
% \citep[e.g.][]{cct,kolesarRothe17} include bias corrections and
% other modifications to the confidence interval, while retaining the
% overall structure.
% \citet{imbens2017optimized} estimate effects with a linear combination of
% outcome values, with coefficients chosen to optimize a minimax criterion.

Similar to our approach, the
permutation-based procedure of \citet{cattaneo2014randomization} uses
covariates to select a window of analysis.
However, its covariate balance tests do not adjust for $R$, instead
seeking a $\mathcal{W}$ over which $X \independent Z$ is not
rejected.
In the LSO case, $\mathcal{W}_{b}$ is rejected as long as $b\geq$
\Sexpr{CFT[['W']][2]}.  Recall that our $R$-adjusted check found
no fault with bandwidths as large as \Sexpr{SHdataDriven[['W']][2]}.
(In both cases, we tested each $\mathcal{W}$
at level $\alpha=0.15$, addressing multiplicity of covariates using
the Bonferroni method.)

Within the chosen window, the permutational approach estimates effects
under the assumption of
ignorability of treatment assignment, $Z \independent Y_C$.
Failure of this assumption may explain differences between the
permutation-based estimate of the AP effect and estimates from the
other two methods shown in Table~\ref{tab:alt}.  A
correlation between \texttt{nextGPA} and $R$---possible even in
regions in which covariate balance cannot be rejected---would
bias a positive effect toward zero.
The Bayesian method of \citet{liMatteiMealli2015BayesianRD}, which
begins from a similar ignorability assumption,
nevertheless models the relationship between $R$ and $Y$ within the
chosen region, to guard against the assumption's failure.
Doing so in the LSO dataset yields a similar point estimate as the
permutational approach, but with a wider confidence interval that
includes the estimates from our and the local linear approach.


\section{Simulation Studies}\label{sec:simulation}
\subsection{Point and Interval Estimates for Three RDD Methods}\label{sec:levelPower}
<<loadSim, results="hide", include=FALSE>>=
source(paste0(lrd_package_location, 'R/simulations.r'))
source(paste0(lrd_package_location, 'R/displaySim.r'))
## NOT RUN:
## outcomeSim <- totalOutcomeSim()
## save(outcomeSim,file='dataResults/outcomeSim.RData')
sims <- sort(grep('outcomeSim',list.files(dataResults_location),value=TRUE),decreasing=TRUE)
#load(paste0(dataResults_location, sims[1]))
@
Our first simulation study compares the performance---bias and confidence interval
coverage and width---of our ``limitless'' method to local-OLS, and
local-permutation methods.
Across all simulation runs, the running variable $R$ was distributed as
$\mathrm{Uniform}(-0.5,0.5)$, the cutoff was $c=0$, and control potential
outcomes as $Y_C=0.75R+\epsilon$, where the 0.75 slope was chosen to
approximately match the estimated slope from the LSO study.
Within this framework, we varied three factors: (a) sample size, (b)
the distribution of regression error $\epsilon$, and (c) the
treatment effect.
We considered three sample sizes: $n=50$, 250, and 2,500.
Regression errors were distributed as either Normal or Student's $t$
with 3 d.f.; to mimic the LSO data, we forced the errors to have a
standard deviation of 0.75.
Finally, the treatment effect was either exactly
zero---i.e. $Y_T= Y_C$---or was set as random, $Y_T=
Y_C+\eta$, where $(\sqrt{3}/0.75)\eta\sim t_3$, i.e. $\eta$
was given the same distribution as $\epsilon$.
Each simulation scenario was run 5,000
%\Sexpr{format(ncol(outcomeSim[[1]]),big.mark=',')}
times.


\input{tab-levelSimulation}


The results are displayed in Table~\ref{tab:level}.
With a linear data generating model and a symmetric window, the bias
for the local permutation approach will generally be equal to the
product of the slope and the bandwidth; in our scenario its bias was
approximately $0.75\times 0.5\approx 0.37$ across simulation runs.
The coverage of permutation confidence intervals decreased with sample size.
The limitless and local OLS methods were approximately unbiased, and
95\% confidence intervals achieved approximately nominal coverage for
$n=250$ or 2,500, and under-covered for $n=50$.
Notably, random treatment effect heterogeneity did not affect bias or
coverage.

Across the board, the local permutation method gave the smallest
confidence intervals; however this came at the expense of coverage.
Our limitless RD method tended to have equal or slightly narrower
interval widths than the local OLS approach, with greater advantage
when $\epsilon$ was distributed as $t_3$, than when $\epsilon$ was
normally distributed.


\subsection{Polynomial Regression}

\input{tab-polynomialSimulation}


\begin{figure}
\centering
% chunk below  generates figure/dgms-1.pdf , but also includes
% unwanted warnings in latex file.  To regenerate dgms-1.pdf,
% set eval to TRUE, comments out the \includegraphics{} and compile;
% then come back and reset it to FALSE
%\includegraphics[width=\maxwidth]{../figure/dgms-1}
<<dgms,fig.height=2,fig.width=6,cache=TRUE,eval=TRUE>>=
#par(mfrow=c(1,3))
dgms()
@

\caption{Data generating models for polynomial simulation.} %\texttt{(Note: If restoring polySim to body of paper, don't forget to take
%it out of appendix.)}}
\label{fig:dgms}
\end{figure}

When $Y$ may not be linear in $R$, flexibility in the
$\mu_{\bm{\beta}}(R)$ function takes on added importance.
% Though higher-order polynomials in $R$ offer that flexibility,
% \citet{gelmanImbens} argue against using them in RDD settings, in part
% because they magnify the leverage of outliers.
% However, robust regression may alleviate this concern.
We ran an additional simulation to explore the potential of robust
polynomial regression to mitigate influence, as discussed in
\S~\ref{sec:test-hypoth-no} above, while adding flexibility
to the specification of the $Y_{C}$ on $R$ regression.
We compared limitless RD analysis, with $\mu_{\bm{\beta}}$ a polynomial in $R$ with
degree 1, 2, 3, 4, or 5 to analogous estimates from OLS.
In the OLS regressions, we followed the advice of \citet[e.g.][p. 318]{lee2010regression} and
included interactions between the $R$-polynomial and $Z$.
Finally, we compared these methods to local-linear regression with the
triangular kernel and the bandwidth of \citet{imbens2012optimal}.
The OLS and limitless methods used the entire range of data.
We simulated data sets of size $n=500$ by drawing $R$ and $\epsilon$
from Uniform $(-1,1)$ and  $t_3$ distributions respectively, then
adding $\epsilon$ to one of the three functions of $R$ shown in
Figure~\ref{fig:dgms} to form $Y_{C}$.
%Additional simulations with standard normal errors are displayed in
%Appendix B.

Table~\ref{tab:poly} displays the results.
For the linear data generating model, all estimates were unbiased,
while root mean squared errors (RMSEs) were lowest for the limitless
method.
For the non-linear data generating models, the limitless estimators
using linear and quadratic specifications had substantial bias, and
bias was much lower for higher-order polynomial specifications.
In contrast, OLS estimators of all polynomial degrees were heavily
biased.
OLS and limitless estimation sharply diverge in the quality of their
point estimates: the RMSE of OLS estimation
increases rapidly with polynomial degree, but the limitless method's does not.
With the method based on robust estimators but not with OLS, increasing
the degree of the polynomial improved accuracy of
estimation in the presence of non-linearity.

The local linear model does not employ higher-order global
polynomials. It fared better than OLS, and had similar bias but
higher RMSE than the limitless method using higher-order polynomials.



\section{Discussion} \label{sec:discussion}
Beginning with \citet{thistlethwaite1960regression}, the dominant mode
of RDD analysis has relied on regression modeling; in its modern
formulation, targeting parameters such as the LATE which are defined
in terms of limits as $r\rightarrow c$.
However, in RDDs with discrete running variables such as LSO's, neither
$\lim_{r\downarrow 0} \EE (Y |R=r)$ nor $\lim_{r\uparrow 0} \EE (Y | R=r)$
exists, except perhaps as $\EE (Y | R=-.01)$ or $\EE (Y | R=0)$.
A separate embarrassment for  limit-based modeling of RDDs occurs if
a donut-shaped $\mathcal{W}$ is necessary to address potential
manipulation of the running variable, as we found to occur with the
LSO data.

% This approach has been largely successful, but as we have seen,
% encounters problems when $R$ is discrete, when donut designs are
% called for, or when researchers are interested in estimating an effect
% for a specific set of subjects.
An alternative approach \citep[e.g.]{cattaneo2014randomization,liMatteiMealli2015BayesianRD} takes the ``local
randomization'' heuristic more literally, analyzing data in a small
region around the cutoff as if it were from a randomized experiment.
However, this approach assumes that potential outcomes are
independent of $R$ in a window around the cutoff.
The LSO case study also demonstrates the importance of acknowledging
and modeling the dependence between $R$ and $Y$. A local permutation method not adjusting for trend
%\marginpar{$\leftarrow$ Outdated cross-ref?}
%(\S~\ref{sec:local-rand-based})
gave discrepant estimates of the
AP effect (Table~\ref{tab:alt}) and performed poorly in simulations
(\S~\ref{sec:simulation}).

Our framework for RDD analysis links the two approaches.
Residual ignorability \eqref{ycheck} assumes that the component of
$Y_c$ that depends on $R$ may be modeled and removed, leaving
residuals $\dt[\thetaInf]{\bm{Y}}{\bm{R}}$ that are independent of $Z$.
Like the local randomization approach, it targets the TOTE or ATE
within $\mathcal{W}$, as opposed to a difference of limits, and
accommodates discreteness in $R$ and donut designs.
In fact, with $\dt{\bm{Y}}{\bm{R}}\equiv\bm{Y}$, it reduces to the local
randomization method.
Like the limit-based approach, it models and accounts for the correlation between
$R$ and $Y$.
With the right modeling and fitting choices, it returns the
classical \textsc{ancova} estimate for an RDD (see \S~\ref{sec:robust-analys-covar}).

Our method improves upon both approaches by using robust
M-estimation to adjust for $R$.
For analysis of potentially imperfect RDDs, we see this
as a necessity.
For instance, covariate balance tests will necessarily be underpowered
to detect imbalance in a small fraction of the sample, so the proper
bandwidth $b$ will be uncertain.
  Likewise, if the initial sample
includes subjects who manipulated their recorded $R$
values, then the use of donut-shaped $\mathcal{W}$ may
remove some, but not all such subjects.  % With finite samples, sample pruning guided by specification
% tests is expected to leave an $O(n^{-1/2})$-sized share of the sample
% that perfect information would have counseled removing.
Robust
M-estimation retains consistency under scenarios such as these, with moderate amounts of
contamination % fractions of this order
\citep{he1991localbreakdown,yohaiZamar1997locallyrobustMestimates}, whereas OLS does not.

If a large fraction of the dataset violates \eqref{ycheck}, even robust M-estimates
can be inconsistent.
Thus, these methods should be used in addition to, rather than instead of, preparatory specification checks.

In simulated RDDs of moderate size, our estimates were unbiased and
our confidence intervals were typically narrower than those from an
OLS-based approach, while achieving nominal coverage.
Further simulations found robust
M-estimation to be compatible with the use of
global cubic and quartic polynomials to
accommodate nonlinear, imperfectly modeled relationships
between $R$ and $Y_{C}$, in marked contrast to methods using OLS to adjust for
trend.
%%Recent RDD methodology literature addresses
%%multiple running variables \citep{papay2011extending,
%%  reardon2012regression}. Residual Ignorability and robust
%%regression apply to such cases without modification.

% Other advantages include limitlessness, the estimation not of opposing limits of
% regression curves but rather of an average effect $\EE(
% Y_{T}-Y_{C} | D_{T}=1, \mathcal{W})$; and a natural, inferentially coherent
% interpretation when the running variable is discrete, or when
% observations at or immediately around threshold have had to be omitted.

\bigskip
\singlespacing
%% Finally, this paper highlights the need for future work on choosing a
%% window of analysis based on a sequence of specification tests.
{%\small
\bibliographystyle{asa}
\bibliography{causalinference}
%\bibliography{../../../adam.sales/drafts/causalinference}
}

\input{lrd-mathyappendix-r1}

\end{document}








