---
title: "Limitless regression discontinuity"
author: "Adam Sales & Ben Hansen*"
date: "ICORS 2018, KU Leuven "
bibliography: ../../bibliography.bib
output: 
  ioslides_presentation
---
<!-- nocite:
  @hansenSales2015cochran-->
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



# Potential outcomes vs structural modeling for RDDs

## Acad. probation and later grades | A "running" example (Lindo et al [-@lindo2010ability]) 

<div class="centered">
<!-- <img src="images/apsemaphore.jpg" width=180px height="270px" /> -->

<!--![](images/figure2_1.jpg) maybe I want something wider & shorter-->
<img src="images/figure2_1.jpg" width="300px" height="300px" />
</div>

Problem: _net of the running variable_ $R$ (college year 1 GPA), what's $Z \equiv \mathcal{I}[R<0]$'s causal effect on $Y$ (subsequent GPA)?

## Regression discontinuity designs | (RDDs): Two views

> - Regression-based view [@thistlethwaite1960regression]: Assume $Y = \alpha + \tau Z + \beta R + \epsilon$, $\epsilon \perp R$, at least locally [@imbens2008regression, ...].  The causal effect is $\tau$. 
> - Potential outcomes view [@lee2008randomized; 2017 obstudies.org discussants]: causal effects are quantities like $E [Y_T - Y_C]$, whatever the form of $E[Y|R]$; "local randomization" gives them to us via ancova/permutation testing/.... 
>- Our paper [-@lrdauthors:main] explores requirements and implications of potential outcomes+"local randomization." 

# Methods for picking out RCTs within RDDs

## Methods for picking out RCTs within RDDs {.build}

>  Q: I think I'm seeing naturally occurring random assignment in the vicinity of a regression discontinuity, but it's hard to isolate the subjects whose assignment can be regarded as haphazard.  Can methods help?

>  A: To separate unwanted chaff from locally randomized wheat, use specification tests....


<!-- (2 slides: balance; McCrary)-->

## $X$es should look "locally randomized" {.smaller}

<div class="centered">
<!--![](images/figure2_1.jpg) maybe I want something wider & shorter-->
<img src="images/figure2_2.jpg" width="300px" height="300px" />
</div>

> - Bandwidth selectors don't look at covariates, e.g. high school $G$rades ($y$ axis). 
> - Due to trend, clear that $E[G | R\leq 0] \neq E[G | R >0 ]$ -- no _balance_ on $G$.
> - Confine attention to $\{R \in \mathcal{W}\}$, $\mathcal{W}$ narrow enough to make imbalance non-significant? -- Doesn't change underlying trend, gets rid of a lot of data.
> - Better: first model trend, e.g. $G \approx \alpha_G + \beta_G R$; then test whether balanced on _residual_, $e_{(\alpha_G, \beta_G)}(G|R)$, not $G$ itself; narrow $\mathcal{W}$ as needed to pass this test.

<div class="notes">

- A modern bandwidth selection chose a window too wide for the linear model
- permutation tests comparing $X$es above and below 0 reject for windows w/ half-width $> .3$
- The window we considered had half-width $.5$.  Removing linear trend, its residuals are perfectly balanced.  

</div>

## Testing balance on the density of $R$ | the McCrary [-@mccrary2008manipulation] test

<div class="columns-2">


>  - Purpose of test: expose whether subjects secured or avoided treatment by manipulating their $R$s.
  
>  - If so, then density of $R$ differs on either side of cutpoint.
>  - A test of "balance" in (log) density
>  - Here we fail, due to students at $Z=0$ who narrowly avoided AP.

  <img src="images/forcing-discrete.jpg" width="300px" height="300px" />

> - To avoid wrath of McCrary, exclude $\{i: Z_i =0\}$.

</div>

<div class="notes">

- The widely used McCrary test checks for equal density of R immediately to eithert side of the cutpoint 
- no-manipulation hypothesis is rejected, due to chunk of students avoiding AP by falling exactly on top of the cutpoint
- Removing that chunk and that chunk only gets us a pass on this test.  
- thinking of this block of students as analogous to attriters in an RCT, this is differential attrition.  

</div>

## Methods for picking out RCTs within RDDs

> Q: I think I'm seeing naturally occurring random assignment in the vicinity of a regression discontinuity, but it's hard to isolate the subjects whose assignment can be regarded as haphazard.  Can methods help?

> A: To separate unwanted chaff from locally randomized wheat, use specification tests. 
> Remedies for test failure may involve reducing the sample.  Deciding which observations to get rid of is an inexact science. 

<!-- NB: FOR ICORS-18 EXCISED 1-SLIDE SECTION HERE, 6/29/2018; CONSIDER RESTORING FOR LESS TECHNICAL AUDIENCES-->

# Methods for limiting contamination sensitivity

## Methods for limiting contamination sensitivity {.build}

>   Q: Tests and adjustments may have gotten rid of the better part of the sample that didn't belong, but maybe they didn't remove every last bit of contamination.  Is there a sensitivity analysis method that addresses this issue?

>   A: You can address sensitivity to contamination during the _primary_ analysis...


<!-- 2 slides: LSO, Krafft points; bounded influence regression-->

## Sensitivity to removing _groups_ of points {.build}

Suppose a latent subgroup, "savvy," that's overrepresented in $\{i: z_i=0\}$.  We can't locate them, but if we could we'd take them all out. How much could this change affect estimates? 

This is the problem of _contamination sensitivity_ [@he1991localbreakdown, @yohaiZamar1997locallyrobustMestimates].  It has important implications for RDDs.

<div class="columns-2">
<img src="images/figure2.4a.jpg" width="300px" height="300px" />

<img src="images/Krafft_points.jpg" width="300px" height="300px" />
</div>

<div class="notes">

- A latent subgroup that doesn't belong is only problematic if they don't follow the same pattern as others.
- If the latent subgroup is both different and large, we ought to be able to find it in tests.
- Otherwise, we're in the domain of "$\epsilon$ contamination"
- OLS handles $\epsilon$ contamination poorly. Methods that handle it differently explore the sample systematically to find small subsets of points the exclusion of which changes the answer, then downweight these, sometimes 0.
- An example from molecular design occurs at right.  OLS gets distracted by the points at far right.  Bounded influence regression downweights them to get a very different fit.

</div>

##Bounded influence robust regression

- E.g., "MM with bisquare psi-function and robust scale",...
- Asymptotically, only these methods limit sensitivity to contamination of a $O(n^{-1/2})$ share of the sample.
- As a bonus, "robustness weights" give an additional diagnostic.
- In R, `robustbase::lmrob`; in Stata, `mmregress`.
- (Our $\mathcal{W}$ + MM estimation w/ bisquare give similar estimates to Lindo et al's: +.23 [.17, .30].)


## Methods for limiting contamination sensitivity

> Q: Tests and adjustments may have gotten rid of the better part of the sample that didn't belong, but maybe they didn't remove every last bit of contamination.  Is there a sensitivity analysis method addresses this issue?


> A: You can address sensitivity to contamination (as opposed to omitted variables) as a part of the primary analysis.  The robust statistics literature finds that only an unusual type of regression limits contamination sensitivity.  While unfamiliar, the methods are efficient, easy to use and readily available in R and Stata.


## References {.smaller}





