\documentclass[12pt]{article} %ordinary compilation
%\documentclass[aap]{imsart} % to approximate no pages via AOAS template
\usepackage{setspace}  %comment out if using imsart
\input{rd-jasa-preamble.tex}
\SweaveOpts{echo=FALSE, results=tex}

%\begin{document} %to compile via imsart
\title{Limitless Regression Discontinuity}

\author{Adam Sales \& Ben B. Hansen\thanks{The authors thank Susan
    Dynarski, Roc\'{i}o Titiunik, Matias Cattaneo, Guido Imbens, Brian Junker,
    Justin McCrary, Walter Mebane, Kerby Shedden, Jeff Smith, the participants in the
    University of Michigan Causal Inference in Education Research
    Seminar and anonymous reviewers for helpful suggestions. This research was supported by the Institute of Education Sciences, U.S. Department of Education (R305B1000012), and an NICHD center grant (R24 HD041028). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors. }}
%\usepckage{Sweave}

\begin{document} % ordinary compilation
\SweaveOpts{concordance=TRUE}
\maketitle

<<analyses>>=
                                        #source('analysis.R')
#if(!is.element('balance0.5',ls())) source('code/newAnalysis.r')
#balance0.5 <- 0.141593 # TEMPORARY!
#BW <- .5
#CI0.5 <- c(.10, .30)
load("RDanalysis.RData")

library(xtable)
library(Hmisc)

latexSN <- function(x){
    x <- format(x)
    x <- sedit(x, c("e+00", "e-0*", "e-*", "e+0*", "e+*"), c("",
        "$\\\\times 10^{-*}$", "$\\\\times 10^{-*}$", "$\\\\times 10^{*}$",
        "$\\\\times 10^{*}$}"))
    x
}

if (!exists("analysis_objects")) analysis_objects <- load('RDanalysis.RData')
@
\section{Introduction}
The ``regression discontinuity design" (RDD)
\citep{thistlethwaite1960regression,cook2008waiting}
is among the most credible alternatives to controlled experimentation
for estimating treatment effects. %\marginpar{wc: make more
%  specific. AS: better? BH: Yep!}
In an RDD, as in a randomized controlled trial (RCT),
there is a known mechanism of assignment to treatment conditions:
each subject has a value of the ``running variable,'' $R$,
and treatment is allocated to subjects for whom $R$ exceeds
(or falls below) a pre-determined cut point.
\citet{lee2008randomized} argued
that the RDD features ``local randomization" of treatment assignment,
and is therefore ``a highly credible and transparent way of estimating
program effects" \citep[][p. 282]{lee2010regression}.

According to the local randomization heuristic, RDDs
can recover important advantages of RCTs---specifically, ignorable treatment assignment, unbiased estimation of
average treatment effects (ATE) and covariate balance---if paired with ordinary regression analyses that target an extraordinary parameter.
In Imbens and Lemieux's telling \citeyearpar{imbens2008regression}, for example, the target of estimation is not
the ATE in any one region around the cutoff
but rather the limit of ATEs over concentric ever-shrinking regions, essentially
an ATE over an infinitesimal interval.
%For instance, in a randomized experiment a covariate $X$ has the same
%distribution regardless of treatment assignment.
%In an RDD, the distribution of $X$, as a function of $R$, is
%continuous at the cutoff $c$.
%That is, the limits of $X$'s distribution, as $R$ approaches $c$ from
%both sides, are identical.
Consistent with this ``limit'' understanding, it is common to analyze RDDs
using regression to estimate the functional
relationship of $r$ to $\EE( Y | R=r)$ with allowance for potential jump discontinuity
at the cut point, interpreted as the treatment effect
\citep[e.g..][]{berk1983capitalizing,angrist1999using,oreopoulos2006estimating}.

%% However, formulating local randomization in terms of limits fails to
%% realize some other benefits of experiments.
%% For instance, randomized experiments allow researchers to estimate
%% treatment effects for specific samples or
%% sub-populations.
%% [In contradistinction,...]
%% Similarly, randomized experiments allow analysts access to
%% distribution-free inferential methods, equally valid for large or
%% small samples
%% \citep{fisher:1935,pitman1937significance,rosenbaum:2002}.
%These assets would
%accompany a more literal interpretation of local randomization: that,
%in a window around $c$, treatment assignment is random or ignorable
%\citep[cf.][]{rubin:1978}.

Construing the goals of RDDs in terms of limits sells their similarity
to RCTs short.
Take the regression discontinuity design found in  \citet*{lindo2010ability} (hereafter LSO).
In many colleges and universities, struggling students are put on
``academic probation"; the school administration monitors these
students and devotes additional resources to them.
In addition, if their grade-point-averages (GPAs) fail to improve,
they are subject to suspension.
LSO attempt to estimate the effect of academic probation on students' subsequent GPAs.
At one large Canadian university, probationary status
was a function of students' first-year cumulative GPAs: students with first-year GPAs below 1.50 or 1.60,
depending on the campus, were put on probation.
Setting $R$ to first-year GPA minus the campus-specific cutoff makes
this an RDD, with cut point 0; comparing subsequent GPAs for students with
$R$ just below and above 0 should reveal the effectiveness of the policy at
promoting satisfactory grades.

Limit interpretations
take such data to identify ATEs for
weighted averages of students in the vecinity of the $R$-threshold
\citep{lee2008randomized}, or perhaps limits of ATEs for
sub-populations with increasing concentration of $r$ around 0
\citep{hahn2001identification}.  The weights or
sub-populations may be ambiguously defined.
Both versions are at odds with our sense that it should have
been possible to compare two specific samples of students, just as in
an RCT.
% Statistical adjustments for the running variable may be
% called for, just as informative analysis of RCTs may call for
% covariate adjustment. There may be ambiguity as to whether the design
% supports comparisons of students with $r$ within any specific window,
% such as $\pm .1$ or $\pm .2$ or $\pm .5$ of the cutpoint; likewise, in RCTs with loss to
% follow-up, it can be ambiguous whether comparisons restricted to
% subjects not lost to follow-up are themselves legitimately RCTs.

The GPAs of the Canadian universities case study being discrete, measured in
1/100s of the grade point, comparison of $\lim_{r\uparrow 0} \EE (Y |
R=r)$ to $\lim_{r\downarrow 0} \EE (Y | R=r)$ cannot coherently
be taken as the investigation's ultimate purpose. Strictly speaking, neither
limit exists, except perhaps as $\EE (Y | R=-.01)$ or $\EE (Y | R=0)
$. If the RDD is to be taken to identify the ATE of a specific
population after all, it is arbitrary to restrict it to students with
$R \in \{-.01, .00\}$, as opposed to a broader interval.
  Arbitrary and incoherent: absent information on how $\EE (Y | R=r)$
  trends at the left of $-.01$,  and on the right of $.00$, there can
  be no apportioning of a difference in those points' $y$-means to
  policy effects, as opposed to continuations of trends in  $\EE (Y |
R=r)$.  This problem is made worse when the analysis is forced to
exclude subjects immediately on or around the cut-point, as can occur
as a consequence of problematic findings from manipulation checks
\citep[discussed in \S~\ref{sec:specification} below]{mccrary2008manipulation}.

% The problem is practical as well as ontological: heuristics assuming
% $R$ to follow a Lebesgue density lead to RDD specification tests
% casting false aspersions on the academic probation study, as we see in
% Section~\ref{sec:recov-from-fail}, even after adapation to the
% discrete setting (Sec.~\ref{sec:specification}).  The limit
% interpretation of RDDs sits poorly with non-continuous running
% variables.  That being said, the local randomization heuristic can
% also engender undue pessimism about candidate RDDs if not properly
% adapted to their structure.  In the academic probation study,
% covariate balance tests that would have been appropriate for an RCT
% permit only quite narrow windows around the cutpoint
% (Sec.~\ref{sec:local-rand-based}), whereas a suitably adapted test
% (Sec.~\ref{sec:specification}) permits windows that are much wider.


To address these issues, this paper draws connections between
distribution-free analysis of RCTs (\S~\ref{sec:randProc});
classical methods for RDDs, which are parametric
(\S~\ref{sec:robust-analys-covar}); contemporary specification
tests for RDDs, which combine parametric and nonparametric elements (\S~\ref{sec:specification});
and modern methods robust regression
(\S~\ref{sec:robust-altern-ordin}).
Section~\ref{sec:theMethod} adapts the RDD's identifying
conditions to limitless estimation targets, with the remainder of that
section assembling methods suitable for estimating such targets in the
presence of common challenges for RDDs.
Section~\ref{sec:appl-effect-acad} identifies one such challenge in
the case of the LSO study, whereas \ldots\marginpar{Update me}
and \S~\ref{sec:discussion} concludes.

%% The resulting method features assumptions that align better with the
%% intuition that RDDs %heuristic motivation that RDDs
%% feature %leverage
%% natural randomization in the vicinity of a cut
%% point; % difficult sentence
%% offers new options in the face of validity threats; and enables
%% distribution-free, if inexact, causal comparisons between specifically
%% defined groups.
%%



\input{rd-jasa-r3-sec2+3}

\section{Application: The Effect of Academic Probation}
\label{sec:appl-effect-acad}


\begin{figure}[htb]
\centering
\mbox{
\subfigure[]{\includegraphics[width=.45\textwidth]{graphics/figure2_1.pdf}}
\quad
\subfigure[]{\includegraphics[width=.45\textwidth]{graphics/hs_gpa.pdf}}
}
\caption{(a) The RDD from LSO. First-year GPAs (x-axis) have been
  centered around a campus-specific cutpoint for academic probation. Subsequent
 GPA (y-axis) has been averaged according to first-year GPA. (b)
Log-transformed high-school grade percentile (a covariate), also averaged by first-year college
 GPAs.}
\label{LSO}
\end{figure}


\citet{lindo2010ability}---LSO---attempted to estimate the effect of
academic probation (AP) on college students at an unnamed
Canadian university.
One of the outcomes that LSO measured was \texttt{nextGPA}, students'
subsequent GPAs, either for the summer or fall term after students'
first years.
Figure~(\ref{LSO}a) displays \texttt{nextGPA} as a function of students'
first-year GPAs.
Their research question was whether AP caused a change in
\texttt{nextGPA}: did students on AP tend to have higher (or lower) subsequent
GPAs?
In all but 50 of 44,362 cases, being on AP coincided with whether
first-year cumulative GPA  ---  the running
variable, $R$ --- fell below a cutoff.
The university in question had three campuses, two of which had
cutoffs of 1.5; the other had a cutoff of 1.6.
To combine data from the three schools, LSO centered each student's
first-year GPA at the appropriate $c$, so $r_i$ is a student $i$'s
(realized) first year GPA, minus the cutoff at his college.
%Then, $Z=\indicator{R\le 0}$.

\subsection{Choosing $\mathcal{W}$ and $\mu_{\beta}(\cdot)$} \label{sec:choosing-mathcalw-f}
A relevant region in which to estimate a treatment effect is within
$\mathcal{W}_{0.5} =  \pm $ 0.5 %$0.3$
grade-points.
%Conventionally, $0.3$ represents the difference in grade points
%between a C, say, and a C-, or any other grade half-step.
This includes students whose AP status could change if their
grades in half their classes changed by a full mark (say from D
to C).
Simplicity recommends a linear specification for the outcome
regression on the forcing variable, and the scatter of $Y$ versus $R$
did not suggest otherwise; so we designated
%\marginpar{$\mu^{1} \mapsto \mu$?} %Manuscript has moved away from
                                %talk about adding monomials or
                                %splines to the specification. Maybe
                                %the 1 is just adds potential for confusion.
$\mu_{\beta}(R_i)=\beta_0+\beta_1 R_i$
(subject to potential revision following diagnostic checks).

To test $\mu_{\beta}$ and $\mathcal{W}_{0.5}$, we combine specification checks
discussed in Section~\ref{sec:specification}.

We conducted balance tests with each of the following covariates: high-school
grade percentile rankings; number of credits attempted in first year
of college; first language other than English;
birth outside of North America; age at college entry;
first language is English, and whether
students were born in North America; and which of
the university's 3
campuses the student attended, represented with indicator variables.
(These are the same pre-treatment variables LSO used in specification
checks.)
For the measurement variables, this amounted to fitting
\textsc{ancova} models, whereas binary covariates
were decomposed as logistic-linear in $R$ and $Z$;
in both cases subsequent Wald tests of $Z$'s coefficent used
Huber-White standard errors. High school grade percentiles were
logit-transformed, a step that somewhat improved the fit of the linear model.
After Bonferroni correction, each of the resulting $p$-values exceeded $0.2$.
%% The combined differences statistic of
%% \citet{hansen:bowers:2008}, %suggested an omnibus statistic
%% %for testing
%% applied to the 7 residual vectors as determined on
%% $\mathcal{W}_{0.5}$, yielded a p-value of $p=$\Sexpr{round(balance0.5,2)}, greater
%% than $\alpha_\mathcal{F}=0.1$.  (This is an approximate, Normal-theory
%% $p$-value, as computed with software of \citet{bowers2016ritools}.)

<<>>=
stopifnot(balance0.5>.1)
@

Unfortunately, the data include no lagged measurement of the outcome;
we must consider the possibility that $\EE( Y_{C}| R= r)$ is more
nearly linear to the right that to the left of the threshold (where
$Y_{C}$ is directly observed).  On the
other hand, Figure~(\ref{LSO}b) displays the logit of students'
high-school grade percentile rankings, %\marginpar{Remember to connect
%  to IK failure!}
and suggests that the curvature in relationship of (logit-transformed)
high-school grade percentiles with $R$ is greater than that of $R$'s
relationship with the outcome; including them in the balance check%,
 after residualizing %based on an admittedly weak model,
lends a
counterbalancing measure of conservatism to the assessment.
\marginpar{What does ``an admittedly weak model'' mean?}


In addition to the substantively motivated bandwidth of 0.5, we
determined the bandwidth that would been selected following the
adaptive procedure of applying placebo tests sequentially and choosing
the largest bandwidth not to be rejected.
Testing at level $\alpha=0.15$ at each step, this ended with the bandwidth \Sexpr{BW}.

The McCrary density test identifies a discontinuity in the running
variable at the cutpoint
\Sexpr{stopifnot(mccrary1<.0001)} %\Sexpr{latexSN(signif(mccrary1,2))}.
($p<0.0001$).  AP is a dubious distinction, and savvy students may try to
avoid it, perhaps by dropping a course, pleading with an instructor,
or otherwise manipulating their GPAs.
Inspection of the distribution of $R$ reveals an unusual
number of students whose first-year GPAs were exactly equal to the AP
cutoff, $R=0$.
Further, among the students with $R=0$, the number
attempting four or fewer credits was unusually high, suggesting that
some students may have dropped a class to barely avoid AP.
On the other hand, the appearance of an anomaly is at least partly a by-product of
discreteness of the data.
GPAs of four or fewer credits are necessarily clumpier than
GPAs of 4.5 credits or more since they average fewer components.
Indeed, $R$'s other high points within $b=0.5$ of the cutpoint,
$R=$\Sexpr{paste(paste0(hipts,conn.chars), collapse=" ")}, also
feature more students attempting four or fewer credits.
In any event, repeating the McCrary procedure
in a window $\mathcal{W}=\{i: |R_i|\in (0,0.5)\}$, with students with
$R=0$ removed, results in a p-value of \Sexpr{signif(mccrarySurgical,2)}.

\subsection{AP Outcome Analysis}

Table ~\ref{tab:results} gives a set of p-values, point estimates, and
interval estimates for the effect of AP.
Each row corresponds to a different specification:
the first row gives the ``main'' analysis, using the window
$\mathcal{W}$.
The next row gives the results from a similar analysis, but this time
with the adaptive window $\mathcal{W}_{a}=$ \Sexpr{BW}, determined by
progressively testing smaller bandwidths until identifying one that
was not rejected at level .15.
The last row, ``IV,'' reports on the fuzzy RDD analysis described in
Section~\ref{sec:recov-from-fail}.

According to the main and adaptive analyses, AP gave a modest benefit
over this range.  The IV analysis is more equivocal, including values
on both sides of 0 in its 95\% confidence interval and presenting a
somewhat smaller point estimate.
%% The results largely agree, although the fuzzy
%% RDD strategy gives a larger confidence interval, straddling zero.
%% The effect of AP students' subsequent GPAs, for students with
%% first-year GPAs about half a grade point from
%% the AP cutoff is between \Sexpr{round(CI0.5[1],2)} and
%% \Sexpr{round(CI0.5[2],1)}, with 95\% confidence.



% latex table generated in R 3.2.1 by xtable 1.8-0 package
% Mon Jun 27 17:58:03 2016
\begin{table}[ht]
\centering
\input{tab-results}
\label{tab:results}
\caption{Estimates of AP effects using the method of Section~\ref{sec:theMethod}, a
  variant selecting $\mathcal{W} $ adaptively and the instrumental
  variables variant described in Section~\ref{sec:recov-from-fail}.
  %% on students'
  %% subsequent GPAs: the main analysis, with a substantively-chosen
  %% bandwidth, an analysis with a data-driven bandwidth, and an analysis
  %% using a fuzzy RD, or instrumental variables (IV), approach to
  %% account for possible student manipulation of the running variable.
}
\end{table}


\section{Comparison with selected alternatives} \label{sec:simil-diff-with}

\subsection{LSO Results from Other Methods} \label{sec:lso-results-from}

How does our method compare with others in the LSO analysis?  To see,
we estimated the same effect using two alternative methods: a more
conventional ``local OLS'' approach, and a permutation-based approach
that shares some similarities with ours.
In this section we will discuss similarities and differences between these
methods and ours, as well as contrast their findings using the same
dataset.


% latex table generated in R 3.2.3 by xtable 1.8-0 package
% Wed Jun 29 11:46:37 2016
\begin{table}[ht]
\centering
\input{tab-alt}
\label{alt}
\caption{The effect of Academic Probation from our main analysis compared with permutation and OLS analyses.}
\end{table}



\subsection{Ordinary least squares}\label{sec:standardMethod}
The standard approach to estimating RDDs is broadly similar to what we
present here, in that both methods require
analysts to specify and fit models for $Y_C$ and $\tau$.  However,
whereas we recommend robust fitting, the standard method uses ordinary
least squares. Fitting of the outcome model may or may not be preceded
by covariate placebo tests, but these rarely play a direct role in
bandwidth selection.
Confidence intervals from the OLS-based approach are typically
Wald-type---that is, $\hat{\tau} \pm z_{\alpha/2} \mathrm{SE}(\hat{\tau})$,
where $z_{\alpha/2}$ is an appropriate normal or t-distribution
quantile---rather than inversions of a families of hypothesis tests.
%% The OLS approach is well-suited to probelms in which the regression
%% model is well-specified and the regression errors are normally
%% distributed, in which case OLS will give efficient results.

The state-of-the-art method, described in,
e.g. \citet{imbens2008regression}, recommends fitting a regression
model with local least squares, using a triangular kernel function.
This modification from OLS is intended to mitigate the effects of
model misspecification by down-weighting observations far from the
cutoff.

To implement this method, we used the \verb|rdd| package in \verb|R| \citep{rdd} to implement the
latest limit-based RDD analysis.  This used a local linear regression,
with the bandwidth recommended by \citet{imbens2012optimal}.
To facilitate comparisons with our method and the permutation-based
method, we used a rectangular kernel---essentially estimating an OLS
model in a region around the cutoff---rather than the conventional triangular
kernel.

The results are displayed in Table ~\ref{alt} as ``Local OLS.'' The
method broadly agrees with ours as to the effect of academic
probation.
Interestingly, the \citet{imbens2012optimal} bandwidth,
1.25, was large enough so as to be rejected by covariate-based
specification tests.
In particular, the p-value for de-trended covariate balance at the cutoff, using a
bandwidth of 1.25, was 0.008.

%; if it is, the tests do not ordinarily
%residualize the covariates prior to testing.

%% Perhaps surprisingly, the standard estimator coincides with our estimator in one particular setting.
%% If analysts use (\ref{standard}) to estimate $\tau$, and, in a different analysis, model $Y_C$ with ordinary least squares as linear, model $\tau$ as constant, and use the difference in means between treatment and control subjects as a test statistic, the two estimates will be identical.
%% We formalize this in the following proposition:
%% \begin{prop}
%% If, in $\mathcal{W}$, a researcher uses OLS to model
%% $\mathbf{Y_C}=f(\mathbf{R}) \beta +\mathbf{\epsilon}$ and $\mathbf{\tau}=\tau_0$, and takes as the test statistic $\mathbf{\Ych'Z}$, then the Hodges-Lehmann estimate of $\tau_0$ will be equal to the OLS estimate of $\tau$ from the regression (\ref{standard}), fit using the data in $\mathcal{W}$.
%% \end{prop}
%% The proof follows an argument of \citet[][ p. 290]{rosenbaum2002covariance}.
%% \begin{proof}
%% Let $\mathcal{R}$ be the matrix formed by joining a column of ones to
%% $\mathbf{R}$. Then let
%% $\mathcal{H}=\mathcal{R}(\mathcal{R}^T\mathcal{R})^{-1}\mathcal{R}'$. Under $H_{\tau_0}:Y_{Ti}-Y_{Ci}=\tau_0$,
%% $\tilde{\mathbf{\Ych}}_{\tau}=(I-\mathcal{H})(\mathbf{Y}-\mathbf{Z}\tau)$ and the test statistic is
%% $\mathbf{Z}^T\tilde{\mathbf{\Ych}}_\tau=\mathbf{Z}^T(I-\mathcal{H})(\mathbf{Y}-\mathbf{Z}\tau)$. When $\tau=\tau_0$, the expected value of the
%% test statistic is $\EE \mathbf{Z}^T \tilde{\mathbf{\Ych}}_\tau=\EE \mathbf{Z}^T\EE \tilde{\mathbf{\Ych}}_\tau=0$ since
%% $\mathcal{R}$ contains a constant term and the model is fit with OLS.
%% The Hodges-Lehmann estimate of $\tau$ solves the equation
%% \begin{align*}
%% \mathbf{Z}^T\tilde{\mathbf{\Ych}}_\tau&=0,\, \mathrm{or}\\
%% \mathbf{Z}^T(I-\mathcal{H})(\mathbf{Y}-\mathbf{Z}\tau)&=0.
%% \end{align*}
%% This is satisfied by
%% $\tau=\frac{\mathbf{Z}^T(I-\mathcal{H})\mathbf{Y}}{\mathbf{Z}^T(I-\mathcal{H})\mathbf{Z}}$, which is equal
%% to the OLS estimate of the coefficient of $Z$ from the regression of
%% $Y$ on a constant, $R$, and $Z$.
%% \end{proof}
%% That is, one of the simplest conventional RDD estimates can be
%% re-interpreted as a Hodges-Lehmann estimate of a constant treatment
%% effect under Residual Ignorability.
%% In this case, rather than provide a new method, we reinterpret the
%% conventional method.
%% However, note that interval estimates and p-values may still differ
%% between the two methods. \marginpar{Not any more (see in-line)}
%% %Rather, as presently defined they're precisely the same, since OLS is
%% % the same as M-estimation w/ $\psi(\cdot ) =  \cdot$

\subsection{Permutation-Based Inference (\citealt{cattaneo2014randomization})}\label{sec:local-rand-based}
In the permutation method of
\citet{cattaneo2014randomization}, one begins by limiting the data to
a small $\mathcal{W}$, then assumes $Z \independent Y_C$.
(The Bayesian method of \citet{liMatteiMealli2015BayesianRD} begins from a similar assumption.)
This corresponds to the case where one ``models''
the relationship between $Y_C$ and $R$ with a constant function---that
is, assumes no relationship between $Y_C$ and $R$---in $\mathcal{W}$,
in which case Residual Ignorability is equivalent to standard
ignorability ($Y_C \independent Z$).

Failure of this assumption could
explain differences between its assessment of LSO and those of the
other two methods shown in Table~\ref{alt}.  The general trend---higher \texttt{nextGPA} for higher $R$---is in the
opposite direction as the apparent effect of AP.  If students with
lower $R$ are treated, but experience a positive effect, then
the two factors may partially cancel each other out, leading to a point estimate that is biased toward zero.
If this is the case, it illustrates the importance of explicitly
modelling $R$ in an RDD analysis, even for small windows $\mathcal{W}$.


%% Standard ignorability can be sustained only if $R$ and $Y_C$ do not
%% associate within $\mathcal{W}$, or $\mathcal{W}$ is too narrow for the
%% relationship to be detected.
%% For an example in which neither condition is likely to be the case, consider \citet{wong2007effectiveness}, which use RDDs to study the effects of pre-school on children's pre-literacy skills.
%% In that study, the running variable is a youngster's age: children born before a certain date are eligible for government pre-school assistance, and those born after are not.
%% The pre-school-age years of a child's life compose a period of rapid cognitive development.
%% It is hard to imagine that, on average, even slightly older students would not perform substantially better on pre-literacy exams than their slightly-younger peers.
%% A window $\mathcal{W}$ would have to be quite small for the systematic
%% age difference to be ignorable.
%% On the other hand, Residual Ignorability only requires a suitable
%% parametric model for pre-literacy skills'  association with age.

%% With unusually comprehensive existing research on the running
%% variable-outcome association, a researcher might posit \eqref{ycheck}
%% with a pre-designated $\bar\theta$, i.e.
%% $\hat\theta \equiv \bar\theta$. In this case strong randomization
%% inferences could be made from $e_{\bar\theta}(\cdot)$, including for
%% example inference about quantile or displacement effects
%% \citep{rosenbaum:2001}, which are not readily available from within
%% our framework.


\subsection{Conditional Ignorability Assumption (\citealt{angrist2012wanna})}\label{sec:CIA}
\marginpar{Suggest that we remove this sec}
\citet{angrist2012wanna} addressed the question of estimating
treatment effects away from the cutoff with a new assumption, called
the ``Conditional Ignorability Assumption,'' or CIA.
The assumption states that, conditional on covariates $X$, $R$ is
  mean-independent of potential outcomes $Y_C$ and $Y_T$. % A fixed
This approach shares two important similarities with ours.
First, it explicitly formulates causal identification in terms of an
ignorability assumption.
Second, it is interested in effects for a sample of subjects which is
not asymptotically vanishing.
%% In its details, though, it is quite different---its identification
%% assumption is different from ours, as is the method it proposes.
%% Unlike our approach,
%% CIA assumes mean independence, which is sufficient for unbiased estimation but not
%% inference.
%% Therefore, for estimation, CIA may be weaker than Residual
%% Ignorability, but for inference it requires some stronger assumptions.
The differences are that CIA asserts the independence of $R$ from the potential
outcomes, whereas Residual Ignorability asserts the independence of
$Z$ from $e_{\bar\theta}(Y_{C}| R)$;
Since it incorporates $X$, an analysis based on CIA requires more modeling steps than our approach, such as matching or regression-based covariance adjustment.


\subsection{A Simulation Study}
\marginpar{Suggest we put sims in appendix}
To shed some light on the circumstances under which our method
performs better, or worse, than the OLS approach and the permutation approach, we conducted a small simulation study.
%The study has two parts: an examination of bandwidth selection under
%the three approaches, followed by a study of hypothesis testing.

In these analyses, the running variable $R$ was generated as
$\mathrm{Uniform}(-1,1)$, with a cutoff $c=0$.
The outcomes were generated by the following model:
\begin{equation}\label{eq:simOutcome}
Y=0.5R+(3R+1.5)\indicator{R<-0.5}+\tau \indicator{R>0}+ \epsilon.
\end{equation}
Within a the window $0 \pm 0.5$, $Y$ is linear in $R$, with a slope of
0.5.
For R below $-$0.5, on the control side, the slope increases to 3.5---hence,
for bandwidths greater than 0.5, a linear model is misspecified.
The treatment effect $\tau$ is set to 1 in some simulation runs and 0
in others; it is added to subjects with running variables $R$ above
the cutoff $c=0$.
The errors $\epsilon$ were distributed as $t_{3}$.

Each simulation run consisted of randomly generating 500 % change this back to 5K _after_ fixing sim code
datasets and
then analyzing each one with the three methods applied to LSO in
Table~\ref{alt}.
% For the bandwidth selection study, there was one covariate $x$ which
% was generated with the same model as $Y$, \eqref{eq:simOutcome}, with
% $\tau$ set to 0.

% \subsubsection{Bandwidth Selection}
% We compared three bandwidth selection techniques.
% The local randomization method, suggested by
% \citet{cattaneo2014randomization}, consists of testing for a
% difference in means of $x$ below and above the cutoff for a seequence
% of possible bandwidths, and selecting the largest bandwidth $b^*$ with
% a p-value above $\alpha=0.15$.

% \citet{imbens2012optimal} suggests a data-driven bandwidth to minimize
% the mean squared error of the OLS estimator by non-parametrically
% estimating the curvature of $\EE[Y|R=r]$.
% We implemented it using the \texttt{rdd} package \citep{rdd} with the
% ``rectangular'' bandwidth, which is more directly comparable to our
% bandwidth-selection technique that the typical triangular kernel.

% Finally, we implemented our technique, picking the largest bandwidth
% for which a balance test of the detrended covariate was not rejected
% at level $\alpha=0.15$.

% In this simulation, the largest symmetric window around the cutoff in
% which $Y$ was linear in $R$ was (-0.5,0.5)---that is, a bandwidth of
% 0.5 is the largest that would return unbiased estimates from a linear
% model.
% The local randomization model, however, requires even smaller
% bandwidths since it does not estimate a slope at all.

% The results are in Table ~\ref{tab:bandwidth}

% % latex table generated in R 3.2.1 by xtable 1.8-0 package
% % Mon Jun 27 10:38:02 2016
% \begin{table}[ht]
% \centering
% \begin{tabular}{rrrr}
%   \hline
%  & Local Randomization & Limitless & OLS \\
%   \hline
% n=50 & 0.56 & 0.79 & 1.03 \\
%   n=500 & 0.26 & 0.70 & 0.91 \\
%   n=5000 & 0.12 & 0.56 & 0.66 \\
%    \hline
% \end{tabular}
% \caption{Average bandwidth choices over 5000 simulation runs for three
%   bandwidth selection methods: ``Local Randomization,'' as described
%   in \citet{cattaneo2014randomization}, the ``limitless'' approach
%   described in Section ~\ref{sec:bandwidthChoice}, and the optimal
%   ``OLS'' bandwidth described in \citet{imbens2012optimal}, with a
%   rectangular kernel}
% \label{tab:bandwidth}
% \end{table}


% Across sample sizes, our technique, denoted ``limitless,'' tended to
% pick bandwidths that were larger than the local randomization
% bandwidths and smaller than the OLS bandwidths.
% The limitless approach tended to pick bandwidths slightly larger than
% the target 0.5, though for the largest sample size, the average was
% approximately on target.
% The local randomization method chose progressively smaller bandwidths
% as the sample size grew, reflecting the fact that its choice is
% entirely a function of the power of a difference in means test to
% detect the presence of a slope in the $\EE[x|R]$ function.


\subsubsection{Simulation Results} \label{sec:simulation-results}

We compared both the level of a null hypothesis test, in the absence
of a true treatment effect, and the power of a test, in the presence
of an effect, on all three methods, at a range of bandwidths and
sample sizes.
Table ~\ref{tab:levelSimulation} displays the percentage of null
hypotheses rejected at the $\alpha=0.05$ level in the absence of a
treatment effect---that is, the true level of the test.

\begin{table}[ht]
\centering
\begin{tabular}{l|ccc|ccc|ccc}
  \hline
 \multicolumn{1}{r}{} &\multicolumn{3}{c}{Permutations}&\multicolumn{3 }{c}{``Limitless''}&\multicolumn{3 }{c}{Local OLS}\\
\multicolumn{1}{r}{bandwidth:} & 0.3 & 0.5 & \multicolumn{1}{c}{0.75} & 0.3 & 0.5 & \multicolumn{1}{c}{0.75} & 0.3 & 0.5 & \multicolumn{1}{c}{0.75}\\
 \hline
n= 50 & 0.07 & 0.06 & 0.20 & 0.11 & 0.09 & 0.08  & 0.17 & 0.10 & 0.08 \\
  n= 500 & 0.13 & 0.34 & 0.96 & 0.05 & 0.05 & 0.10  & 0.06 & 0.04 & 0.06  \\
  n= 5000 & 0.61 & 1.00 & 1.00 & 0.07 & 0.04 & 0.50 & 0.06 & 0.04 & 0.29\\
   \hline
\end{tabular}
\caption{Empirical sizes of hypothesis tests
  %% for the local
  %% randomization method, our ``limitless'' method, and the local OLS
  %% method,
  as simulated ($N=500$) % change this back to 5K _after_ fixing sim code
  at a range of bandwidths and sample
  sizes.  (At the .75 bandwidth each method uses a misspecified model,
  and is expected to perform poorly, particularly with large $n$.)
  %% With
  %% smaller bandwidths and at $n=50$, all methods' sizes are distinguishably larger
  %% than .05, with the local permutation method's being
  %% closest to the .05 target.  With smaller bandwidths but $n=500$ or
  %% 5000, type 1 error rates of the local
  %% permutation method greatly exceed 0.05, while OLS's and the
  %% method of this paper's fluctuate slightly around 0.05.
}
\label{tab:levelSimulation}
\end{table}

For sufficiently small bandwidths, $bw=0.3$ or $0.5$, the limitless RD
method performs as advertised, with the exception of slightly inflated
type 1 error rates in small samples.
The error rates in large bandwidths grow with sample size, as the
misspecification becomes statistically significant with higher probability.
The local OLS method performs similarly.

The local randomization method performed roughly as advertised for
small bandwidths and small sample sizes, but its type 1 error rates
increased rapidly with sample size at all bandwidths.

We also examined the three methods' power to detect a treatment effect
of roughly a third the size of the residual variance ($\delta=0.33$).
For these simulations, we only report results from bandwidths 0.3 and
0.5, since higher bandwidths lead to an inflated type I error rate.
The results are displayed in Table ~\ref{tab:powerSimulation}.

% latex table generated in R 3.2.1 by xtable 1.8-0 package
% Mon Jun 27 17:14:17 2016
\begin{table}[ht]
\centering
\begin{tabular}{l|cc|cc|cc}
  \hline
 \multicolumn{1}{r}{}   &\multicolumn{ 2 }{c}{Permutation}&\multicolumn{ 2 }{c}{``Limitless''}&\multicolumn{ 2 }{c}{Local OLS}\\
\multicolumn{1}{r}{bandwidth:} & 0.3 & \multicolumn{1}{c}{0.5} & 0.3 & \multicolumn{1}{c}{0.5} & 0.3 & \multicolumn{1}{c}{0.5} \\
 \hline
n= 50 & 0.33 & 0.59 & 0.17 & 0.18 & 0.23 & 0.19 \\
  n= 500 & 1.00 & 1.00 & 0.64 & 0.86 & 0.47 & 0.66 \\
  n= 5000 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
   \hline
\end{tabular}
\label{tab:powerSimulation}
\caption{Power to detect an effect of size $\delta=0.33$, in
  $N=5000$ simulations.
  %% The local permutation
  %% approach has greatest power at $n=50$ and $500$, although for
  %% $n=500$ its type 1 error rates are also high
  %% (Table~\ref{tab:levelSimulation}). The remaining methods perform similarly, although the method of this paper has somewhat better power
  %% in the moderate sample condition.
}
\end{table}

When $n=50$, none of the methods was well powered; that being said,
the permutation method edged out limitless regression
discontinuity and local OLS.
When $n=500$, the permutation method rejected the null
hypothesis for every simulation run; however, at these bandwidths its
type-I error rates were similarly inflated.

The power of our method at $n=500$, especially at a
bandwidth of 0.5, is competitive with the power of the permutation method,
without incurring the same high type-I error rates.
Additionally, its power is significantly higher than that of the local
OLS method.
In particular, with a bandwidth of 0.5, the limitless method's power
exceeds the common threshold of 80\%, while the local OLS achieve
power of 66\%.







\section{Discussion} \label{sec:discussion}

% The detrending steps recommending in this paper and elsewhere \citep{imbens2008regression} may increase the
% utility of covariate placebo tests.  Without prior detrending, such
% tests are too likely to end in rejection when applied to a covariate
% that associates with the running variable.
%Furthermore, robust
%detrending has an important advantage over detrending via ordinary
%least squares. As a candidate window grows to exceed the range within
%which covariate ignorability holds, the robust fit maintains stability
%in the face of contamination with data that may follow a different
%trend, and this added stability can in turn make it easier to detect
%differences above and beneath the RDD threshold.
%
There is little reason to expect covariate placebo testing alone to
exclude windows $\mathcal{W}$ that are only moderately too wide; this
is particularly so in the absence of lagged measurements of outcome
variables.  Likewise, if the initial sample includes subjects with the
capability to manipulate their recorded $R$ values, then the use of
donut-shaped $\mathcal{W}$ can be expected to remove some such
subjects but not necessarily purify the sample of them.  Bounded
influence regression is important when either of these forms of
contamination may be present.  As \citet{he1991localbreakdown} and
\citet[Thm.~2.1 \textit{ff}.]{yohaiZamar1997locallyrobustMestimates}
have shown, in order to limit sensitivity even to small,
i.e. $O(n^{-1/2})$, fractions of contaminated data, it is necessary to
use a bounded influence regression fitter; both OLS and
first-generation robust methods such as Huber's are inconsistent under
corruption of $O(n^{-1/2})$-sized shares of the sample.

Residual Ignorability places conditions on $Y_{C}$, which is
incompletely observed; practical criticism of the assumption uses ${\mathbf{y}_H} = \mathbf{y} -
g_{\hat\tau}(\mathbf{r}) \mathbf{d}$ as a
stand-in for $\mathbf{y}_{C}$, with $\hat\tau$ an M- or Hodges-Lehmann
estimate. Because both M-estimates and the sandwich standard errors we
have paired with them are heteroskedasticity-consistent, under \eqref{ycheck} our tests and
subsidiary regression fits
remain valid if this reconstruction of $\mathbf{y}_{C}$ is only correct
only in an on-average sense. Under \eqref{ycheck}, $\hat\tau$ will be
consistent for $\bar\tau$ solving
\begin{equation} \label{eq:concl1}
\EE \big\{ e_{\bar\theta}(Y - Dg_{\tau}(R) | R) \big| R \big\}
\equiv \EE \big\{ e_{\bar\theta}(Y_{C} | R) \big| R \big\} ,
\end{equation}
if the family $\{g_{\tau}(\cdot) : \tau \}$ is sufficiently broad that
a solution for \eqref{eq:concl1} exists.

Estimates and confidence intervals based on
Section~\ref{sec:recov-from-fail}'s IV-type specification assume the
exclusion restriction \citep{Angrist:etal:1996}.  This condition fails
if the mere threat of AP affected subsequent GPAs differently in
treatment and control groups.  However, in that case the test of the
no-effect hypothesis would remain valid, provided that Residual
Ignorability is valid, albeit as a test of the policy's overall
effect, as opposed to its effect specifically on students placed on
AP.

The method of this article is intrinsically asymptotic, a limitation.
%% Although a study of our method's
%% finite-sample properties in RDDs broadly is beyond the scope of this
%% article, related literatures have reached pertinent conclusions.
It blends Wald methods with generalized score tests
\citep{boos1992genscoretest}, an approach adapted to moderately large samples.
% (A ``pure'' generalized score test would have
% avoided Section~\ref{sec:test-hypoth-no}'s shortcut of borrowing its
% $t$-statistic from a regression fit, instead calculating both the
% numerator and denominator of \eqref{eq:tedef} under the constraint of
% $H$.)
More research is needed to determine whether such a method would
improve small-sample performance% as documented in Section~\ref{sec:simulation-results}
; theory and simulations not specific to RDDs
suggest that it may, if perhaps at the expense of power to detect smaller effects in larger samples \citep{heShao1996Bahadurefficiencyofscoretests,guoetal2005robustscoretestsmalln}.
  %% Its heteroskedasticity-consistent variance estimates may s sometimes perform poorly in finite samples, due
  %% to the difficulty of tallying contributions to sampling variability
  %% from high-leverage observations
  %% \citep{cribarietal2007leverage,cribarietal2008leverageerrata}.
  %%  However, robust \textit{regression}
  %% \citep{tukey1962future,huber1967behavior,hampel1974influence}
  %% attacks the problem of high leverage directly, as discussed
  %% above in Sec.~\ref{sec:model-eey-c-r},
  %% limiting the need to keep track of influential observations'
  %% outsized contributions to coefficients' sampling variability.
  %% %% Simulation studies of modern robust fitting methods,
  %% %% including the one used in this article,
  %% %% document close agreement between asymptotic size and empirical
  %% %% coverage at $n=25$
  %% %% \citep{kollerstahel2011robust}.
  %% These
  %% studies considered Wald-type tests, whereas ours combine aspects of
  %% Wald and generalized score tests, which may perform better in small
%%  samples; see Section~\ref{sec:randProc}, above.
%% (Our tests did not estimate $\mathrm{SE}_{s}$ while holding the
%% regression's $z$-coefficient fixed at the hypothesized value, as a true
%% generalized score test would, because that was not supported by the
%% robust fitting software. Indeed, pairings of robust
%% regression with robust standard errors, as recommended by
%% \citealp{crouxetal2004robustSEforrobustreg}, are surprisingly uncommon. )



%% In the study of academic probation discussed here, our method yielded
%% a wider confidence than the conventional method.  Its disadvantage may
%% stem in part from our decision to focus attention on a substantively
%% meaningful analysis window that was narrower than the conventional
%% method's -- and narrower than the largest $\mathcal{W}$ that would
%% have been compatible with it.

Recently, the RDD methodology literature has begun to address the case
of multiple running variables \citep{papay2011extending,
  reardon2012regression}.
The method we present here extends to that case in a straightforward
way, using multivariate modeling techniques to disentangle outcomes
from the running variables and joint permutation tests for inference.

It can be argued that the most credible model of any actual data will
permit them to be discrete \citep{holland:1979}.  Advantages of this
paper's approach to RDDs include natural interpretation and
statistical inference when the running variable is discrete, or when
observations at or immediately around threshold must be removed from
the analytic sample; estimation of effects $Y_{T}-Y_{C}$ as averaged
over intervals $\mathcal{W}$ of positive extent; and explicit
attention to both type 1 and type 2 errors when trimming of this
window is guided by specification tests.

%% Finally, this paper highlights the need for future work on choosing a
%% window of analysis based on a sequence of specification tests.
\appendix
\include{rd-jasa-r3-mathyappendices}

\bibliographystyle{asa}
\bibliography{causalinference}
%\bibliography{../../../adam.sales/drafts/causalinference}


\end{document}


